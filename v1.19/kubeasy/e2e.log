I0906 10:15:36.054493      20 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-098945227
I0906 10:15:36.054556      20 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0906 10:15:36.054972      20 e2e.go:129] Starting e2e run "6ff83ee3-076e-4ce8-9fcb-997f420d524a" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1662459335 - Will randomize all specs
Will run 305 of 5484 specs

Sep  6 10:15:36.082: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
E0906 10:15:36.083681      20 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
Sep  6 10:15:36.085: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep  6 10:15:36.096: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  6 10:15:36.115: INFO: 6 / 6 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  6 10:15:36.115: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep  6 10:15:36.115: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  6 10:15:36.120: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Sep  6 10:15:36.120: INFO: e2e test version: v1.19.16
Sep  6 10:15:36.121: INFO: kube-apiserver version: v1.19.16
Sep  6 10:15:36.121: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:15:36.125: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:15:36.125: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-probe
Sep  6 10:15:36.147: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:15:36.158: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:15:38.161: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:15:40.161: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:15:42.162: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:15:44.162: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:15:46.162: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:15:48.162: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:15:50.162: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:15:52.162: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:15:54.162: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:15:56.161: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:15:58.162: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:16:00.162: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:16:02.161: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:16:04.162: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:16:06.163: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:16:08.162: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Running (Ready = false)
Sep  6 10:16:10.161: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Running (Ready = false)
Sep  6 10:16:12.161: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Running (Ready = false)
Sep  6 10:16:14.161: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Running (Ready = false)
Sep  6 10:16:16.161: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Running (Ready = false)
Sep  6 10:16:18.162: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Running (Ready = false)
Sep  6 10:16:20.162: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Running (Ready = false)
Sep  6 10:16:22.161: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Running (Ready = false)
Sep  6 10:16:24.162: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Running (Ready = false)
Sep  6 10:16:26.161: INFO: The status of Pod test-webserver-1cd38275-4c8b-4caf-b719-c6e6ec7b1f5b is Running (Ready = true)
Sep  6 10:16:26.163: INFO: Container started at 2022-09-06 10:16:07 +0000 UTC, pod became ready at 2022-09-06 10:16:24 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:16:26.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4621" for this suite.

• [SLOW TEST:50.044 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":1,"skipped":13,"failed":0}
S
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:16:26.170: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:16:26.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5411" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":2,"skipped":14,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:16:26.200: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-70883195-2eb6-4e58-aa6d-3244094328f0 in namespace container-probe-2263
Sep  6 10:16:46.233: INFO: Started pod busybox-70883195-2eb6-4e58-aa6d-3244094328f0 in namespace container-probe-2263
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 10:16:46.236: INFO: Initial restart count of pod busybox-70883195-2eb6-4e58-aa6d-3244094328f0 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:20:46.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2263" for this suite.

• [SLOW TEST:260.502 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":3,"skipped":17,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:20:46.702: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-dbb689b0-539d-43ed-ae87-4f3d30d5fb37
STEP: Creating a pod to test consume configMaps
Sep  6 10:20:46.738: INFO: Waiting up to 5m0s for pod "pod-configmaps-6b220b6d-9408-47ad-a234-89253888c537" in namespace "configmap-3575" to be "Succeeded or Failed"
Sep  6 10:20:46.759: INFO: Pod "pod-configmaps-6b220b6d-9408-47ad-a234-89253888c537": Phase="Pending", Reason="", readiness=false. Elapsed: 21.118237ms
Sep  6 10:20:48.765: INFO: Pod "pod-configmaps-6b220b6d-9408-47ad-a234-89253888c537": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026697829s
Sep  6 10:20:50.768: INFO: Pod "pod-configmaps-6b220b6d-9408-47ad-a234-89253888c537": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030000546s
STEP: Saw pod success
Sep  6 10:20:50.768: INFO: Pod "pod-configmaps-6b220b6d-9408-47ad-a234-89253888c537" satisfied condition "Succeeded or Failed"
Sep  6 10:20:50.770: INFO: Trying to get logs from node vm114011 pod pod-configmaps-6b220b6d-9408-47ad-a234-89253888c537 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 10:20:50.789: INFO: Waiting for pod pod-configmaps-6b220b6d-9408-47ad-a234-89253888c537 to disappear
Sep  6 10:20:50.792: INFO: Pod pod-configmaps-6b220b6d-9408-47ad-a234-89253888c537 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:20:50.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3575" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":4,"skipped":32,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:20:50.804: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep  6 10:20:51.073: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 10:20:54.087: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:20:54.091: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:20:55.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8918" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":5,"skipped":56,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:20:55.382: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep  6 10:20:55.431: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 10:20:55.438: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 10:20:55.444: INFO: 
Logging pods the apiserver thinks is on node vm114011 before test
Sep  6 10:20:55.448: INFO: kube-flannel-ds-7mh49 from kube-system started at 2022-09-06 10:12:05 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:55.448: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 10:20:55.448: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-9wpsp from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 10:20:55.448: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:20:55.448: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 10:20:55.449: INFO: 
Logging pods the apiserver thinks is on node vm114012 before test
Sep  6 10:20:55.454: INFO: kube-flannel-ds-8kz6j from kube-system started at 2022-09-06 10:12:05 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:55.455: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 10:20:55.455: INFO: metrics-server-6f58bc76cc-dv8ws from kube-system started at 2022-09-06 10:13:09 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:55.455: INFO: 	Container metrics-server ready: true, restart count 0
Sep  6 10:20:55.455: INFO: sonobuoy from sonobuoy started at 2022-09-06 10:14:15 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:55.455: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 10:20:55.455: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-tdhsh from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 10:20:55.455: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:20:55.455: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 10:20:55.455: INFO: 
Logging pods the apiserver thinks is on node vm114013 before test
Sep  6 10:20:55.461: INFO: coredns-588b5cd46d-jcnzp from kube-system started at 2022-09-06 10:13:07 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:55.461: INFO: 	Container coredns ready: true, restart count 0
Sep  6 10:20:55.461: INFO: kube-flannel-ds-jxdkc from kube-system started at 2022-09-06 10:12:05 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:55.461: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 10:20:55.461: INFO: metrics-server-6f58bc76cc-hppkr from kube-system started at 2022-09-06 10:13:09 +0000 UTC (1 container statuses recorded)
Sep  6 10:20:55.461: INFO: 	Container metrics-server ready: true, restart count 0
Sep  6 10:20:55.461: INFO: sonobuoy-e2e-job-8d290fb976034269 from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 10:20:55.461: INFO: 	Container e2e ready: true, restart count 0
Sep  6 10:20:55.461: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:20:55.461: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-lb9kk from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 10:20:55.461: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:20:55.461: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17123e3fa2dbb0e2], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17123e3fa3b051a3], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:20:56.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5590" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":6,"skipped":76,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:20:56.500: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Sep  6 10:21:16.558: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8612 PodName:var-expansion-fa00681e-43cb-4ea4-a4bc-1509f6243775 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:21:16.559: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: test for file in mounted path
Sep  6 10:21:16.635: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8612 PodName:var-expansion-fa00681e-43cb-4ea4-a4bc-1509f6243775 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:21:16.635: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: updating the annotation value
Sep  6 10:21:17.220: INFO: Successfully updated pod "var-expansion-fa00681e-43cb-4ea4-a4bc-1509f6243775"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Sep  6 10:21:17.222: INFO: Deleting pod "var-expansion-fa00681e-43cb-4ea4-a4bc-1509f6243775" in namespace "var-expansion-8612"
Sep  6 10:21:17.234: INFO: Wait up to 5m0s for pod "var-expansion-fa00681e-43cb-4ea4-a4bc-1509f6243775" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:21:51.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8612" for this suite.

• [SLOW TEST:54.751 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":7,"skipped":88,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:21:51.251: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0906 10:22:31.322058      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0906 10:22:31.322332      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0906 10:22:31.322401      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep  6 10:22:31.322: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  6 10:22:31.323: INFO: Deleting pod "simpletest.rc-4ftdc" in namespace "gc-5680"
Sep  6 10:22:31.334: INFO: Deleting pod "simpletest.rc-5z2px" in namespace "gc-5680"
Sep  6 10:22:31.346: INFO: Deleting pod "simpletest.rc-77jb7" in namespace "gc-5680"
Sep  6 10:22:31.356: INFO: Deleting pod "simpletest.rc-lhhx7" in namespace "gc-5680"
Sep  6 10:22:31.387: INFO: Deleting pod "simpletest.rc-lqq2q" in namespace "gc-5680"
Sep  6 10:22:31.404: INFO: Deleting pod "simpletest.rc-qtzrr" in namespace "gc-5680"
Sep  6 10:22:31.437: INFO: Deleting pod "simpletest.rc-x2ffg" in namespace "gc-5680"
Sep  6 10:22:31.464: INFO: Deleting pod "simpletest.rc-xgh87" in namespace "gc-5680"
Sep  6 10:22:31.511: INFO: Deleting pod "simpletest.rc-xwgvx" in namespace "gc-5680"
Sep  6 10:22:31.533: INFO: Deleting pod "simpletest.rc-z89tv" in namespace "gc-5680"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:22:31.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5680" for this suite.

• [SLOW TEST:40.340 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":8,"skipped":94,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:22:31.592: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 10:22:31.685: INFO: Waiting up to 5m0s for pod "downwardapi-volume-617564d2-7676-448a-9c19-c68622bed6c3" in namespace "downward-api-9888" to be "Succeeded or Failed"
Sep  6 10:22:31.711: INFO: Pod "downwardapi-volume-617564d2-7676-448a-9c19-c68622bed6c3": Phase="Pending", Reason="", readiness=false. Elapsed: 26.374228ms
Sep  6 10:22:33.715: INFO: Pod "downwardapi-volume-617564d2-7676-448a-9c19-c68622bed6c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029661303s
STEP: Saw pod success
Sep  6 10:22:33.715: INFO: Pod "downwardapi-volume-617564d2-7676-448a-9c19-c68622bed6c3" satisfied condition "Succeeded or Failed"
Sep  6 10:22:33.721: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-617564d2-7676-448a-9c19-c68622bed6c3 container client-container: <nil>
STEP: delete the pod
Sep  6 10:22:33.740: INFO: Waiting for pod downwardapi-volume-617564d2-7676-448a-9c19-c68622bed6c3 to disappear
Sep  6 10:22:33.744: INFO: Pod downwardapi-volume-617564d2-7676-448a-9c19-c68622bed6c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:22:33.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9888" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":9,"skipped":99,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:22:33.753: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Sep  6 10:22:33.776: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-30 proxy --unix-socket=/tmp/kubectl-proxy-unix650737346/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:22:33.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-30" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":10,"skipped":116,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:22:33.836: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:22:33.864: INFO: Creating deployment "webserver-deployment"
Sep  6 10:22:33.870: INFO: Waiting for observed generation 1
Sep  6 10:22:35.881: INFO: Waiting for all required pods to come up
Sep  6 10:22:35.885: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep  6 10:23:55.895: INFO: Waiting for deployment "webserver-deployment" to complete
Sep  6 10:23:55.900: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep  6 10:23:55.907: INFO: Updating deployment webserver-deployment
Sep  6 10:23:55.907: INFO: Waiting for observed generation 2
Sep  6 10:23:57.912: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  6 10:23:57.916: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  6 10:23:57.918: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  6 10:23:57.925: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  6 10:23:57.925: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  6 10:23:57.928: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  6 10:23:57.934: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep  6 10:23:57.934: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep  6 10:23:57.944: INFO: Updating deployment webserver-deployment
Sep  6 10:23:57.944: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep  6 10:23:57.950: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  6 10:23:59.982: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep  6 10:24:00.013: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5069 /apis/apps/v1/namespaces/deployment-5069/deployments/webserver-deployment 89d32c6e-0fcb-454e-8c8f-a43cb40003c5 3667 3 2022-09-06 10:22:33 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-09-06 10:22:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-09-06 10:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028c0e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-09-06 10:23:57 +0000 UTC,LastTransitionTime:2022-09-06 10:23:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2022-09-06 10:23:58 +0000 UTC,LastTransitionTime:2022-09-06 10:22:33 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep  6 10:24:00.019: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-5069 /apis/apps/v1/namespaces/deployment-5069/replicasets/webserver-deployment-795d758f88 e422e222-8e1e-4f08-b032-5b864546155d 3664 3 2022-09-06 10:23:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 89d32c6e-0fcb-454e-8c8f-a43cb40003c5 0xc0028c12f7 0xc0028c12f8}] []  [{kube-controller-manager Update apps/v1 2022-09-06 10:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89d32c6e-0fcb-454e-8c8f-a43cb40003c5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028c1378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:24:00.019: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep  6 10:24:00.019: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-5069 /apis/apps/v1/namespaces/deployment-5069/replicasets/webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 3654 3 2022-09-06 10:22:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 89d32c6e-0fcb-454e-8c8f-a43cb40003c5 0xc0028c13d7 0xc0028c13d8}] []  [{kube-controller-manager Update apps/v1 2022-09-06 10:22:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89d32c6e-0fcb-454e-8c8f-a43cb40003c5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028c1448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:24:00.035: INFO: Pod "webserver-deployment-795d758f88-2c87b" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2c87b webserver-deployment-795d758f88- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-795d758f88-2c87b 473dc713-276b-49dc-99ea-37161711f08a 3669 0 2022-09-06 10:23:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e422e222-8e1e-4f08-b032-5b864546155d 0xc002c87880 0xc002c87881}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e422e222-8e1e-4f08-b032-5b864546155d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114011,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.11,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.035: INFO: Pod "webserver-deployment-795d758f88-2xhvj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2xhvj webserver-deployment-795d758f88- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-795d758f88-2xhvj 89048ffe-ec6d-41b6-826b-ceffd4c89091 3661 0 2022-09-06 10:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e422e222-8e1e-4f08-b032-5b864546155d 0xc002c87a27 0xc002c87a28}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e422e222-8e1e-4f08-b032-5b864546155d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114013,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.13,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.035: INFO: Pod "webserver-deployment-795d758f88-4bjhs" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4bjhs webserver-deployment-795d758f88- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-795d758f88-4bjhs 8301dae4-2cc0-45e0-9bd7-2d045e3d2d29 3689 0 2022-09-06 10:23:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e422e222-8e1e-4f08-b032-5b864546155d 0xc002c87bd7 0xc002c87bd8}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e422e222-8e1e-4f08-b032-5b864546155d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114012,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.12,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.035: INFO: Pod "webserver-deployment-795d758f88-6k4d5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6k4d5 webserver-deployment-795d758f88- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-795d758f88-6k4d5 41e25907-0330-4096-8c20-bd04050e093a 3558 0 2022-09-06 10:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e422e222-8e1e-4f08-b032-5b864546155d 0xc002c87d87 0xc002c87d88}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e422e222-8e1e-4f08-b032-5b864546155d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114012,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.12,PodIP:,StartTime:2022-09-06 10:23:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.035: INFO: Pod "webserver-deployment-795d758f88-cn27d" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-cn27d webserver-deployment-795d758f88- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-795d758f88-cn27d 2a877ae4-b019-4637-9412-d6ea3408ea6c 3652 0 2022-09-06 10:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e422e222-8e1e-4f08-b032-5b864546155d 0xc002c87f37 0xc002c87f38}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e422e222-8e1e-4f08-b032-5b864546155d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114011,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.11,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.035: INFO: Pod "webserver-deployment-795d758f88-hdj9l" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hdj9l webserver-deployment-795d758f88- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-795d758f88-hdj9l a0ec2ee6-f0ae-4b6e-b518-fb740d3558a1 3532 0 2022-09-06 10:23:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e422e222-8e1e-4f08-b032-5b864546155d 0xc000266437 0xc000266438}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e422e222-8e1e-4f08-b032-5b864546155d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114011,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.11,PodIP:,StartTime:2022-09-06 10:23:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.035: INFO: Pod "webserver-deployment-795d758f88-rb6kt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rb6kt webserver-deployment-795d758f88- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-795d758f88-rb6kt e710527f-a246-422f-b952-9faeffa4c7b3 3678 0 2022-09-06 10:23:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e422e222-8e1e-4f08-b032-5b864546155d 0xc000266a57 0xc000266a58}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e422e222-8e1e-4f08-b032-5b864546155d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114012,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.12,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.035: INFO: Pod "webserver-deployment-795d758f88-rcncj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rcncj webserver-deployment-795d758f88- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-795d758f88-rcncj 332830be-d937-4117-82b5-f9f0ef43ca32 3546 0 2022-09-06 10:23:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e422e222-8e1e-4f08-b032-5b864546155d 0xc000266fa7 0xc000266fa8}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e422e222-8e1e-4f08-b032-5b864546155d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114012,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.12,PodIP:,StartTime:2022-09-06 10:23:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.035: INFO: Pod "webserver-deployment-795d758f88-rgw79" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rgw79 webserver-deployment-795d758f88- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-795d758f88-rgw79 34370ff0-6d05-4561-a3c0-ebc7e47ac30b 3548 0 2022-09-06 10:23:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e422e222-8e1e-4f08-b032-5b864546155d 0xc000267a37 0xc000267a38}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e422e222-8e1e-4f08-b032-5b864546155d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114013,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.13,PodIP:,StartTime:2022-09-06 10:23:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.035: INFO: Pod "webserver-deployment-795d758f88-s2sxn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-s2sxn webserver-deployment-795d758f88- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-795d758f88-s2sxn f4b03845-6906-49c6-8392-f1a4de279a73 3668 0 2022-09-06 10:23:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e422e222-8e1e-4f08-b032-5b864546155d 0xc00298c227 0xc00298c228}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e422e222-8e1e-4f08-b032-5b864546155d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114013,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.13,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.036: INFO: Pod "webserver-deployment-795d758f88-vfd59" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vfd59 webserver-deployment-795d758f88- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-795d758f88-vfd59 9124a020-44f8-4fea-af8e-c2dba7ea4db2 3675 0 2022-09-06 10:23:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e422e222-8e1e-4f08-b032-5b864546155d 0xc00298c417 0xc00298c418}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e422e222-8e1e-4f08-b032-5b864546155d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114011,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.11,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.036: INFO: Pod "webserver-deployment-795d758f88-w9kff" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-w9kff webserver-deployment-795d758f88- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-795d758f88-w9kff 30869e62-4d5d-46fb-8478-9b86c6d37b56 3560 0 2022-09-06 10:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e422e222-8e1e-4f08-b032-5b864546155d 0xc00298c667 0xc00298c668}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e422e222-8e1e-4f08-b032-5b864546155d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114013,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.13,PodIP:,StartTime:2022-09-06 10:23:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.036: INFO: Pod "webserver-deployment-795d758f88-xjhdt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xjhdt webserver-deployment-795d758f88- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-795d758f88-xjhdt 045031e0-b493-43af-b6e8-95b16c0cec89 3666 0 2022-09-06 10:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e422e222-8e1e-4f08-b032-5b864546155d 0xc00298c817 0xc00298c818}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e422e222-8e1e-4f08-b032-5b864546155d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114012,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.12,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.036: INFO: Pod "webserver-deployment-dd94f59b7-59pvj" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-59pvj webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-59pvj 826dcc5c-e828-4010-b8dc-39769c2144f9 3620 0 2022-09-06 10:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298c9c0 0xc00298c9c1}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114011,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.11,PodIP:,StartTime:2022-09-06 10:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.036: INFO: Pod "webserver-deployment-dd94f59b7-69gcj" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-69gcj webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-69gcj 4d83fd38-bca1-4bf6-b313-ace6e82fea22 3681 0 2022-09-06 10:23:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298cb40 0xc00298cb41}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114012,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.12,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.036: INFO: Pod "webserver-deployment-dd94f59b7-9tvd8" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9tvd8 webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-9tvd8 ef5a4c72-b4ea-47b2-86a9-d5da0b863230 3685 0 2022-09-06 10:23:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298ccc0 0xc00298ccc1}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114012,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.12,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.037: INFO: Pod "webserver-deployment-dd94f59b7-brtwz" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-brtwz webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-brtwz 7bac2a76-d86d-46a9-9b6e-d440a5d8435d 3377 0 2022-09-06 10:22:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298ce40 0xc00298ce41}] []  [{kube-controller-manager Update v1 2022-09-06 10:22:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114011,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.11,PodIP:10.244.0.11,StartTime:2022-09-06 10:22:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-06 10:23:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://44bde2cfc7b6f47187811c17356189cbcdf0984b20f9d49f413cb704055f7fb5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.037: INFO: Pod "webserver-deployment-dd94f59b7-cjqzb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cjqzb webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-cjqzb cb757bb3-3988-4aec-b70e-079f2176af48 3618 0 2022-09-06 10:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298cfe0 0xc00298cfe1}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114012,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.12,PodIP:,StartTime:2022-09-06 10:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.037: INFO: Pod "webserver-deployment-dd94f59b7-ktl4m" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ktl4m webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-ktl4m c21d5b45-6882-482d-8bb2-fc2eb2fbc17c 3630 0 2022-09-06 10:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298d160 0xc00298d161}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114013,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.13,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.037: INFO: Pod "webserver-deployment-dd94f59b7-n85gd" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-n85gd webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-n85gd a091acc3-930b-481a-bdba-d204bf4c7a16 3341 0 2022-09-06 10:22:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298d2e0 0xc00298d2e1}] []  [{kube-controller-manager Update v1 2022-09-06 10:22:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114013,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.13,PodIP:10.244.2.9,StartTime:2022-09-06 10:22:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-06 10:23:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://69652737a84877d08583cb1b915736588111602cf597892a8c44defc577bf354,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.037: INFO: Pod "webserver-deployment-dd94f59b7-n9s4r" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-n9s4r webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-n9s4r 6bd45b53-3cbe-4494-85ef-2d19222336f4 3589 0 2022-09-06 10:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298d480 0xc00298d481}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114012,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.12,PodIP:,StartTime:2022-09-06 10:23:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.037: INFO: Pod "webserver-deployment-dd94f59b7-pmfr9" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pmfr9 webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-pmfr9 63f0515f-6a28-433a-b3cd-c6f2aeeae72b 3658 0 2022-09-06 10:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298d600 0xc00298d601}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114013,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.13,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.037: INFO: Pod "webserver-deployment-dd94f59b7-pqxjc" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pqxjc webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-pqxjc f28b0367-972a-4182-b4ca-264287acb064 3399 0 2022-09-06 10:22:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298d780 0xc00298d781}] []  [{kube-controller-manager Update v1 2022-09-06 10:22:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114013,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.13,PodIP:10.244.2.10,StartTime:2022-09-06 10:22:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-06 10:23:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://31351c2d10302eb7221880d43a2d1f4a80452b52a16c68904ef49a13c3a6df2b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.037: INFO: Pod "webserver-deployment-dd94f59b7-qkv6l" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qkv6l webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-qkv6l 4795f2a1-f114-4179-b29c-de7fce3db004 3438 0 2022-09-06 10:22:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298d920 0xc00298d921}] []  [{kube-controller-manager Update v1 2022-09-06 10:22:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114011,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.11,PodIP:10.244.0.12,StartTime:2022-09-06 10:22:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-06 10:23:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://efe1607e4c61f68748516dd668a94ccea435a5daa991639c03a8c829d44cdfc3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.037: INFO: Pod "webserver-deployment-dd94f59b7-sbj7f" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sbj7f webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-sbj7f 1e3bfe04-89a2-457c-ab79-4c9cf65722b9 3663 0 2022-09-06 10:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298dac0 0xc00298dac1}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114011,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.11,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.038: INFO: Pod "webserver-deployment-dd94f59b7-vq2v5" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vq2v5 webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-vq2v5 9282209b-d1b6-492c-8afe-285d1ecb69f6 3331 0 2022-09-06 10:22:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298dc40 0xc00298dc41}] []  [{kube-controller-manager Update v1 2022-09-06 10:22:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114013,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.13,PodIP:10.244.2.8,StartTime:2022-09-06 10:22:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-06 10:23:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://86388ae3404e888f1cd0720252d30a772b245fe7e7ffd021e5c76a5c40b4cb42,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.038: INFO: Pod "webserver-deployment-dd94f59b7-vsrtg" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vsrtg webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-vsrtg 3a01ec57-ca66-4599-9256-52c46b591fb6 3357 0 2022-09-06 10:22:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298dde0 0xc00298dde1}] []  [{kube-controller-manager Update v1 2022-09-06 10:22:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114012,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.12,PodIP:10.244.1.9,StartTime:2022-09-06 10:22:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-06 10:23:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c1b03d9c7dfb5e11d163f0247e11a73a6dde848658995f6edb85b875c5771c7e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.047: INFO: Pod "webserver-deployment-dd94f59b7-w5qjz" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-w5qjz webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-w5qjz 561b20d7-0c4f-402e-b9cc-b6e1d7ae9670 3410 0 2022-09-06 10:22:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc00298df80 0xc00298df81}] []  [{kube-controller-manager Update v1 2022-09-06 10:22:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114012,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.12,PodIP:10.244.1.10,StartTime:2022-09-06 10:22:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-06 10:23:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://91abc5fe1473d867259a79eca5e3f2ed13fbf9f3a62320ad248ee7cb3b20bf3e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.048: INFO: Pod "webserver-deployment-dd94f59b7-w9dgp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-w9dgp webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-w9dgp 75222e2f-4a22-4b48-bdba-90b900b7a1a2 3680 0 2022-09-06 10:23:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc002332120 0xc002332121}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114011,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.11,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.048: INFO: Pod "webserver-deployment-dd94f59b7-wxpd8" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wxpd8 webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-wxpd8 79106bec-52f3-4adb-a996-44d3c16860b0 3649 0 2022-09-06 10:23:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc0023322a0 0xc0023322a1}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114012,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.12,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.048: INFO: Pod "webserver-deployment-dd94f59b7-xlzbx" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xlzbx webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-xlzbx 0116a2c0-becf-4ecc-8850-c72b16e7b8f0 3673 0 2022-09-06 10:23:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc002332430 0xc002332431}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114013,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.13,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.048: INFO: Pod "webserver-deployment-dd94f59b7-z9m6m" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z9m6m webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-z9m6m d2cbcf06-a2db-4c04-986b-3acf0b7db9d1 3315 0 2022-09-06 10:22:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc0023325b0 0xc0023325b1}] []  [{kube-controller-manager Update v1 2022-09-06 10:22:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:22:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114011,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:22:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.11,PodIP:10.244.0.10,StartTime:2022-09-06 10:22:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-06 10:22:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c3ae79e96a38d2beff0f009714f8bcf5ac00441676b5e5ee4997f655b0e9dc6d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 10:24:00.048: INFO: Pod "webserver-deployment-dd94f59b7-zp88p" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zp88p webserver-deployment-dd94f59b7- deployment-5069 /api/v1/namespaces/deployment-5069/pods/webserver-deployment-dd94f59b7-zp88p 3b033d79-fd9b-44c0-a8f2-c8be3c58e1e8 3672 0 2022-09-06 10:23:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ec794d4c-58a6-48bc-928a-66e6de27c827 0xc002332750 0xc002332751}] []  [{kube-controller-manager Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec794d4c-58a6-48bc-928a-66e6de27c827\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzxzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzxzz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzxzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114011,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:23:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.11,PodIP:,StartTime:2022-09-06 10:23:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:24:00.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5069" for this suite.

• [SLOW TEST:86.240 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":11,"skipped":148,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:24:00.076: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Sep  6 10:24:58.167: INFO: Pod pod-hostip-ec540816-b0d0-4708-aafb-9935bf49d0d6 has hostIP: 172.16.114.11
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:24:58.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8899" for this suite.

• [SLOW TEST:58.100 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":12,"skipped":158,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:24:58.177: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep  6 10:24:58.209: INFO: Waiting up to 5m0s for pod "downward-api-cf20ec47-8c76-4206-80c1-a8c3b9207cf9" in namespace "downward-api-6997" to be "Succeeded or Failed"
Sep  6 10:24:58.221: INFO: Pod "downward-api-cf20ec47-8c76-4206-80c1-a8c3b9207cf9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.511877ms
Sep  6 10:25:00.225: INFO: Pod "downward-api-cf20ec47-8c76-4206-80c1-a8c3b9207cf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016467271s
STEP: Saw pod success
Sep  6 10:25:00.225: INFO: Pod "downward-api-cf20ec47-8c76-4206-80c1-a8c3b9207cf9" satisfied condition "Succeeded or Failed"
Sep  6 10:25:00.229: INFO: Trying to get logs from node vm114012 pod downward-api-cf20ec47-8c76-4206-80c1-a8c3b9207cf9 container dapi-container: <nil>
STEP: delete the pod
Sep  6 10:25:00.252: INFO: Waiting for pod downward-api-cf20ec47-8c76-4206-80c1-a8c3b9207cf9 to disappear
Sep  6 10:25:00.255: INFO: Pod downward-api-cf20ec47-8c76-4206-80c1-a8c3b9207cf9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:25:00.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6997" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":13,"skipped":168,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:25:00.265: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Sep  6 10:25:00.347: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:25:13.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8416" for this suite.

• [SLOW TEST:12.973 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":14,"skipped":174,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:25:13.239: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 10:25:13.412: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 10:25:16.426: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:25:16.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7031" for this suite.
STEP: Destroying namespace "webhook-7031-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":15,"skipped":178,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:25:16.616: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-9c16a6b0-8f1e-4025-8e94-ede02dbb3044
STEP: Creating a pod to test consume configMaps
Sep  6 10:25:16.676: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-87d892d8-1417-492a-bf93-66ff330fcb80" in namespace "projected-38" to be "Succeeded or Failed"
Sep  6 10:25:16.681: INFO: Pod "pod-projected-configmaps-87d892d8-1417-492a-bf93-66ff330fcb80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.934582ms
Sep  6 10:25:18.685: INFO: Pod "pod-projected-configmaps-87d892d8-1417-492a-bf93-66ff330fcb80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009038858s
STEP: Saw pod success
Sep  6 10:25:18.685: INFO: Pod "pod-projected-configmaps-87d892d8-1417-492a-bf93-66ff330fcb80" satisfied condition "Succeeded or Failed"
Sep  6 10:25:18.690: INFO: Trying to get logs from node vm114011 pod pod-projected-configmaps-87d892d8-1417-492a-bf93-66ff330fcb80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 10:25:18.706: INFO: Waiting for pod pod-projected-configmaps-87d892d8-1417-492a-bf93-66ff330fcb80 to disappear
Sep  6 10:25:18.710: INFO: Pod pod-projected-configmaps-87d892d8-1417-492a-bf93-66ff330fcb80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:25:18.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-38" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":16,"skipped":192,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:25:18.719: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:25:18.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6690" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":17,"skipped":198,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:25:18.757: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:25:18.802: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  6 10:25:23.805: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 10:25:23.806: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  6 10:25:25.810: INFO: Creating deployment "test-rollover-deployment"
Sep  6 10:25:25.817: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  6 10:25:27.823: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  6 10:25:27.828: INFO: Ensure that both replica sets have 1 created replica
Sep  6 10:25:27.833: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  6 10:25:27.840: INFO: Updating deployment test-rollover-deployment
Sep  6 10:25:27.840: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  6 10:25:29.847: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  6 10:25:29.853: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  6 10:25:29.858: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:25:29.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056727, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:25:31.865: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:25:31.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056727, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:25:33.867: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:25:33.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056727, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:25:35.864: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:25:35.864: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056727, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:25:37.867: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:25:37.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056727, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:25:39.871: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:25:39.871: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056727, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:25:41.865: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:25:41.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056727, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:25:43.867: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:25:43.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056742, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:25:45.867: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:25:45.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056742, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:25:47.867: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:25:47.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056742, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:25:49.866: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:25:49.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056742, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:25:51.869: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 10:25:51.869: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056742, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798056725, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 10:25:53.867: INFO: 
Sep  6 10:25:53.867: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep  6 10:25:53.875: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9060 /apis/apps/v1/namespaces/deployment-9060/deployments/test-rollover-deployment 7f3910a5-9fdb-49d9-a80a-738939335a67 4499 2 2022-09-06 10:25:25 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-09-06 10:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-09-06 10:25:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0063b5a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-09-06 10:25:25 +0000 UTC,LastTransitionTime:2022-09-06 10:25:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2022-09-06 10:25:52 +0000 UTC,LastTransitionTime:2022-09-06 10:25:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 10:25:53.879: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-9060 /apis/apps/v1/namespaces/deployment-9060/replicasets/test-rollover-deployment-5797c7764 0754f6b3-7cdb-45fd-a8c9-cf783c084d04 4488 2 2022-09-06 10:25:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 7f3910a5-9fdb-49d9-a80a-738939335a67 0xc0063d2190 0xc0063d2191}] []  [{kube-controller-manager Update apps/v1 2022-09-06 10:25:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f3910a5-9fdb-49d9-a80a-738939335a67\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0063d22f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:25:53.879: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  6 10:25:53.879: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9060 /apis/apps/v1/namespaces/deployment-9060/replicasets/test-rollover-controller a813383b-753b-4b03-92c5-efa56fb08e39 4498 2 2022-09-06 10:25:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 7f3910a5-9fdb-49d9-a80a-738939335a67 0xc0063d200f 0xc0063d2050}] []  [{e2e.test Update apps/v1 2022-09-06 10:25:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-09-06 10:25:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f3910a5-9fdb-49d9-a80a-738939335a67\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0063d2118 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:25:53.879: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-9060 /apis/apps/v1/namespaces/deployment-9060/replicasets/test-rollover-deployment-78bc8b888c c44591da-7853-4d53-b3ec-e27157d0a746 4409 2 2022-09-06 10:25:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 7f3910a5-9fdb-49d9-a80a-738939335a67 0xc0063d2367 0xc0063d2368}] []  [{kube-controller-manager Update apps/v1 2022-09-06 10:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f3910a5-9fdb-49d9-a80a-738939335a67\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0063d2478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:25:53.883: INFO: Pod "test-rollover-deployment-5797c7764-xt6rs" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-xt6rs test-rollover-deployment-5797c7764- deployment-9060 /api/v1/namespaces/deployment-9060/pods/test-rollover-deployment-5797c7764-xt6rs dacb3472-727d-4884-896d-201311df8c7b 4461 0 2022-09-06 10:25:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 0754f6b3-7cdb-45fd-a8c9-cf783c084d04 0xc0063d2ce0 0xc0063d2ce1}] []  [{kube-controller-manager Update v1 2022-09-06 10:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0754f6b3-7cdb-45fd-a8c9-cf783c084d04\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:25:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9zpqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9zpqm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9zpqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114012,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:25:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:25:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.12,PodIP:10.244.1.26,StartTime:2022-09-06 10:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-06 10:25:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://dfb1ea31528b7c69e8e0dcdc9e21aac40fc60c710cf66215d0e79b07a62e9279,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:25:53.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9060" for this suite.

• [SLOW TEST:35.140 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":18,"skipped":227,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:25:53.898: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7286
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-7286
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7286
Sep  6 10:25:53.947: INFO: Found 0 stateful pods, waiting for 1
Sep  6 10:26:03.951: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep  6 10:26:03.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 10:26:04.214: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 10:26:04.215: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 10:26:04.215: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 10:26:04.218: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  6 10:26:14.222: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 10:26:14.222: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 10:26:14.232: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep  6 10:26:14.232: INFO: ss-0  vm114011  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:25:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:25:53 +0000 UTC  }]
Sep  6 10:26:14.232: INFO: 
Sep  6 10:26:14.232: INFO: StatefulSet ss has not reached scale 3, at 1
Sep  6 10:26:15.236: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997659249s
Sep  6 10:26:16.241: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993452896s
Sep  6 10:26:17.245: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988383925s
Sep  6 10:26:18.248: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984686449s
Sep  6 10:26:19.252: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981212086s
Sep  6 10:26:20.255: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978250897s
Sep  6 10:26:21.262: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973447014s
Sep  6 10:26:22.267: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967139943s
Sep  6 10:26:23.272: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.804766ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7286
Sep  6 10:26:24.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:26:24.409: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  6 10:26:24.409: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 10:26:24.409: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 10:26:24.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:26:24.565: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  6 10:26:24.565: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 10:26:24.565: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 10:26:24.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:26:24.722: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  6 10:26:24.722: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 10:26:24.722: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 10:26:24.727: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 10:26:24.727: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 10:26:24.727: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep  6 10:26:24.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 10:26:24.899: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 10:26:24.899: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 10:26:24.899: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 10:26:24.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 10:26:25.051: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 10:26:25.051: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 10:26:25.051: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 10:26:25.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 10:26:25.202: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 10:26:25.202: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 10:26:25.202: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 10:26:25.202: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 10:26:25.205: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  6 10:26:35.212: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 10:26:35.212: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 10:26:35.212: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 10:26:35.222: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep  6 10:26:35.222: INFO: ss-0  vm114011  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:25:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:25:53 +0000 UTC  }]
Sep  6 10:26:35.222: INFO: ss-1  vm114012  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  }]
Sep  6 10:26:35.222: INFO: ss-2  vm114013  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  }]
Sep  6 10:26:35.222: INFO: 
Sep  6 10:26:35.222: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 10:26:36.226: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep  6 10:26:36.226: INFO: ss-0  vm114011  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:25:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:25:53 +0000 UTC  }]
Sep  6 10:26:36.226: INFO: ss-1  vm114012  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  }]
Sep  6 10:26:36.226: INFO: ss-2  vm114013  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  }]
Sep  6 10:26:36.226: INFO: 
Sep  6 10:26:36.226: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 10:26:37.230: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep  6 10:26:37.230: INFO: ss-1  vm114012  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  }]
Sep  6 10:26:37.230: INFO: 
Sep  6 10:26:37.230: INFO: StatefulSet ss has not reached scale 0, at 1
Sep  6 10:26:38.234: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep  6 10:26:38.234: INFO: ss-1  vm114012  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  }]
Sep  6 10:26:38.234: INFO: 
Sep  6 10:26:38.234: INFO: StatefulSet ss has not reached scale 0, at 1
Sep  6 10:26:39.237: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep  6 10:26:39.237: INFO: ss-1  vm114012  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  }]
Sep  6 10:26:39.237: INFO: 
Sep  6 10:26:39.237: INFO: StatefulSet ss has not reached scale 0, at 1
Sep  6 10:26:40.241: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep  6 10:26:40.241: INFO: ss-1  vm114012  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  }]
Sep  6 10:26:40.241: INFO: 
Sep  6 10:26:40.241: INFO: StatefulSet ss has not reached scale 0, at 1
Sep  6 10:26:41.245: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep  6 10:26:41.245: INFO: ss-1  vm114012  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  }]
Sep  6 10:26:41.245: INFO: 
Sep  6 10:26:41.245: INFO: StatefulSet ss has not reached scale 0, at 1
Sep  6 10:26:42.249: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep  6 10:26:42.249: INFO: ss-1  vm114012  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  }]
Sep  6 10:26:42.249: INFO: 
Sep  6 10:26:42.249: INFO: StatefulSet ss has not reached scale 0, at 1
Sep  6 10:26:43.252: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep  6 10:26:43.253: INFO: ss-1  vm114012  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  }]
Sep  6 10:26:43.253: INFO: 
Sep  6 10:26:43.253: INFO: StatefulSet ss has not reached scale 0, at 1
Sep  6 10:26:44.257: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Sep  6 10:26:44.257: INFO: ss-1  vm114012  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-09-06 10:26:14 +0000 UTC  }]
Sep  6 10:26:44.257: INFO: 
Sep  6 10:26:44.257: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7286
Sep  6 10:26:45.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:26:45.342: INFO: rc: 1
Sep  6 10:26:45.342: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Sep  6 10:26:55.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:26:55.403: INFO: rc: 1
Sep  6 10:26:55.403: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:27:05.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:27:05.482: INFO: rc: 1
Sep  6 10:27:05.483: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:27:15.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:27:15.556: INFO: rc: 1
Sep  6 10:27:15.556: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:27:25.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:27:25.621: INFO: rc: 1
Sep  6 10:27:25.621: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:27:35.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:27:35.681: INFO: rc: 1
Sep  6 10:27:35.681: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:27:45.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:27:45.742: INFO: rc: 1
Sep  6 10:27:45.743: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:27:55.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:27:55.806: INFO: rc: 1
Sep  6 10:27:55.806: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:28:05.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:28:05.864: INFO: rc: 1
Sep  6 10:28:05.864: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:28:15.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:28:15.939: INFO: rc: 1
Sep  6 10:28:15.939: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:28:25.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:28:25.997: INFO: rc: 1
Sep  6 10:28:25.997: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:28:35.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:28:36.057: INFO: rc: 1
Sep  6 10:28:36.058: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:28:46.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:28:46.128: INFO: rc: 1
Sep  6 10:28:46.128: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:28:56.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:28:56.187: INFO: rc: 1
Sep  6 10:28:56.187: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:29:06.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:29:06.251: INFO: rc: 1
Sep  6 10:29:06.251: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:29:16.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:29:16.319: INFO: rc: 1
Sep  6 10:29:16.319: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:29:26.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:29:26.388: INFO: rc: 1
Sep  6 10:29:26.388: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:29:36.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:29:36.459: INFO: rc: 1
Sep  6 10:29:36.459: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:29:46.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:29:46.520: INFO: rc: 1
Sep  6 10:29:46.520: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:29:56.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:29:56.584: INFO: rc: 1
Sep  6 10:29:56.584: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:30:06.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:30:06.651: INFO: rc: 1
Sep  6 10:30:06.651: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:30:16.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:30:16.731: INFO: rc: 1
Sep  6 10:30:16.731: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:30:26.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:30:26.801: INFO: rc: 1
Sep  6 10:30:26.801: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:30:36.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:30:36.863: INFO: rc: 1
Sep  6 10:30:36.863: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:30:46.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:30:46.936: INFO: rc: 1
Sep  6 10:30:46.936: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:30:56.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:30:56.999: INFO: rc: 1
Sep  6 10:30:56.999: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:31:07.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:31:07.071: INFO: rc: 1
Sep  6 10:31:07.071: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:31:17.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:31:17.143: INFO: rc: 1
Sep  6 10:31:17.143: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:31:27.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:31:27.206: INFO: rc: 1
Sep  6 10:31:27.206: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:31:37.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:31:37.274: INFO: rc: 1
Sep  6 10:31:37.274: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep  6 10:31:47.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7286 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:31:47.348: INFO: rc: 1
Sep  6 10:31:47.349: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Sep  6 10:31:47.349: INFO: Scaling statefulset ss to 0
Sep  6 10:31:47.364: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  6 10:31:47.368: INFO: Deleting all statefulset in ns statefulset-7286
Sep  6 10:31:47.371: INFO: Scaling statefulset ss to 0
Sep  6 10:31:47.380: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 10:31:47.381: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:31:47.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7286" for this suite.

• [SLOW TEST:353.512 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":19,"skipped":238,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:31:47.411: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:31:47.445: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep  6 10:31:49.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-4033 --namespace=crd-publish-openapi-4033 create -f -'
Sep  6 10:31:49.447: INFO: stderr: ""
Sep  6 10:31:49.447: INFO: stdout: "e2e-test-crd-publish-openapi-3180-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  6 10:31:49.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-4033 --namespace=crd-publish-openapi-4033 delete e2e-test-crd-publish-openapi-3180-crds test-foo'
Sep  6 10:31:49.514: INFO: stderr: ""
Sep  6 10:31:49.514: INFO: stdout: "e2e-test-crd-publish-openapi-3180-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep  6 10:31:49.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-4033 --namespace=crd-publish-openapi-4033 apply -f -'
Sep  6 10:31:49.685: INFO: stderr: ""
Sep  6 10:31:49.685: INFO: stdout: "e2e-test-crd-publish-openapi-3180-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  6 10:31:49.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-4033 --namespace=crd-publish-openapi-4033 delete e2e-test-crd-publish-openapi-3180-crds test-foo'
Sep  6 10:31:49.757: INFO: stderr: ""
Sep  6 10:31:49.757: INFO: stdout: "e2e-test-crd-publish-openapi-3180-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep  6 10:31:49.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-4033 --namespace=crd-publish-openapi-4033 create -f -'
Sep  6 10:31:49.902: INFO: rc: 1
Sep  6 10:31:49.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-4033 --namespace=crd-publish-openapi-4033 apply -f -'
Sep  6 10:31:50.045: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep  6 10:31:50.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-4033 --namespace=crd-publish-openapi-4033 create -f -'
Sep  6 10:31:50.170: INFO: rc: 1
Sep  6 10:31:50.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-4033 --namespace=crd-publish-openapi-4033 apply -f -'
Sep  6 10:31:50.305: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep  6 10:31:50.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-4033 explain e2e-test-crd-publish-openapi-3180-crds'
Sep  6 10:31:50.465: INFO: stderr: ""
Sep  6 10:31:50.465: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3180-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep  6 10:31:50.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-4033 explain e2e-test-crd-publish-openapi-3180-crds.metadata'
Sep  6 10:31:50.618: INFO: stderr: ""
Sep  6 10:31:50.618: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3180-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep  6 10:31:50.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-4033 explain e2e-test-crd-publish-openapi-3180-crds.spec'
Sep  6 10:31:50.776: INFO: stderr: ""
Sep  6 10:31:50.776: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3180-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep  6 10:31:50.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-4033 explain e2e-test-crd-publish-openapi-3180-crds.spec.bars'
Sep  6 10:31:50.933: INFO: stderr: ""
Sep  6 10:31:50.933: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3180-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep  6 10:31:50.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-4033 explain e2e-test-crd-publish-openapi-3180-crds.spec.bars2'
Sep  6 10:31:51.084: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:31:52.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4033" for this suite.

• [SLOW TEST:5.437 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":20,"skipped":269,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:31:52.850: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1512
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep  6 10:31:52.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-9567 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Sep  6 10:31:52.997: INFO: stderr: ""
Sep  6 10:31:52.997: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Sep  6 10:31:53.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-9567 delete pods e2e-test-httpd-pod'
Sep  6 10:31:59.903: INFO: stderr: ""
Sep  6 10:31:59.903: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:31:59.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9567" for this suite.

• [SLOW TEST:7.062 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1509
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":21,"skipped":306,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:31:59.912: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1869
STEP: creating service affinity-clusterip in namespace services-1869
STEP: creating replication controller affinity-clusterip in namespace services-1869
I0906 10:31:59.963784      20 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-1869, replica count: 3
I0906 10:32:03.014566      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 10:32:06.015060      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 10:32:09.015310      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 10:32:12.015712      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 10:32:12.021: INFO: Creating new exec pod
Sep  6 10:32:15.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-1869 exec execpod-affinitynsdgb -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Sep  6 10:32:15.192: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep  6 10:32:15.192: INFO: stdout: ""
Sep  6 10:32:15.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-1869 exec execpod-affinitynsdgb -- /bin/sh -x -c nc -zv -t -w 2 10.105.43.36 80'
Sep  6 10:32:15.359: INFO: stderr: "+ nc -zv -t -w 2 10.105.43.36 80\nConnection to 10.105.43.36 80 port [tcp/http] succeeded!\n"
Sep  6 10:32:15.360: INFO: stdout: ""
Sep  6 10:32:15.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-1869 exec execpod-affinitynsdgb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.43.36:80/ ; done'
Sep  6 10:32:15.577: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.36:80/\n"
Sep  6 10:32:15.577: INFO: stdout: "\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t\naffinity-clusterip-gzw5t"
Sep  6 10:32:15.577: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.577: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.577: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.577: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.577: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.577: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.578: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.578: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.578: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.578: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.578: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.578: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.578: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.578: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.578: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.578: INFO: Received response from host: affinity-clusterip-gzw5t
Sep  6 10:32:15.578: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1869, will wait for the garbage collector to delete the pods
Sep  6 10:32:15.652: INFO: Deleting ReplicationController affinity-clusterip took: 5.57902ms
Sep  6 10:32:15.753: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.633789ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:32:28.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1869" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:28.116 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":22,"skipped":317,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:32:28.029: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep  6 10:32:28.062: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 10:32:28.073: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 10:32:28.078: INFO: 
Logging pods the apiserver thinks is on node vm114011 before test
Sep  6 10:32:28.084: INFO: kube-flannel-ds-7mh49 from kube-system started at 2022-09-06 10:12:05 +0000 UTC (1 container statuses recorded)
Sep  6 10:32:28.084: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 10:32:28.084: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-9wpsp from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 10:32:28.084: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:32:28.084: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 10:32:28.084: INFO: 
Logging pods the apiserver thinks is on node vm114012 before test
Sep  6 10:32:28.092: INFO: kube-flannel-ds-8kz6j from kube-system started at 2022-09-06 10:12:05 +0000 UTC (1 container statuses recorded)
Sep  6 10:32:28.092: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 10:32:28.092: INFO: metrics-server-6f58bc76cc-dv8ws from kube-system started at 2022-09-06 10:13:09 +0000 UTC (1 container statuses recorded)
Sep  6 10:32:28.092: INFO: 	Container metrics-server ready: true, restart count 0
Sep  6 10:32:28.092: INFO: sonobuoy from sonobuoy started at 2022-09-06 10:14:15 +0000 UTC (1 container statuses recorded)
Sep  6 10:32:28.092: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 10:32:28.092: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-tdhsh from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 10:32:28.092: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:32:28.092: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 10:32:28.092: INFO: 
Logging pods the apiserver thinks is on node vm114013 before test
Sep  6 10:32:28.101: INFO: coredns-588b5cd46d-jcnzp from kube-system started at 2022-09-06 10:13:07 +0000 UTC (1 container statuses recorded)
Sep  6 10:32:28.101: INFO: 	Container coredns ready: true, restart count 0
Sep  6 10:32:28.101: INFO: kube-flannel-ds-jxdkc from kube-system started at 2022-09-06 10:12:05 +0000 UTC (1 container statuses recorded)
Sep  6 10:32:28.101: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 10:32:28.101: INFO: metrics-server-6f58bc76cc-hppkr from kube-system started at 2022-09-06 10:13:09 +0000 UTC (1 container statuses recorded)
Sep  6 10:32:28.101: INFO: 	Container metrics-server ready: true, restart count 0
Sep  6 10:32:28.101: INFO: sonobuoy-e2e-job-8d290fb976034269 from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 10:32:28.101: INFO: 	Container e2e ready: true, restart count 0
Sep  6 10:32:28.101: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:32:28.101: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-lb9kk from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 10:32:28.101: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:32:28.101: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1b5444c5-c20b-42ad-8ad3-f84e151a7992 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-1b5444c5-c20b-42ad-8ad3-f84e151a7992 off the node vm114011
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1b5444c5-c20b-42ad-8ad3-f84e151a7992
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:32:36.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1306" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.189 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":23,"skipped":330,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:32:36.218: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:32:36.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4647" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":24,"skipped":368,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:32:36.269: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:32:36.307: INFO: Create a RollingUpdate DaemonSet
Sep  6 10:32:36.311: INFO: Check that daemon pods launch on every node of the cluster
Sep  6 10:32:36.317: INFO: Number of nodes with available pods: 0
Sep  6 10:32:36.317: INFO: Node vm114011 is running more than one daemon pod
Sep  6 10:32:37.327: INFO: Number of nodes with available pods: 0
Sep  6 10:32:37.327: INFO: Node vm114011 is running more than one daemon pod
Sep  6 10:32:38.328: INFO: Number of nodes with available pods: 2
Sep  6 10:32:38.328: INFO: Node vm114012 is running more than one daemon pod
Sep  6 10:32:39.324: INFO: Number of nodes with available pods: 3
Sep  6 10:32:39.324: INFO: Number of running nodes: 3, number of available pods: 3
Sep  6 10:32:39.324: INFO: Update the DaemonSet to trigger a rollout
Sep  6 10:32:39.331: INFO: Updating DaemonSet daemon-set
Sep  6 10:32:42.347: INFO: Roll back the DaemonSet before rollout is complete
Sep  6 10:32:42.356: INFO: Updating DaemonSet daemon-set
Sep  6 10:32:42.356: INFO: Make sure DaemonSet rollback is complete
Sep  6 10:32:42.361: INFO: Wrong image for pod: daemon-set-p7zmn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep  6 10:32:42.361: INFO: Pod daemon-set-p7zmn is not available
Sep  6 10:32:43.372: INFO: Wrong image for pod: daemon-set-p7zmn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep  6 10:32:43.372: INFO: Pod daemon-set-p7zmn is not available
Sep  6 10:32:44.371: INFO: Pod daemon-set-zwpr9 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-234, will wait for the garbage collector to delete the pods
Sep  6 10:32:44.439: INFO: Deleting DaemonSet.extensions daemon-set took: 5.872474ms
Sep  6 10:32:44.539: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.423567ms
Sep  6 10:32:59.943: INFO: Number of nodes with available pods: 0
Sep  6 10:32:59.943: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 10:32:59.947: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-234/daemonsets","resourceVersion":"6108"},"items":null}

Sep  6 10:32:59.949: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-234/pods","resourceVersion":"6108"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:32:59.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-234" for this suite.

• [SLOW TEST:23.697 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":25,"skipped":394,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:32:59.966: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-bf482b4d-719f-497d-b50c-94e036671c7f
STEP: Creating a pod to test consume configMaps
Sep  6 10:32:59.999: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d3cac0df-4f0f-4aef-ace1-5db1a9932c72" in namespace "projected-620" to be "Succeeded or Failed"
Sep  6 10:33:00.003: INFO: Pod "pod-projected-configmaps-d3cac0df-4f0f-4aef-ace1-5db1a9932c72": Phase="Pending", Reason="", readiness=false. Elapsed: 3.60229ms
Sep  6 10:33:02.006: INFO: Pod "pod-projected-configmaps-d3cac0df-4f0f-4aef-ace1-5db1a9932c72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006515774s
STEP: Saw pod success
Sep  6 10:33:02.006: INFO: Pod "pod-projected-configmaps-d3cac0df-4f0f-4aef-ace1-5db1a9932c72" satisfied condition "Succeeded or Failed"
Sep  6 10:33:02.008: INFO: Trying to get logs from node vm114011 pod pod-projected-configmaps-d3cac0df-4f0f-4aef-ace1-5db1a9932c72 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 10:33:02.030: INFO: Waiting for pod pod-projected-configmaps-d3cac0df-4f0f-4aef-ace1-5db1a9932c72 to disappear
Sep  6 10:33:02.032: INFO: Pod pod-projected-configmaps-d3cac0df-4f0f-4aef-ace1-5db1a9932c72 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:33:02.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-620" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":26,"skipped":416,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:33:02.040: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:33:13.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2063" for this suite.

• [SLOW TEST:11.079 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":27,"skipped":421,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:33:13.120: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 10:33:13.432: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 10:33:16.450: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep  6 10:33:20.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=webhook-7655 attach --namespace=webhook-7655 to-be-attached-pod -i -c=container1'
Sep  6 10:33:20.547: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:33:20.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7655" for this suite.
STEP: Destroying namespace "webhook-7655-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.537 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":28,"skipped":465,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:33:20.656: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Sep  6 10:33:20.718: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-1136 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:33:20.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1136" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":29,"skipped":482,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:33:20.812: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:33:43.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8895" for this suite.

• [SLOW TEST:22.280 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":30,"skipped":486,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:33:43.092: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Sep  6 10:35:43.641: INFO: Successfully updated pod "var-expansion-d65786a0-a943-40e3-9af8-f3c43cf7325a"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Sep  6 10:35:45.656: INFO: Deleting pod "var-expansion-d65786a0-a943-40e3-9af8-f3c43cf7325a" in namespace "var-expansion-3096"
Sep  6 10:35:45.665: INFO: Wait up to 5m0s for pod "var-expansion-d65786a0-a943-40e3-9af8-f3c43cf7325a" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:36:21.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3096" for this suite.

• [SLOW TEST:158.593 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":31,"skipped":500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:36:21.688: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Sep  6 10:36:21.725: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  6 10:37:21.745: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:37:21.747: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Sep  6 10:37:23.800: INFO: found a healthy node: vm114011
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:37:31.877: INFO: pods created so far: [1 1 1]
Sep  6 10:37:31.877: INFO: length of pods created so far: 3
Sep  6 10:37:45.888: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:37:52.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-921" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:37:52.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5133" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:91.269 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":32,"skipped":527,"failed":0}
SSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:37:52.957: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:37:52.984: INFO: Waiting up to 5m0s for pod "busybox-user-65534-014bf4bc-82bb-4044-96ee-c8307dbb2eab" in namespace "security-context-test-7220" to be "Succeeded or Failed"
Sep  6 10:37:52.987: INFO: Pod "busybox-user-65534-014bf4bc-82bb-4044-96ee-c8307dbb2eab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.858057ms
Sep  6 10:37:54.992: INFO: Pod "busybox-user-65534-014bf4bc-82bb-4044-96ee-c8307dbb2eab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00763412s
Sep  6 10:37:54.992: INFO: Pod "busybox-user-65534-014bf4bc-82bb-4044-96ee-c8307dbb2eab" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:37:54.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7220" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":33,"skipped":531,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:37:55.000: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 10:37:55.229: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 10:37:58.248: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep  6 10:37:58.294: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:37:58.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4253" for this suite.
STEP: Destroying namespace "webhook-4253-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":34,"skipped":531,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:37:58.490: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8823.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8823.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8823.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8823.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8823.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8823.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8823.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8823.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8823.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8823.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 10:38:18.651: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:18.657: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:18.661: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:18.664: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:18.667: INFO: Unable to read wheezy_udp@PodARecord from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:18.670: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:18.674: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:18.677: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:18.680: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:18.683: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:18.686: INFO: Unable to read jessie_udp@PodARecord from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:18.689: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:18.689: INFO: Lookups using dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8823.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8823.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local jessie_udp@dns-test-service-2.dns-8823.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8823.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 10:38:23.693: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:23.697: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:23.701: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:23.706: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:23.709: INFO: Unable to read wheezy_udp@PodARecord from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:23.712: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:23.715: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:23.719: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:23.722: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:23.726: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:23.729: INFO: Unable to read jessie_udp@PodARecord from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:23.732: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:23.732: INFO: Lookups using dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8823.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8823.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local jessie_udp@dns-test-service-2.dns-8823.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8823.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 10:38:28.693: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:28.697: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:28.702: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:28.706: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:28.712: INFO: Unable to read wheezy_udp@PodARecord from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:28.715: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:28.728: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:28.733: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:28.737: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:28.740: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8823.svc.cluster.local from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:28.744: INFO: Unable to read jessie_udp@PodARecord from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:28.748: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2: the server could not find the requested resource (get pods dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2)
Sep  6 10:38:28.748: INFO: Lookups using dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8823.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8823.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8823.svc.cluster.local jessie_udp@dns-test-service-2.dns-8823.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8823.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 10:38:33.726: INFO: DNS probes using dns-8823/dns-test-cf896d78-f250-457f-93cd-3b40f907e1d2 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:38:33.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8823" for this suite.

• [SLOW TEST:35.320 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":35,"skipped":547,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:38:33.811: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:38:35.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2333" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":36,"skipped":560,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:38:35.913: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep  6 10:38:39.980: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2602 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:38:39.980: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:38:40.075: INFO: Exec stderr: ""
Sep  6 10:38:40.078: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2602 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:38:40.078: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:38:40.163: INFO: Exec stderr: ""
Sep  6 10:38:40.163: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2602 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:38:40.163: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:38:40.252: INFO: Exec stderr: ""
Sep  6 10:38:40.252: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2602 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:38:40.252: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:38:40.338: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep  6 10:38:40.338: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2602 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:38:40.338: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:38:40.416: INFO: Exec stderr: ""
Sep  6 10:38:40.416: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2602 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:38:40.416: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:38:40.490: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep  6 10:38:40.490: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2602 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:38:40.490: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:38:40.579: INFO: Exec stderr: ""
Sep  6 10:38:40.579: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2602 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:38:40.579: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:38:40.650: INFO: Exec stderr: ""
Sep  6 10:38:40.650: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2602 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:38:40.650: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:38:40.725: INFO: Exec stderr: ""
Sep  6 10:38:40.725: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2602 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:38:40.725: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:38:40.797: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:38:40.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2602" for this suite.
•{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":37,"skipped":569,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:38:40.805: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:38:40.829: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  6 10:38:45.832: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 10:38:45.832: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep  6 10:38:47.859: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8738 /apis/apps/v1/namespaces/deployment-8738/deployments/test-cleanup-deployment 985dd00c-8f6e-4e41-be8f-b84852b634b6 7713 1 2022-09-06 10:38:45 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-09-06 10:38:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-09-06 10:38:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002497b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-09-06 10:38:45 +0000 UTC,LastTransitionTime:2022-09-06 10:38:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-5d446bdd47" has successfully progressed.,LastUpdateTime:2022-09-06 10:38:47 +0000 UTC,LastTransitionTime:2022-09-06 10:38:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 10:38:47.861: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-8738 /apis/apps/v1/namespaces/deployment-8738/replicasets/test-cleanup-deployment-5d446bdd47 6e058486-08f1-406a-b746-26c814d83d10 7702 1 2022-09-06 10:38:45 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 985dd00c-8f6e-4e41-be8f-b84852b634b6 0xc0025aa757 0xc0025aa758}] []  [{kube-controller-manager Update apps/v1 2022-09-06 10:38:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"985dd00c-8f6e-4e41-be8f-b84852b634b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025aa878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  6 10:38:47.864: INFO: Pod "test-cleanup-deployment-5d446bdd47-b25jl" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-b25jl test-cleanup-deployment-5d446bdd47- deployment-8738 /api/v1/namespaces/deployment-8738/pods/test-cleanup-deployment-5d446bdd47-b25jl 0a10c883-91ea-41dd-b9cb-70a3c9e3fbce 7701 0 2022-09-06 10:38:45 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 6e058486-08f1-406a-b746-26c814d83d10 0xc003775b27 0xc003775b28}] []  [{kube-controller-manager Update v1 2022-09-06 10:38:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e058486-08f1-406a-b746-26c814d83d10\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 10:38:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xhf85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xhf85,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xhf85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114011,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:38:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:38:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:38:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 10:38:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.11,PodIP:10.244.0.48,StartTime:2022-09-06 10:38:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-06 10:38:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://9c69cfcd164930c43c96432309031954c0b77919212e5d83fc3da4e6e5dbca7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:38:47.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8738" for this suite.

• [SLOW TEST:7.068 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":38,"skipped":584,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:38:47.875: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:39:03.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7414" for this suite.

• [SLOW TEST:16.074 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":39,"skipped":604,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:39:03.948: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep  6 10:39:03.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-5417 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Sep  6 10:39:04.067: INFO: stderr: ""
Sep  6 10:39:04.067: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Sep  6 10:39:04.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-5417 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Sep  6 10:39:04.292: INFO: stderr: ""
Sep  6 10:39:04.292: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Sep  6 10:39:04.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-5417 delete pods e2e-test-httpd-pod'
Sep  6 10:39:06.119: INFO: stderr: ""
Sep  6 10:39:06.119: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:39:06.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5417" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":40,"skipped":607,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:39:06.137: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-4664/secret-test-23b1803e-d686-4b4d-aaeb-e2ffe859e08c
STEP: Creating a pod to test consume secrets
Sep  6 10:39:06.170: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7577745-b6a9-49a4-927d-b307dc5dd0a1" in namespace "secrets-4664" to be "Succeeded or Failed"
Sep  6 10:39:06.186: INFO: Pod "pod-configmaps-a7577745-b6a9-49a4-927d-b307dc5dd0a1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.014299ms
Sep  6 10:39:08.191: INFO: Pod "pod-configmaps-a7577745-b6a9-49a4-927d-b307dc5dd0a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020126229s
STEP: Saw pod success
Sep  6 10:39:08.191: INFO: Pod "pod-configmaps-a7577745-b6a9-49a4-927d-b307dc5dd0a1" satisfied condition "Succeeded or Failed"
Sep  6 10:39:08.194: INFO: Trying to get logs from node vm114011 pod pod-configmaps-a7577745-b6a9-49a4-927d-b307dc5dd0a1 container env-test: <nil>
STEP: delete the pod
Sep  6 10:39:08.219: INFO: Waiting for pod pod-configmaps-a7577745-b6a9-49a4-927d-b307dc5dd0a1 to disappear
Sep  6 10:39:08.221: INFO: Pod pod-configmaps-a7577745-b6a9-49a4-927d-b307dc5dd0a1 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:39:08.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4664" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":41,"skipped":625,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:39:08.234: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 10:39:08.275: INFO: Waiting up to 5m0s for pod "downwardapi-volume-648354eb-0da7-426f-899c-4c9bd7d86c09" in namespace "downward-api-7307" to be "Succeeded or Failed"
Sep  6 10:39:08.288: INFO: Pod "downwardapi-volume-648354eb-0da7-426f-899c-4c9bd7d86c09": Phase="Pending", Reason="", readiness=false. Elapsed: 13.008487ms
Sep  6 10:39:10.291: INFO: Pod "downwardapi-volume-648354eb-0da7-426f-899c-4c9bd7d86c09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016156798s
STEP: Saw pod success
Sep  6 10:39:10.291: INFO: Pod "downwardapi-volume-648354eb-0da7-426f-899c-4c9bd7d86c09" satisfied condition "Succeeded or Failed"
Sep  6 10:39:10.293: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-648354eb-0da7-426f-899c-4c9bd7d86c09 container client-container: <nil>
STEP: delete the pod
Sep  6 10:39:10.312: INFO: Waiting for pod downwardapi-volume-648354eb-0da7-426f-899c-4c9bd7d86c09 to disappear
Sep  6 10:39:10.316: INFO: Pod downwardapi-volume-648354eb-0da7-426f-899c-4c9bd7d86c09 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:39:10.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7307" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":42,"skipped":630,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:39:10.326: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep  6 10:39:10.357: INFO: Waiting up to 5m0s for pod "pod-986454d8-49e4-4ff6-ad23-eb5af879380f" in namespace "emptydir-3801" to be "Succeeded or Failed"
Sep  6 10:39:10.365: INFO: Pod "pod-986454d8-49e4-4ff6-ad23-eb5af879380f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.492849ms
Sep  6 10:39:12.369: INFO: Pod "pod-986454d8-49e4-4ff6-ad23-eb5af879380f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011975074s
STEP: Saw pod success
Sep  6 10:39:12.369: INFO: Pod "pod-986454d8-49e4-4ff6-ad23-eb5af879380f" satisfied condition "Succeeded or Failed"
Sep  6 10:39:12.372: INFO: Trying to get logs from node vm114011 pod pod-986454d8-49e4-4ff6-ad23-eb5af879380f container test-container: <nil>
STEP: delete the pod
Sep  6 10:39:12.391: INFO: Waiting for pod pod-986454d8-49e4-4ff6-ad23-eb5af879380f to disappear
Sep  6 10:39:12.395: INFO: Pod pod-986454d8-49e4-4ff6-ad23-eb5af879380f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:39:12.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3801" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":43,"skipped":633,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:39:12.406: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-978
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-978
STEP: creating replication controller externalsvc in namespace services-978
I0906 10:39:12.469296      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-978, replica count: 2
I0906 10:39:15.519818      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Sep  6 10:39:15.539: INFO: Creating new exec pod
Sep  6 10:39:17.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-978 exec execpod8h7zl -- /bin/sh -x -c nslookup clusterip-service.services-978.svc.cluster.local'
Sep  6 10:39:17.755: INFO: stderr: "+ nslookup clusterip-service.services-978.svc.cluster.local\n"
Sep  6 10:39:17.755: INFO: stdout: "Server:\t\t10.96.0.2\nAddress:\t10.96.0.2#53\n\nclusterip-service.services-978.svc.cluster.local\tcanonical name = externalsvc.services-978.svc.cluster.local.\nName:\texternalsvc.services-978.svc.cluster.local\nAddress: 10.103.25.229\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-978, will wait for the garbage collector to delete the pods
Sep  6 10:39:17.816: INFO: Deleting ReplicationController externalsvc took: 6.60816ms
Sep  6 10:39:17.916: INFO: Terminating ReplicationController externalsvc pods took: 100.448184ms
Sep  6 10:39:29.947: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:39:29.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-978" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:17.570 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":44,"skipped":639,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:39:29.976: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:39:34.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-354" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":45,"skipped":650,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:39:34.685: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-ce764c0c-3ce0-49ab-886e-8ea1eb4510cb
STEP: Creating configMap with name cm-test-opt-upd-c102548b-b848-4529-a90b-06475b3cf936
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ce764c0c-3ce0-49ab-886e-8ea1eb4510cb
STEP: Updating configmap cm-test-opt-upd-c102548b-b848-4529-a90b-06475b3cf936
STEP: Creating configMap with name cm-test-opt-create-e3b1f041-ce89-4ed5-920d-d0ea4b2e0163
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:39:40.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1252" for this suite.

• [SLOW TEST:6.145 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":46,"skipped":650,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:39:40.830: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:39:42.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4265" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":47,"skipped":688,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:39:42.899: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-9e534c12-dded-4bb6-81a9-48818e59e740
STEP: Creating a pod to test consume secrets
Sep  6 10:39:42.936: INFO: Waiting up to 5m0s for pod "pod-secrets-ac9d9ca9-71dd-4fee-a814-27d037009157" in namespace "secrets-3957" to be "Succeeded or Failed"
Sep  6 10:39:42.940: INFO: Pod "pod-secrets-ac9d9ca9-71dd-4fee-a814-27d037009157": Phase="Pending", Reason="", readiness=false. Elapsed: 3.189976ms
Sep  6 10:39:44.943: INFO: Pod "pod-secrets-ac9d9ca9-71dd-4fee-a814-27d037009157": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006610092s
STEP: Saw pod success
Sep  6 10:39:44.943: INFO: Pod "pod-secrets-ac9d9ca9-71dd-4fee-a814-27d037009157" satisfied condition "Succeeded or Failed"
Sep  6 10:39:44.946: INFO: Trying to get logs from node vm114011 pod pod-secrets-ac9d9ca9-71dd-4fee-a814-27d037009157 container secret-env-test: <nil>
STEP: delete the pod
Sep  6 10:39:44.960: INFO: Waiting for pod pod-secrets-ac9d9ca9-71dd-4fee-a814-27d037009157 to disappear
Sep  6 10:39:44.964: INFO: Pod pod-secrets-ac9d9ca9-71dd-4fee-a814-27d037009157 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:39:44.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3957" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":48,"skipped":711,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:39:44.976: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-1aaedf8d-1203-4047-a5c9-4df5666d62eb in namespace container-probe-5119
Sep  6 10:39:47.030: INFO: Started pod liveness-1aaedf8d-1203-4047-a5c9-4df5666d62eb in namespace container-probe-5119
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 10:39:47.034: INFO: Initial restart count of pod liveness-1aaedf8d-1203-4047-a5c9-4df5666d62eb is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:43:47.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5119" for this suite.

• [SLOW TEST:242.538 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":49,"skipped":722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:43:47.516: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-8368
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8368 to expose endpoints map[]
Sep  6 10:43:47.592: INFO: successfully validated that service multi-endpoint-test in namespace services-8368 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8368
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8368 to expose endpoints map[pod1:[100]]
Sep  6 10:43:50.667: INFO: successfully validated that service multi-endpoint-test in namespace services-8368 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8368
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8368 to expose endpoints map[pod1:[100] pod2:[101]]
Sep  6 10:43:51.689: INFO: successfully validated that service multi-endpoint-test in namespace services-8368 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-8368
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8368 to expose endpoints map[pod2:[101]]
Sep  6 10:43:51.733: INFO: successfully validated that service multi-endpoint-test in namespace services-8368 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8368
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8368 to expose endpoints map[]
Sep  6 10:43:51.767: INFO: successfully validated that service multi-endpoint-test in namespace services-8368 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:43:51.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8368" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":50,"skipped":803,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:43:51.821: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 10:43:51.883: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e531a3be-d386-408c-aba4-4ca19b06cacd" in namespace "downward-api-1180" to be "Succeeded or Failed"
Sep  6 10:43:51.886: INFO: Pod "downwardapi-volume-e531a3be-d386-408c-aba4-4ca19b06cacd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.031205ms
Sep  6 10:43:53.890: INFO: Pod "downwardapi-volume-e531a3be-d386-408c-aba4-4ca19b06cacd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006152527s
STEP: Saw pod success
Sep  6 10:43:53.890: INFO: Pod "downwardapi-volume-e531a3be-d386-408c-aba4-4ca19b06cacd" satisfied condition "Succeeded or Failed"
Sep  6 10:43:53.893: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-e531a3be-d386-408c-aba4-4ca19b06cacd container client-container: <nil>
STEP: delete the pod
Sep  6 10:43:53.929: INFO: Waiting for pod downwardapi-volume-e531a3be-d386-408c-aba4-4ca19b06cacd to disappear
Sep  6 10:43:53.934: INFO: Pod downwardapi-volume-e531a3be-d386-408c-aba4-4ca19b06cacd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:43:53.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1180" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":51,"skipped":811,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:43:53.946: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-db3c64bf-610f-43da-9848-5aaed230142f
STEP: Creating a pod to test consume configMaps
Sep  6 10:43:53.994: INFO: Waiting up to 5m0s for pod "pod-configmaps-e6a0e7ca-3ab5-474b-8476-cd385945a250" in namespace "configmap-7686" to be "Succeeded or Failed"
Sep  6 10:43:53.999: INFO: Pod "pod-configmaps-e6a0e7ca-3ab5-474b-8476-cd385945a250": Phase="Pending", Reason="", readiness=false. Elapsed: 4.404136ms
Sep  6 10:43:56.002: INFO: Pod "pod-configmaps-e6a0e7ca-3ab5-474b-8476-cd385945a250": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007440319s
STEP: Saw pod success
Sep  6 10:43:56.002: INFO: Pod "pod-configmaps-e6a0e7ca-3ab5-474b-8476-cd385945a250" satisfied condition "Succeeded or Failed"
Sep  6 10:43:56.005: INFO: Trying to get logs from node vm114011 pod pod-configmaps-e6a0e7ca-3ab5-474b-8476-cd385945a250 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 10:43:56.019: INFO: Waiting for pod pod-configmaps-e6a0e7ca-3ab5-474b-8476-cd385945a250 to disappear
Sep  6 10:43:56.023: INFO: Pod pod-configmaps-e6a0e7ca-3ab5-474b-8476-cd385945a250 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:43:56.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7686" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":52,"skipped":837,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:43:56.034: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep  6 10:43:56.059: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep  6 10:44:05.429: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:44:08.152: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:44:18.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2333" for this suite.

• [SLOW TEST:22.728 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":53,"skipped":837,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:44:18.763: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 10:44:19.037: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  6 10:44:21.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798057859, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798057859, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798057859, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798057859, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 10:44:24.063: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:44:24.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4363" for this suite.
STEP: Destroying namespace "webhook-4363-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.413 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":54,"skipped":856,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:44:24.176: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 10:44:24.525: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 10:44:27.544: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:44:27.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3818" for this suite.
STEP: Destroying namespace "webhook-3818-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":55,"skipped":882,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:44:27.650: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep  6 10:44:27.698: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 10:44:27.711: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 10:44:27.716: INFO: 
Logging pods the apiserver thinks is on node vm114011 before test
Sep  6 10:44:27.721: INFO: kube-flannel-ds-7mh49 from kube-system started at 2022-09-06 10:12:05 +0000 UTC (1 container statuses recorded)
Sep  6 10:44:27.721: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 10:44:27.721: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-9wpsp from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 10:44:27.721: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:44:27.721: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 10:44:27.721: INFO: 
Logging pods the apiserver thinks is on node vm114012 before test
Sep  6 10:44:27.726: INFO: kube-flannel-ds-8kz6j from kube-system started at 2022-09-06 10:12:05 +0000 UTC (1 container statuses recorded)
Sep  6 10:44:27.726: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 10:44:27.726: INFO: metrics-server-6f58bc76cc-dv8ws from kube-system started at 2022-09-06 10:13:09 +0000 UTC (1 container statuses recorded)
Sep  6 10:44:27.726: INFO: 	Container metrics-server ready: true, restart count 0
Sep  6 10:44:27.726: INFO: sonobuoy from sonobuoy started at 2022-09-06 10:14:15 +0000 UTC (1 container statuses recorded)
Sep  6 10:44:27.726: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 10:44:27.726: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-tdhsh from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 10:44:27.726: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:44:27.726: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 10:44:27.726: INFO: 
Logging pods the apiserver thinks is on node vm114013 before test
Sep  6 10:44:27.732: INFO: coredns-588b5cd46d-jcnzp from kube-system started at 2022-09-06 10:13:07 +0000 UTC (1 container statuses recorded)
Sep  6 10:44:27.732: INFO: 	Container coredns ready: true, restart count 0
Sep  6 10:44:27.732: INFO: kube-flannel-ds-jxdkc from kube-system started at 2022-09-06 10:12:05 +0000 UTC (1 container statuses recorded)
Sep  6 10:44:27.732: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 10:44:27.733: INFO: metrics-server-6f58bc76cc-hppkr from kube-system started at 2022-09-06 10:13:09 +0000 UTC (1 container statuses recorded)
Sep  6 10:44:27.733: INFO: 	Container metrics-server ready: true, restart count 0
Sep  6 10:44:27.733: INFO: sonobuoy-e2e-job-8d290fb976034269 from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 10:44:27.733: INFO: 	Container e2e ready: true, restart count 0
Sep  6 10:44:27.733: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:44:27.733: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-lb9kk from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 10:44:27.733: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 10:44:27.733: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5a40339a-e80f-4020-a32c-288b7876d315 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-5a40339a-e80f-4020-a32c-288b7876d315 off the node vm114011
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5a40339a-e80f-4020-a32c-288b7876d315
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:44:35.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3313" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.233 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":56,"skipped":883,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:44:35.883: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-t28b
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 10:44:35.936: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-t28b" in namespace "subpath-9019" to be "Succeeded or Failed"
Sep  6 10:44:35.947: INFO: Pod "pod-subpath-test-downwardapi-t28b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.538437ms
Sep  6 10:44:37.951: INFO: Pod "pod-subpath-test-downwardapi-t28b": Phase="Running", Reason="", readiness=true. Elapsed: 2.014792496s
Sep  6 10:44:39.956: INFO: Pod "pod-subpath-test-downwardapi-t28b": Phase="Running", Reason="", readiness=true. Elapsed: 4.01990225s
Sep  6 10:44:41.959: INFO: Pod "pod-subpath-test-downwardapi-t28b": Phase="Running", Reason="", readiness=true. Elapsed: 6.022253844s
Sep  6 10:44:43.962: INFO: Pod "pod-subpath-test-downwardapi-t28b": Phase="Running", Reason="", readiness=true. Elapsed: 8.025593462s
Sep  6 10:44:45.965: INFO: Pod "pod-subpath-test-downwardapi-t28b": Phase="Running", Reason="", readiness=true. Elapsed: 10.028843596s
Sep  6 10:44:47.969: INFO: Pod "pod-subpath-test-downwardapi-t28b": Phase="Running", Reason="", readiness=true. Elapsed: 12.032941207s
Sep  6 10:44:49.973: INFO: Pod "pod-subpath-test-downwardapi-t28b": Phase="Running", Reason="", readiness=true. Elapsed: 14.036099352s
Sep  6 10:44:51.976: INFO: Pod "pod-subpath-test-downwardapi-t28b": Phase="Running", Reason="", readiness=true. Elapsed: 16.039246254s
Sep  6 10:44:53.979: INFO: Pod "pod-subpath-test-downwardapi-t28b": Phase="Running", Reason="", readiness=true. Elapsed: 18.042948748s
Sep  6 10:44:55.984: INFO: Pod "pod-subpath-test-downwardapi-t28b": Phase="Running", Reason="", readiness=true. Elapsed: 20.048019347s
Sep  6 10:44:57.989: INFO: Pod "pod-subpath-test-downwardapi-t28b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.052735863s
STEP: Saw pod success
Sep  6 10:44:57.989: INFO: Pod "pod-subpath-test-downwardapi-t28b" satisfied condition "Succeeded or Failed"
Sep  6 10:44:57.993: INFO: Trying to get logs from node vm114012 pod pod-subpath-test-downwardapi-t28b container test-container-subpath-downwardapi-t28b: <nil>
STEP: delete the pod
Sep  6 10:44:58.013: INFO: Waiting for pod pod-subpath-test-downwardapi-t28b to disappear
Sep  6 10:44:58.015: INFO: Pod pod-subpath-test-downwardapi-t28b no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-t28b
Sep  6 10:44:58.015: INFO: Deleting pod "pod-subpath-test-downwardapi-t28b" in namespace "subpath-9019"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:44:58.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9019" for this suite.

• [SLOW TEST:22.142 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":57,"skipped":883,"failed":0}
S
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:44:58.025: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Sep  6 10:44:58.067: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:44:58.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9614" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":58,"skipped":884,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:44:58.088: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 10:44:58.532: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 10:45:01.551: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:45:13.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9587" for this suite.
STEP: Destroying namespace "webhook-9587-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.645 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":59,"skipped":908,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:45:13.734: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-2171be91-819d-4ee3-a942-bd60beca6515
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:45:13.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4794" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":60,"skipped":913,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:45:13.877: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 10:45:13.944: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0f8f17e-076d-41b1-b4c9-ee4348be3f9d" in namespace "projected-289" to be "Succeeded or Failed"
Sep  6 10:45:13.956: INFO: Pod "downwardapi-volume-e0f8f17e-076d-41b1-b4c9-ee4348be3f9d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.954649ms
Sep  6 10:45:15.959: INFO: Pod "downwardapi-volume-e0f8f17e-076d-41b1-b4c9-ee4348be3f9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014398251s
STEP: Saw pod success
Sep  6 10:45:15.959: INFO: Pod "downwardapi-volume-e0f8f17e-076d-41b1-b4c9-ee4348be3f9d" satisfied condition "Succeeded or Failed"
Sep  6 10:45:15.961: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-e0f8f17e-076d-41b1-b4c9-ee4348be3f9d container client-container: <nil>
STEP: delete the pod
Sep  6 10:45:15.975: INFO: Waiting for pod downwardapi-volume-e0f8f17e-076d-41b1-b4c9-ee4348be3f9d to disappear
Sep  6 10:45:15.978: INFO: Pod downwardapi-volume-e0f8f17e-076d-41b1-b4c9-ee4348be3f9d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:45:15.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-289" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":61,"skipped":915,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:45:15.988: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  6 10:45:16.018: INFO: Waiting up to 5m0s for pod "pod-06f98912-3807-4f45-95ac-7ffa8d4cc072" in namespace "emptydir-5901" to be "Succeeded or Failed"
Sep  6 10:45:16.026: INFO: Pod "pod-06f98912-3807-4f45-95ac-7ffa8d4cc072": Phase="Pending", Reason="", readiness=false. Elapsed: 8.064308ms
Sep  6 10:45:18.029: INFO: Pod "pod-06f98912-3807-4f45-95ac-7ffa8d4cc072": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011142683s
Sep  6 10:45:20.033: INFO: Pod "pod-06f98912-3807-4f45-95ac-7ffa8d4cc072": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015144968s
STEP: Saw pod success
Sep  6 10:45:20.034: INFO: Pod "pod-06f98912-3807-4f45-95ac-7ffa8d4cc072" satisfied condition "Succeeded or Failed"
Sep  6 10:45:20.037: INFO: Trying to get logs from node vm114011 pod pod-06f98912-3807-4f45-95ac-7ffa8d4cc072 container test-container: <nil>
STEP: delete the pod
Sep  6 10:45:20.057: INFO: Waiting for pod pod-06f98912-3807-4f45-95ac-7ffa8d4cc072 to disappear
Sep  6 10:45:20.060: INFO: Pod pod-06f98912-3807-4f45-95ac-7ffa8d4cc072 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:45:20.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5901" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":62,"skipped":933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:45:20.072: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep  6 10:45:22.632: INFO: Successfully updated pod "annotationupdate717385f2-3b0d-4f00-86f7-8e50c804b9dd"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:45:24.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6890" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":63,"skipped":958,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:45:24.657: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-bef84e97-47b4-4ec3-9647-792b9d8cfc2c in namespace container-probe-7002
Sep  6 10:45:26.691: INFO: Started pod liveness-bef84e97-47b4-4ec3-9647-792b9d8cfc2c in namespace container-probe-7002
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 10:45:26.695: INFO: Initial restart count of pod liveness-bef84e97-47b4-4ec3-9647-792b9d8cfc2c is 0
Sep  6 10:45:42.729: INFO: Restart count of pod container-probe-7002/liveness-bef84e97-47b4-4ec3-9647-792b9d8cfc2c is now 1 (16.034829923s elapsed)
Sep  6 10:46:02.763: INFO: Restart count of pod container-probe-7002/liveness-bef84e97-47b4-4ec3-9647-792b9d8cfc2c is now 2 (36.06877338s elapsed)
Sep  6 10:46:22.800: INFO: Restart count of pod container-probe-7002/liveness-bef84e97-47b4-4ec3-9647-792b9d8cfc2c is now 3 (56.105650476s elapsed)
Sep  6 10:46:42.835: INFO: Restart count of pod container-probe-7002/liveness-bef84e97-47b4-4ec3-9647-792b9d8cfc2c is now 4 (1m16.140669898s elapsed)
Sep  6 10:47:52.970: INFO: Restart count of pod container-probe-7002/liveness-bef84e97-47b4-4ec3-9647-792b9d8cfc2c is now 5 (2m26.275595664s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:47:52.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7002" for this suite.

• [SLOW TEST:148.339 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":64,"skipped":967,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:47:52.996: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:47:53.027: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:47:57.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7298" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":65,"skipped":981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:47:57.144: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-733e418c-2f0c-4a41-af0f-88df712ab3d8
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-733e418c-2f0c-4a41-af0f-88df712ab3d8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:48:01.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9654" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":66,"skipped":1043,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:48:01.234: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep  6 10:48:01.268: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7078 /api/v1/namespaces/watch-7078/configmaps/e2e-watch-test-watch-closed a3efc8ad-be7f-49c3-97cc-e713f04115c5 10208 0 2022-09-06 10:48:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:48:01.269: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7078 /api/v1/namespaces/watch-7078/configmaps/e2e-watch-test-watch-closed a3efc8ad-be7f-49c3-97cc-e713f04115c5 10209 0 2022-09-06 10:48:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep  6 10:48:01.280: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7078 /api/v1/namespaces/watch-7078/configmaps/e2e-watch-test-watch-closed a3efc8ad-be7f-49c3-97cc-e713f04115c5 10210 0 2022-09-06 10:48:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:48:01.280: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7078 /api/v1/namespaces/watch-7078/configmaps/e2e-watch-test-watch-closed a3efc8ad-be7f-49c3-97cc-e713f04115c5 10211 0 2022-09-06 10:48:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:48:01.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7078" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":67,"skipped":1066,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:48:01.287: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 10:48:01.551: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  6 10:48:03.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798058081, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798058081, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798058081, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798058081, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 10:48:06.576: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:48:06.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9351" for this suite.
STEP: Destroying namespace "webhook-9351-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.403 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":68,"skipped":1081,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:48:06.692: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep  6 10:48:06.740: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6365 /api/v1/namespaces/watch-6365/configmaps/e2e-watch-test-configmap-a 59e7e9a9-d132-475e-bb6f-fb00df6d9c71 10306 0 2022-09-06 10:48:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:48:06.741: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6365 /api/v1/namespaces/watch-6365/configmaps/e2e-watch-test-configmap-a 59e7e9a9-d132-475e-bb6f-fb00df6d9c71 10306 0 2022-09-06 10:48:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep  6 10:48:16.748: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6365 /api/v1/namespaces/watch-6365/configmaps/e2e-watch-test-configmap-a 59e7e9a9-d132-475e-bb6f-fb00df6d9c71 10353 0 2022-09-06 10:48:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:48:16.748: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6365 /api/v1/namespaces/watch-6365/configmaps/e2e-watch-test-configmap-a 59e7e9a9-d132-475e-bb6f-fb00df6d9c71 10353 0 2022-09-06 10:48:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep  6 10:48:26.760: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6365 /api/v1/namespaces/watch-6365/configmaps/e2e-watch-test-configmap-a 59e7e9a9-d132-475e-bb6f-fb00df6d9c71 10385 0 2022-09-06 10:48:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:48:26.760: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6365 /api/v1/namespaces/watch-6365/configmaps/e2e-watch-test-configmap-a 59e7e9a9-d132-475e-bb6f-fb00df6d9c71 10385 0 2022-09-06 10:48:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep  6 10:48:36.767: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6365 /api/v1/namespaces/watch-6365/configmaps/e2e-watch-test-configmap-a 59e7e9a9-d132-475e-bb6f-fb00df6d9c71 10415 0 2022-09-06 10:48:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:48:36.767: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6365 /api/v1/namespaces/watch-6365/configmaps/e2e-watch-test-configmap-a 59e7e9a9-d132-475e-bb6f-fb00df6d9c71 10415 0 2022-09-06 10:48:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep  6 10:48:46.774: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6365 /api/v1/namespaces/watch-6365/configmaps/e2e-watch-test-configmap-b c05c738e-a707-4291-8eaf-405d0aee3d83 10442 0 2022-09-06 10:48:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:48:46.775: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6365 /api/v1/namespaces/watch-6365/configmaps/e2e-watch-test-configmap-b c05c738e-a707-4291-8eaf-405d0aee3d83 10442 0 2022-09-06 10:48:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep  6 10:48:56.782: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6365 /api/v1/namespaces/watch-6365/configmaps/e2e-watch-test-configmap-b c05c738e-a707-4291-8eaf-405d0aee3d83 10467 0 2022-09-06 10:48:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 10:48:56.782: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6365 /api/v1/namespaces/watch-6365/configmaps/e2e-watch-test-configmap-b c05c738e-a707-4291-8eaf-405d0aee3d83 10467 0 2022-09-06 10:48:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-09-06 10:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:49:06.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6365" for this suite.

• [SLOW TEST:60.102 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":69,"skipped":1099,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:49:06.795: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 10:49:06.823: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea90048b-7726-4d27-91c2-77bfa8e1c347" in namespace "projected-1160" to be "Succeeded or Failed"
Sep  6 10:49:06.827: INFO: Pod "downwardapi-volume-ea90048b-7726-4d27-91c2-77bfa8e1c347": Phase="Pending", Reason="", readiness=false. Elapsed: 4.43078ms
Sep  6 10:49:08.831: INFO: Pod "downwardapi-volume-ea90048b-7726-4d27-91c2-77bfa8e1c347": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007676301s
STEP: Saw pod success
Sep  6 10:49:08.831: INFO: Pod "downwardapi-volume-ea90048b-7726-4d27-91c2-77bfa8e1c347" satisfied condition "Succeeded or Failed"
Sep  6 10:49:08.834: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-ea90048b-7726-4d27-91c2-77bfa8e1c347 container client-container: <nil>
STEP: delete the pod
Sep  6 10:49:08.856: INFO: Waiting for pod downwardapi-volume-ea90048b-7726-4d27-91c2-77bfa8e1c347 to disappear
Sep  6 10:49:08.859: INFO: Pod downwardapi-volume-ea90048b-7726-4d27-91c2-77bfa8e1c347 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:49:08.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1160" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":70,"skipped":1103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:49:08.869: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:51:08.908: INFO: Deleting pod "var-expansion-c2c33af1-4f8f-4e9a-ab6f-119ab0ee86fb" in namespace "var-expansion-5913"
Sep  6 10:51:08.915: INFO: Wait up to 5m0s for pod "var-expansion-c2c33af1-4f8f-4e9a-ab6f-119ab0ee86fb" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:51:10.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5913" for this suite.

• [SLOW TEST:122.065 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":71,"skipped":1128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:51:10.934: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep  6 10:51:13.505: INFO: Successfully updated pod "labelsupdate7e30e7fb-8623-43a1-ad7c-1ca078a691ac"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:51:17.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4650" for this suite.

• [SLOW TEST:6.604 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":72,"skipped":1194,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:51:17.539: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-3f5fcae2-3e7b-4d10-a024-9deb9bbc714a
STEP: Creating a pod to test consume configMaps
Sep  6 10:51:17.576: INFO: Waiting up to 5m0s for pod "pod-configmaps-4188ee06-9974-40ec-9154-fa46d92118ca" in namespace "configmap-9454" to be "Succeeded or Failed"
Sep  6 10:51:17.581: INFO: Pod "pod-configmaps-4188ee06-9974-40ec-9154-fa46d92118ca": Phase="Pending", Reason="", readiness=false. Elapsed: 5.683658ms
Sep  6 10:51:19.587: INFO: Pod "pod-configmaps-4188ee06-9974-40ec-9154-fa46d92118ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010974653s
STEP: Saw pod success
Sep  6 10:51:19.587: INFO: Pod "pod-configmaps-4188ee06-9974-40ec-9154-fa46d92118ca" satisfied condition "Succeeded or Failed"
Sep  6 10:51:19.590: INFO: Trying to get logs from node vm114012 pod pod-configmaps-4188ee06-9974-40ec-9154-fa46d92118ca container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 10:51:19.614: INFO: Waiting for pod pod-configmaps-4188ee06-9974-40ec-9154-fa46d92118ca to disappear
Sep  6 10:51:19.617: INFO: Pod pod-configmaps-4188ee06-9974-40ec-9154-fa46d92118ca no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:51:19.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9454" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":73,"skipped":1219,"failed":0}
SSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:51:19.626: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep  6 10:51:19.679: INFO: starting watch
STEP: patching
STEP: updating
Sep  6 10:51:19.691: INFO: waiting for watch events with expected annotations
Sep  6 10:51:19.691: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:51:19.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-4445" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":74,"skipped":1225,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:51:19.718: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-e5eff79b-5619-4ce6-ae54-c1b84300d29b
STEP: Creating configMap with name cm-test-opt-upd-cad62df2-4228-4a1e-b787-9ca59647e506
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e5eff79b-5619-4ce6-ae54-c1b84300d29b
STEP: Updating configmap cm-test-opt-upd-cad62df2-4228-4a1e-b787-9ca59647e506
STEP: Creating configMap with name cm-test-opt-create-7777b0d7-8035-422e-a93d-9a5d0082c53b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:51:23.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8161" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":75,"skipped":1239,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:51:23.856: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep  6 10:51:23.884: INFO: Waiting up to 5m0s for pod "downward-api-b0d0d508-2674-4f0f-9b3e-58a41f4491f4" in namespace "downward-api-9898" to be "Succeeded or Failed"
Sep  6 10:51:23.888: INFO: Pod "downward-api-b0d0d508-2674-4f0f-9b3e-58a41f4491f4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.955778ms
Sep  6 10:51:25.892: INFO: Pod "downward-api-b0d0d508-2674-4f0f-9b3e-58a41f4491f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007601622s
STEP: Saw pod success
Sep  6 10:51:25.892: INFO: Pod "downward-api-b0d0d508-2674-4f0f-9b3e-58a41f4491f4" satisfied condition "Succeeded or Failed"
Sep  6 10:51:25.901: INFO: Trying to get logs from node vm114011 pod downward-api-b0d0d508-2674-4f0f-9b3e-58a41f4491f4 container dapi-container: <nil>
STEP: delete the pod
Sep  6 10:51:25.919: INFO: Waiting for pod downward-api-b0d0d508-2674-4f0f-9b3e-58a41f4491f4 to disappear
Sep  6 10:51:25.922: INFO: Pod downward-api-b0d0d508-2674-4f0f-9b3e-58a41f4491f4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:51:25.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9898" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":76,"skipped":1239,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:51:25.931: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep  6 10:51:25.987: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep  6 10:51:25.995: INFO: starting watch
STEP: patching
STEP: updating
Sep  6 10:51:26.010: INFO: waiting for watch events with expected annotations
Sep  6 10:51:26.011: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:51:26.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-7670" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":77,"skipped":1248,"failed":0}
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:51:26.067: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:51:26.091: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6904
I0906 10:51:26.100683      20 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6904, replica count: 1
I0906 10:51:27.151711      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 10:51:27.267: INFO: Created: latency-svc-gqhrm
Sep  6 10:51:27.278: INFO: Got endpoints: latency-svc-gqhrm [26.053983ms]
Sep  6 10:51:27.339: INFO: Created: latency-svc-lf2wr
Sep  6 10:51:27.339: INFO: Got endpoints: latency-svc-lf2wr [61.012114ms]
Sep  6 10:51:27.340: INFO: Created: latency-svc-dxxtt
Sep  6 10:51:27.345: INFO: Got endpoints: latency-svc-dxxtt [65.888276ms]
Sep  6 10:51:27.354: INFO: Created: latency-svc-wvpcn
Sep  6 10:51:27.360: INFO: Got endpoints: latency-svc-wvpcn [81.437797ms]
Sep  6 10:51:27.382: INFO: Created: latency-svc-nn6rd
Sep  6 10:51:27.397: INFO: Created: latency-svc-8cgk6
Sep  6 10:51:27.403: INFO: Got endpoints: latency-svc-nn6rd [123.698202ms]
Sep  6 10:51:27.408: INFO: Got endpoints: latency-svc-8cgk6 [128.155723ms]
Sep  6 10:51:27.414: INFO: Created: latency-svc-c9prj
Sep  6 10:51:27.420: INFO: Got endpoints: latency-svc-c9prj [140.951377ms]
Sep  6 10:51:27.428: INFO: Created: latency-svc-frnnp
Sep  6 10:51:27.442: INFO: Got endpoints: latency-svc-frnnp [33.899429ms]
Sep  6 10:51:27.452: INFO: Created: latency-svc-c7tm2
Sep  6 10:51:27.461: INFO: Got endpoints: latency-svc-c7tm2 [181.538292ms]
Sep  6 10:51:27.470: INFO: Created: latency-svc-h6pxz
Sep  6 10:51:27.490: INFO: Created: latency-svc-2p7ck
Sep  6 10:51:27.491: INFO: Got endpoints: latency-svc-2p7ck [210.549002ms]
Sep  6 10:51:27.491: INFO: Got endpoints: latency-svc-h6pxz [210.990534ms]
Sep  6 10:51:27.500: INFO: Created: latency-svc-tljq7
Sep  6 10:51:27.511: INFO: Got endpoints: latency-svc-tljq7 [231.00737ms]
Sep  6 10:51:27.517: INFO: Created: latency-svc-hcbjs
Sep  6 10:51:27.530: INFO: Got endpoints: latency-svc-hcbjs [249.765639ms]
Sep  6 10:51:27.544: INFO: Created: latency-svc-vw4ct
Sep  6 10:51:27.550: INFO: Got endpoints: latency-svc-vw4ct [270.187967ms]
Sep  6 10:51:27.559: INFO: Created: latency-svc-lttgf
Sep  6 10:51:27.568: INFO: Got endpoints: latency-svc-lttgf [288.203153ms]
Sep  6 10:51:27.575: INFO: Created: latency-svc-ngj8d
Sep  6 10:51:27.580: INFO: Got endpoints: latency-svc-ngj8d [300.672519ms]
Sep  6 10:51:27.592: INFO: Created: latency-svc-4hvzq
Sep  6 10:51:27.602: INFO: Got endpoints: latency-svc-4hvzq [321.927498ms]
Sep  6 10:51:27.604: INFO: Created: latency-svc-mvc2s
Sep  6 10:51:27.614: INFO: Created: latency-svc-27mmx
Sep  6 10:51:27.620: INFO: Got endpoints: latency-svc-27mmx [275.329136ms]
Sep  6 10:51:27.620: INFO: Got endpoints: latency-svc-mvc2s [280.976173ms]
Sep  6 10:51:27.623: INFO: Created: latency-svc-tvhhv
Sep  6 10:51:27.628: INFO: Created: latency-svc-vpklc
Sep  6 10:51:27.635: INFO: Got endpoints: latency-svc-tvhhv [275.107428ms]
Sep  6 10:51:27.637: INFO: Created: latency-svc-mgqvz
Sep  6 10:51:27.640: INFO: Got endpoints: latency-svc-vpklc [236.970994ms]
Sep  6 10:51:27.645: INFO: Got endpoints: latency-svc-mgqvz [224.045671ms]
Sep  6 10:51:27.654: INFO: Created: latency-svc-7h6nh
Sep  6 10:51:27.666: INFO: Got endpoints: latency-svc-7h6nh [224.040583ms]
Sep  6 10:51:27.675: INFO: Created: latency-svc-hxwr9
Sep  6 10:51:27.682: INFO: Got endpoints: latency-svc-hxwr9 [220.858259ms]
Sep  6 10:51:27.683: INFO: Created: latency-svc-nqc4l
Sep  6 10:51:27.689: INFO: Got endpoints: latency-svc-nqc4l [197.937337ms]
Sep  6 10:51:27.693: INFO: Created: latency-svc-qc7xv
Sep  6 10:51:27.699: INFO: Got endpoints: latency-svc-qc7xv [208.616338ms]
Sep  6 10:51:27.708: INFO: Created: latency-svc-x9d6w
Sep  6 10:51:27.711: INFO: Got endpoints: latency-svc-x9d6w [199.252324ms]
Sep  6 10:51:27.713: INFO: Created: latency-svc-bnrzq
Sep  6 10:51:27.722: INFO: Got endpoints: latency-svc-bnrzq [191.94839ms]
Sep  6 10:51:27.726: INFO: Created: latency-svc-z9rb2
Sep  6 10:51:27.732: INFO: Got endpoints: latency-svc-z9rb2 [182.005529ms]
Sep  6 10:51:27.735: INFO: Created: latency-svc-hlslb
Sep  6 10:51:27.740: INFO: Got endpoints: latency-svc-hlslb [171.645141ms]
Sep  6 10:51:27.746: INFO: Created: latency-svc-2pbq6
Sep  6 10:51:27.761: INFO: Got endpoints: latency-svc-2pbq6 [180.232384ms]
Sep  6 10:51:27.765: INFO: Created: latency-svc-vwp9t
Sep  6 10:51:27.771: INFO: Got endpoints: latency-svc-vwp9t [168.6018ms]
Sep  6 10:51:27.772: INFO: Created: latency-svc-hsjq7
Sep  6 10:51:27.796: INFO: Got endpoints: latency-svc-hsjq7 [175.688662ms]
Sep  6 10:51:27.804: INFO: Created: latency-svc-h8c54
Sep  6 10:51:27.809: INFO: Got endpoints: latency-svc-h8c54 [189.007783ms]
Sep  6 10:51:27.816: INFO: Created: latency-svc-f6kfd
Sep  6 10:51:27.822: INFO: Created: latency-svc-5k726
Sep  6 10:51:27.824: INFO: Got endpoints: latency-svc-f6kfd [188.265004ms]
Sep  6 10:51:27.829: INFO: Got endpoints: latency-svc-5k726 [188.548722ms]
Sep  6 10:51:27.845: INFO: Created: latency-svc-dmqjw
Sep  6 10:51:27.850: INFO: Got endpoints: latency-svc-dmqjw [205.082886ms]
Sep  6 10:51:27.857: INFO: Created: latency-svc-qhkrm
Sep  6 10:51:27.864: INFO: Got endpoints: latency-svc-qhkrm [198.203648ms]
Sep  6 10:51:27.865: INFO: Created: latency-svc-h4pfl
Sep  6 10:51:27.877: INFO: Got endpoints: latency-svc-h4pfl [195.004472ms]
Sep  6 10:51:27.878: INFO: Created: latency-svc-r7dnj
Sep  6 10:51:27.883: INFO: Got endpoints: latency-svc-r7dnj [194.325462ms]
Sep  6 10:51:27.893: INFO: Created: latency-svc-j7nvf
Sep  6 10:51:27.904: INFO: Got endpoints: latency-svc-j7nvf [203.898073ms]
Sep  6 10:51:27.909: INFO: Created: latency-svc-jlw8j
Sep  6 10:51:27.921: INFO: Got endpoints: latency-svc-jlw8j [210.672752ms]
Sep  6 10:51:27.936: INFO: Created: latency-svc-bwth7
Sep  6 10:51:27.947: INFO: Got endpoints: latency-svc-bwth7 [225.272844ms]
Sep  6 10:51:27.952: INFO: Created: latency-svc-8wgsh
Sep  6 10:51:27.959: INFO: Got endpoints: latency-svc-8wgsh [226.733963ms]
Sep  6 10:51:27.965: INFO: Created: latency-svc-bndm4
Sep  6 10:51:27.974: INFO: Got endpoints: latency-svc-bndm4 [233.758081ms]
Sep  6 10:51:27.979: INFO: Created: latency-svc-kw654
Sep  6 10:51:27.987: INFO: Got endpoints: latency-svc-kw654 [225.9984ms]
Sep  6 10:51:27.996: INFO: Created: latency-svc-r4s92
Sep  6 10:51:28.006: INFO: Got endpoints: latency-svc-r4s92 [235.004773ms]
Sep  6 10:51:28.011: INFO: Created: latency-svc-sqvs2
Sep  6 10:51:28.018: INFO: Got endpoints: latency-svc-sqvs2 [222.08011ms]
Sep  6 10:51:28.027: INFO: Created: latency-svc-bmlgg
Sep  6 10:51:28.030: INFO: Got endpoints: latency-svc-bmlgg [220.283435ms]
Sep  6 10:51:28.034: INFO: Created: latency-svc-98p86
Sep  6 10:51:28.041: INFO: Got endpoints: latency-svc-98p86 [216.804761ms]
Sep  6 10:51:28.043: INFO: Created: latency-svc-9xzpw
Sep  6 10:51:28.053: INFO: Got endpoints: latency-svc-9xzpw [223.964645ms]
Sep  6 10:51:28.053: INFO: Created: latency-svc-shcjf
Sep  6 10:51:28.061: INFO: Got endpoints: latency-svc-shcjf [210.61204ms]
Sep  6 10:51:28.066: INFO: Created: latency-svc-txjrq
Sep  6 10:51:28.082: INFO: Got endpoints: latency-svc-txjrq [217.57723ms]
Sep  6 10:51:28.085: INFO: Created: latency-svc-6jg5w
Sep  6 10:51:28.091: INFO: Got endpoints: latency-svc-6jg5w [213.418208ms]
Sep  6 10:51:28.102: INFO: Created: latency-svc-57svx
Sep  6 10:51:28.113: INFO: Created: latency-svc-szhgp
Sep  6 10:51:28.114: INFO: Got endpoints: latency-svc-57svx [230.786841ms]
Sep  6 10:51:28.123: INFO: Created: latency-svc-prs7v
Sep  6 10:51:28.129: INFO: Got endpoints: latency-svc-szhgp [225.640764ms]
Sep  6 10:51:28.130: INFO: Got endpoints: latency-svc-prs7v [208.704047ms]
Sep  6 10:51:28.138: INFO: Created: latency-svc-w54nn
Sep  6 10:51:28.147: INFO: Got endpoints: latency-svc-w54nn [199.304479ms]
Sep  6 10:51:28.151: INFO: Created: latency-svc-tq89z
Sep  6 10:51:28.156: INFO: Got endpoints: latency-svc-tq89z [196.286972ms]
Sep  6 10:51:28.166: INFO: Created: latency-svc-b9k7l
Sep  6 10:51:28.169: INFO: Created: latency-svc-8r5xk
Sep  6 10:51:28.174: INFO: Got endpoints: latency-svc-b9k7l [187.545727ms]
Sep  6 10:51:28.195: INFO: Created: latency-svc-hblrw
Sep  6 10:51:28.195: INFO: Got endpoints: latency-svc-8r5xk [220.966831ms]
Sep  6 10:51:28.209: INFO: Got endpoints: latency-svc-hblrw [203.111246ms]
Sep  6 10:51:28.234: INFO: Created: latency-svc-nr5tr
Sep  6 10:51:28.243: INFO: Got endpoints: latency-svc-nr5tr [224.463895ms]
Sep  6 10:51:28.265: INFO: Created: latency-svc-4ggw4
Sep  6 10:51:28.281: INFO: Got endpoints: latency-svc-4ggw4 [251.531294ms]
Sep  6 10:51:28.282: INFO: Created: latency-svc-cql78
Sep  6 10:51:28.292: INFO: Got endpoints: latency-svc-cql78 [251.169461ms]
Sep  6 10:51:28.313: INFO: Created: latency-svc-qdqww
Sep  6 10:51:28.322: INFO: Created: latency-svc-gk7sd
Sep  6 10:51:28.322: INFO: Got endpoints: latency-svc-qdqww [269.716722ms]
Sep  6 10:51:28.333: INFO: Got endpoints: latency-svc-gk7sd [271.894427ms]
Sep  6 10:51:28.338: INFO: Created: latency-svc-8279j
Sep  6 10:51:28.354: INFO: Got endpoints: latency-svc-8279j [272.097132ms]
Sep  6 10:51:28.357: INFO: Created: latency-svc-s8wmc
Sep  6 10:51:28.365: INFO: Got endpoints: latency-svc-s8wmc [274.476211ms]
Sep  6 10:51:28.373: INFO: Created: latency-svc-zvtvj
Sep  6 10:51:28.395: INFO: Got endpoints: latency-svc-zvtvj [279.981856ms]
Sep  6 10:51:28.397: INFO: Created: latency-svc-zm6b8
Sep  6 10:51:28.406: INFO: Got endpoints: latency-svc-zm6b8 [277.08061ms]
Sep  6 10:51:28.411: INFO: Created: latency-svc-7lp6c
Sep  6 10:51:28.421: INFO: Got endpoints: latency-svc-7lp6c [290.870272ms]
Sep  6 10:51:28.428: INFO: Created: latency-svc-dhnq6
Sep  6 10:51:28.436: INFO: Got endpoints: latency-svc-dhnq6 [289.750573ms]
Sep  6 10:51:28.438: INFO: Created: latency-svc-lw7hg
Sep  6 10:51:28.444: INFO: Got endpoints: latency-svc-lw7hg [287.859497ms]
Sep  6 10:51:28.450: INFO: Created: latency-svc-mwjm4
Sep  6 10:51:28.457: INFO: Got endpoints: latency-svc-mwjm4 [282.336912ms]
Sep  6 10:51:28.473: INFO: Created: latency-svc-jbdwr
Sep  6 10:51:28.477: INFO: Got endpoints: latency-svc-jbdwr [281.453327ms]
Sep  6 10:51:28.484: INFO: Created: latency-svc-qxxwr
Sep  6 10:51:28.492: INFO: Got endpoints: latency-svc-qxxwr [283.380557ms]
Sep  6 10:51:28.496: INFO: Created: latency-svc-bbh2l
Sep  6 10:51:28.500: INFO: Got endpoints: latency-svc-bbh2l [257.139107ms]
Sep  6 10:51:28.508: INFO: Created: latency-svc-8nkf2
Sep  6 10:51:28.513: INFO: Got endpoints: latency-svc-8nkf2 [231.420303ms]
Sep  6 10:51:28.518: INFO: Created: latency-svc-m2zbf
Sep  6 10:51:28.522: INFO: Got endpoints: latency-svc-m2zbf [229.871302ms]
Sep  6 10:51:28.532: INFO: Created: latency-svc-rzhnz
Sep  6 10:51:28.535: INFO: Got endpoints: latency-svc-rzhnz [212.882509ms]
Sep  6 10:51:28.542: INFO: Created: latency-svc-gfbwx
Sep  6 10:51:28.555: INFO: Got endpoints: latency-svc-gfbwx [222.431423ms]
Sep  6 10:51:28.566: INFO: Created: latency-svc-7gvsd
Sep  6 10:51:28.566: INFO: Got endpoints: latency-svc-7gvsd [212.245655ms]
Sep  6 10:51:28.575: INFO: Created: latency-svc-gkjdh
Sep  6 10:51:28.583: INFO: Created: latency-svc-fjktp
Sep  6 10:51:28.583: INFO: Got endpoints: latency-svc-gkjdh [217.776987ms]
Sep  6 10:51:28.587: INFO: Got endpoints: latency-svc-fjktp [191.93323ms]
Sep  6 10:51:28.597: INFO: Created: latency-svc-8x2zr
Sep  6 10:51:28.605: INFO: Got endpoints: latency-svc-8x2zr [198.77797ms]
Sep  6 10:51:28.606: INFO: Created: latency-svc-7hjk4
Sep  6 10:51:28.618: INFO: Created: latency-svc-tgrnz
Sep  6 10:51:28.626: INFO: Got endpoints: latency-svc-tgrnz [189.604611ms]
Sep  6 10:51:28.627: INFO: Got endpoints: latency-svc-7hjk4 [205.480717ms]
Sep  6 10:51:28.635: INFO: Created: latency-svc-nxdtx
Sep  6 10:51:28.639: INFO: Got endpoints: latency-svc-nxdtx [195.091763ms]
Sep  6 10:51:28.648: INFO: Created: latency-svc-k4tmf
Sep  6 10:51:28.656: INFO: Created: latency-svc-ts9z5
Sep  6 10:51:28.657: INFO: Got endpoints: latency-svc-k4tmf [199.96001ms]
Sep  6 10:51:28.667: INFO: Got endpoints: latency-svc-ts9z5 [190.322173ms]
Sep  6 10:51:28.673: INFO: Created: latency-svc-dpj8d
Sep  6 10:51:28.678: INFO: Got endpoints: latency-svc-dpj8d [185.010158ms]
Sep  6 10:51:28.684: INFO: Created: latency-svc-8wchk
Sep  6 10:51:28.687: INFO: Got endpoints: latency-svc-8wchk [186.957267ms]
Sep  6 10:51:28.691: INFO: Created: latency-svc-mlxm4
Sep  6 10:51:28.698: INFO: Got endpoints: latency-svc-mlxm4 [184.829407ms]
Sep  6 10:51:28.702: INFO: Created: latency-svc-8z7hq
Sep  6 10:51:28.707: INFO: Got endpoints: latency-svc-8z7hq [184.711002ms]
Sep  6 10:51:28.710: INFO: Created: latency-svc-q68t5
Sep  6 10:51:28.713: INFO: Got endpoints: latency-svc-q68t5 [178.066213ms]
Sep  6 10:51:28.720: INFO: Created: latency-svc-hshnj
Sep  6 10:51:28.725: INFO: Got endpoints: latency-svc-hshnj [169.642152ms]
Sep  6 10:51:28.728: INFO: Created: latency-svc-n4rvl
Sep  6 10:51:28.740: INFO: Got endpoints: latency-svc-n4rvl [173.371091ms]
Sep  6 10:51:28.743: INFO: Created: latency-svc-n7j7b
Sep  6 10:51:28.752: INFO: Got endpoints: latency-svc-n7j7b [168.513053ms]
Sep  6 10:51:28.756: INFO: Created: latency-svc-8w4h2
Sep  6 10:51:28.762: INFO: Got endpoints: latency-svc-8w4h2 [175.098501ms]
Sep  6 10:51:28.764: INFO: Created: latency-svc-bxk98
Sep  6 10:51:28.773: INFO: Got endpoints: latency-svc-bxk98 [167.373012ms]
Sep  6 10:51:28.782: INFO: Created: latency-svc-sbltm
Sep  6 10:51:28.791: INFO: Got endpoints: latency-svc-sbltm [164.431305ms]
Sep  6 10:51:28.802: INFO: Created: latency-svc-74wfr
Sep  6 10:51:28.815: INFO: Got endpoints: latency-svc-74wfr [188.670019ms]
Sep  6 10:51:28.821: INFO: Created: latency-svc-vcwjp
Sep  6 10:51:28.823: INFO: Got endpoints: latency-svc-vcwjp [183.737019ms]
Sep  6 10:51:28.824: INFO: Created: latency-svc-knztc
Sep  6 10:51:28.833: INFO: Got endpoints: latency-svc-knztc [175.726527ms]
Sep  6 10:51:28.846: INFO: Created: latency-svc-gfsbs
Sep  6 10:51:28.850: INFO: Got endpoints: latency-svc-gfsbs [182.977353ms]
Sep  6 10:51:28.857: INFO: Created: latency-svc-xkfr2
Sep  6 10:51:28.864: INFO: Got endpoints: latency-svc-xkfr2 [186.374112ms]
Sep  6 10:51:28.866: INFO: Created: latency-svc-hhpg5
Sep  6 10:51:28.883: INFO: Created: latency-svc-tjxlx
Sep  6 10:51:28.883: INFO: Got endpoints: latency-svc-hhpg5 [196.302227ms]
Sep  6 10:51:28.895: INFO: Got endpoints: latency-svc-tjxlx [197.502856ms]
Sep  6 10:51:28.896: INFO: Created: latency-svc-2rz8x
Sep  6 10:51:28.909: INFO: Created: latency-svc-6cdfh
Sep  6 10:51:28.919: INFO: Got endpoints: latency-svc-2rz8x [212.549334ms]
Sep  6 10:51:28.920: INFO: Created: latency-svc-pjdrz
Sep  6 10:51:28.921: INFO: Got endpoints: latency-svc-6cdfh [207.651172ms]
Sep  6 10:51:28.931: INFO: Got endpoints: latency-svc-pjdrz [205.800363ms]
Sep  6 10:51:28.936: INFO: Created: latency-svc-nzl2r
Sep  6 10:51:28.948: INFO: Got endpoints: latency-svc-nzl2r [208.586973ms]
Sep  6 10:51:28.956: INFO: Created: latency-svc-26dr2
Sep  6 10:51:28.978: INFO: Got endpoints: latency-svc-26dr2 [226.294038ms]
Sep  6 10:51:28.982: INFO: Created: latency-svc-dmqm8
Sep  6 10:51:28.992: INFO: Got endpoints: latency-svc-dmqm8 [230.073032ms]
Sep  6 10:51:28.998: INFO: Created: latency-svc-6rxjt
Sep  6 10:51:29.014: INFO: Created: latency-svc-wjl2r
Sep  6 10:51:29.018: INFO: Got endpoints: latency-svc-wjl2r [227.562391ms]
Sep  6 10:51:29.018: INFO: Got endpoints: latency-svc-6rxjt [245.555105ms]
Sep  6 10:51:29.034: INFO: Created: latency-svc-pnr5g
Sep  6 10:51:29.040: INFO: Created: latency-svc-9hlxs
Sep  6 10:51:29.054: INFO: Got endpoints: latency-svc-pnr5g [238.929129ms]
Sep  6 10:51:29.055: INFO: Got endpoints: latency-svc-9hlxs [232.373922ms]
Sep  6 10:51:29.062: INFO: Created: latency-svc-mjpxs
Sep  6 10:51:29.066: INFO: Got endpoints: latency-svc-mjpxs [232.969963ms]
Sep  6 10:51:29.081: INFO: Created: latency-svc-zr2pw
Sep  6 10:51:29.086: INFO: Created: latency-svc-76ch6
Sep  6 10:51:29.089: INFO: Got endpoints: latency-svc-zr2pw [239.227734ms]
Sep  6 10:51:29.096: INFO: Got endpoints: latency-svc-76ch6 [231.598177ms]
Sep  6 10:51:29.100: INFO: Created: latency-svc-xlc6j
Sep  6 10:51:29.117: INFO: Got endpoints: latency-svc-xlc6j [233.367668ms]
Sep  6 10:51:29.120: INFO: Created: latency-svc-cpg57
Sep  6 10:51:29.133: INFO: Created: latency-svc-mtwb9
Sep  6 10:51:29.139: INFO: Got endpoints: latency-svc-cpg57 [243.986141ms]
Sep  6 10:51:29.149: INFO: Got endpoints: latency-svc-mtwb9 [229.457154ms]
Sep  6 10:51:29.157: INFO: Created: latency-svc-8j96t
Sep  6 10:51:29.163: INFO: Got endpoints: latency-svc-8j96t [241.635803ms]
Sep  6 10:51:29.168: INFO: Created: latency-svc-mbs2f
Sep  6 10:51:29.175: INFO: Got endpoints: latency-svc-mbs2f [244.116016ms]
Sep  6 10:51:29.180: INFO: Created: latency-svc-8z2vd
Sep  6 10:51:29.203: INFO: Got endpoints: latency-svc-8z2vd [254.974814ms]
Sep  6 10:51:29.216: INFO: Created: latency-svc-m4529
Sep  6 10:51:29.216: INFO: Got endpoints: latency-svc-m4529 [238.076694ms]
Sep  6 10:51:29.227: INFO: Created: latency-svc-fzfx9
Sep  6 10:51:29.233: INFO: Got endpoints: latency-svc-fzfx9 [240.985218ms]
Sep  6 10:51:29.246: INFO: Created: latency-svc-zdjvw
Sep  6 10:51:29.247: INFO: Got endpoints: latency-svc-zdjvw [228.886603ms]
Sep  6 10:51:29.259: INFO: Created: latency-svc-m7m2h
Sep  6 10:51:29.276: INFO: Created: latency-svc-5wjmj
Sep  6 10:51:29.279: INFO: Got endpoints: latency-svc-m7m2h [260.037156ms]
Sep  6 10:51:29.284: INFO: Got endpoints: latency-svc-5wjmj [230.029246ms]
Sep  6 10:51:29.296: INFO: Created: latency-svc-zpx5b
Sep  6 10:51:29.304: INFO: Got endpoints: latency-svc-zpx5b [248.833143ms]
Sep  6 10:51:29.313: INFO: Created: latency-svc-qzx87
Sep  6 10:51:29.324: INFO: Created: latency-svc-v9zlh
Sep  6 10:51:29.325: INFO: Got endpoints: latency-svc-qzx87 [258.77929ms]
Sep  6 10:51:29.329: INFO: Got endpoints: latency-svc-v9zlh [239.097551ms]
Sep  6 10:51:29.353: INFO: Created: latency-svc-f2qvk
Sep  6 10:51:29.366: INFO: Created: latency-svc-s4clh
Sep  6 10:51:29.371: INFO: Got endpoints: latency-svc-f2qvk [274.887567ms]
Sep  6 10:51:29.389: INFO: Got endpoints: latency-svc-s4clh [271.982701ms]
Sep  6 10:51:29.390: INFO: Created: latency-svc-hn8kt
Sep  6 10:51:29.400: INFO: Got endpoints: latency-svc-hn8kt [260.70347ms]
Sep  6 10:51:29.405: INFO: Created: latency-svc-ndhf4
Sep  6 10:51:29.410: INFO: Created: latency-svc-4grd9
Sep  6 10:51:29.414: INFO: Got endpoints: latency-svc-ndhf4 [264.905052ms]
Sep  6 10:51:29.424: INFO: Got endpoints: latency-svc-4grd9 [260.633975ms]
Sep  6 10:51:29.430: INFO: Created: latency-svc-cncln
Sep  6 10:51:29.432: INFO: Got endpoints: latency-svc-cncln [256.497506ms]
Sep  6 10:51:29.442: INFO: Created: latency-svc-z8lmv
Sep  6 10:51:29.453: INFO: Got endpoints: latency-svc-z8lmv [249.590082ms]
Sep  6 10:51:29.470: INFO: Created: latency-svc-kblcd
Sep  6 10:51:29.475: INFO: Got endpoints: latency-svc-kblcd [258.799899ms]
Sep  6 10:51:29.479: INFO: Created: latency-svc-trtr4
Sep  6 10:51:29.483: INFO: Got endpoints: latency-svc-trtr4 [249.692256ms]
Sep  6 10:51:29.492: INFO: Created: latency-svc-l2qnm
Sep  6 10:51:29.504: INFO: Created: latency-svc-5wtlx
Sep  6 10:51:29.510: INFO: Got endpoints: latency-svc-l2qnm [262.226785ms]
Sep  6 10:51:29.518: INFO: Got endpoints: latency-svc-5wtlx [238.93759ms]
Sep  6 10:51:29.522: INFO: Created: latency-svc-2vmwv
Sep  6 10:51:29.529: INFO: Got endpoints: latency-svc-2vmwv [244.665687ms]
Sep  6 10:51:29.531: INFO: Created: latency-svc-g7z92
Sep  6 10:51:29.539: INFO: Got endpoints: latency-svc-g7z92 [234.900034ms]
Sep  6 10:51:29.544: INFO: Created: latency-svc-6r5f9
Sep  6 10:51:29.549: INFO: Got endpoints: latency-svc-6r5f9 [224.299723ms]
Sep  6 10:51:29.574: INFO: Created: latency-svc-7nq2x
Sep  6 10:51:29.574: INFO: Got endpoints: latency-svc-7nq2x [245.231429ms]
Sep  6 10:51:29.586: INFO: Created: latency-svc-z8nt2
Sep  6 10:51:29.598: INFO: Got endpoints: latency-svc-z8nt2 [227.241517ms]
Sep  6 10:51:29.600: INFO: Created: latency-svc-mzxks
Sep  6 10:51:29.606: INFO: Created: latency-svc-8546l
Sep  6 10:51:29.615: INFO: Got endpoints: latency-svc-mzxks [225.74894ms]
Sep  6 10:51:29.616: INFO: Got endpoints: latency-svc-8546l [215.325619ms]
Sep  6 10:51:29.624: INFO: Created: latency-svc-sjczq
Sep  6 10:51:29.627: INFO: Got endpoints: latency-svc-sjczq [213.063092ms]
Sep  6 10:51:29.634: INFO: Created: latency-svc-8zz5h
Sep  6 10:51:29.645: INFO: Created: latency-svc-p2jww
Sep  6 10:51:29.650: INFO: Got endpoints: latency-svc-8zz5h [226.020443ms]
Sep  6 10:51:29.654: INFO: Got endpoints: latency-svc-p2jww [221.627202ms]
Sep  6 10:51:29.659: INFO: Created: latency-svc-9wb8p
Sep  6 10:51:29.679: INFO: Got endpoints: latency-svc-9wb8p [226.113302ms]
Sep  6 10:51:29.689: INFO: Created: latency-svc-fjvjg
Sep  6 10:51:29.696: INFO: Got endpoints: latency-svc-fjvjg [220.780774ms]
Sep  6 10:51:29.700: INFO: Created: latency-svc-v7hjz
Sep  6 10:51:29.719: INFO: Created: latency-svc-ntwgb
Sep  6 10:51:29.719: INFO: Got endpoints: latency-svc-ntwgb [209.278516ms]
Sep  6 10:51:29.719: INFO: Got endpoints: latency-svc-v7hjz [236.083061ms]
Sep  6 10:51:29.727: INFO: Created: latency-svc-f2nsh
Sep  6 10:51:29.734: INFO: Got endpoints: latency-svc-f2nsh [215.904868ms]
Sep  6 10:51:29.736: INFO: Created: latency-svc-tsrhk
Sep  6 10:51:29.745: INFO: Got endpoints: latency-svc-tsrhk [215.710325ms]
Sep  6 10:51:29.748: INFO: Created: latency-svc-nsbxv
Sep  6 10:51:29.755: INFO: Got endpoints: latency-svc-nsbxv [215.72614ms]
Sep  6 10:51:29.762: INFO: Created: latency-svc-jnzst
Sep  6 10:51:29.768: INFO: Got endpoints: latency-svc-jnzst [218.679691ms]
Sep  6 10:51:29.776: INFO: Created: latency-svc-89vbl
Sep  6 10:51:29.791: INFO: Got endpoints: latency-svc-89vbl [217.316857ms]
Sep  6 10:51:29.791: INFO: Created: latency-svc-42m9q
Sep  6 10:51:29.809: INFO: Got endpoints: latency-svc-42m9q [210.393723ms]
Sep  6 10:51:29.818: INFO: Created: latency-svc-6dxrf
Sep  6 10:51:29.837: INFO: Got endpoints: latency-svc-6dxrf [222.050645ms]
Sep  6 10:51:29.848: INFO: Created: latency-svc-qnhvz
Sep  6 10:51:29.860: INFO: Got endpoints: latency-svc-qnhvz [244.381899ms]
Sep  6 10:51:29.869: INFO: Created: latency-svc-nwq9f
Sep  6 10:51:29.876: INFO: Got endpoints: latency-svc-nwq9f [249.268183ms]
Sep  6 10:51:29.887: INFO: Created: latency-svc-57lzj
Sep  6 10:51:29.896: INFO: Got endpoints: latency-svc-57lzj [242.733348ms]
Sep  6 10:51:29.911: INFO: Created: latency-svc-zn895
Sep  6 10:51:29.916: INFO: Got endpoints: latency-svc-zn895 [265.654079ms]
Sep  6 10:51:29.922: INFO: Created: latency-svc-45qvb
Sep  6 10:51:29.933: INFO: Created: latency-svc-m8nlm
Sep  6 10:51:29.939: INFO: Got endpoints: latency-svc-45qvb [259.288837ms]
Sep  6 10:51:29.942: INFO: Got endpoints: latency-svc-m8nlm [246.143876ms]
Sep  6 10:51:29.948: INFO: Created: latency-svc-pnq6t
Sep  6 10:51:29.966: INFO: Got endpoints: latency-svc-pnq6t [247.240543ms]
Sep  6 10:51:29.976: INFO: Created: latency-svc-7qlxs
Sep  6 10:51:29.981: INFO: Got endpoints: latency-svc-7qlxs [261.543463ms]
Sep  6 10:51:29.986: INFO: Created: latency-svc-6zjwg
Sep  6 10:51:29.992: INFO: Got endpoints: latency-svc-6zjwg [257.743345ms]
Sep  6 10:51:30.014: INFO: Created: latency-svc-ht7wt
Sep  6 10:51:30.017: INFO: Created: latency-svc-cp8mm
Sep  6 10:51:30.027: INFO: Got endpoints: latency-svc-cp8mm [281.924107ms]
Sep  6 10:51:30.031: INFO: Got endpoints: latency-svc-ht7wt [275.677508ms]
Sep  6 10:51:30.039: INFO: Created: latency-svc-2hdh9
Sep  6 10:51:30.048: INFO: Got endpoints: latency-svc-2hdh9 [280.017913ms]
Sep  6 10:51:30.049: INFO: Created: latency-svc-77z86
Sep  6 10:51:30.058: INFO: Got endpoints: latency-svc-77z86 [266.701942ms]
Sep  6 10:51:30.062: INFO: Created: latency-svc-r8c8l
Sep  6 10:51:30.069: INFO: Got endpoints: latency-svc-r8c8l [260.348523ms]
Sep  6 10:51:30.079: INFO: Created: latency-svc-bgsg9
Sep  6 10:51:30.087: INFO: Got endpoints: latency-svc-bgsg9 [249.602588ms]
Sep  6 10:51:30.098: INFO: Created: latency-svc-c9hrl
Sep  6 10:51:30.101: INFO: Got endpoints: latency-svc-c9hrl [241.445719ms]
Sep  6 10:51:30.115: INFO: Created: latency-svc-cf5m9
Sep  6 10:51:30.131: INFO: Created: latency-svc-2cs2g
Sep  6 10:51:30.136: INFO: Got endpoints: latency-svc-cf5m9 [260.124197ms]
Sep  6 10:51:30.140: INFO: Got endpoints: latency-svc-2cs2g [243.418985ms]
Sep  6 10:51:30.153: INFO: Created: latency-svc-lgd6t
Sep  6 10:51:30.156: INFO: Created: latency-svc-78xgl
Sep  6 10:51:30.159: INFO: Got endpoints: latency-svc-lgd6t [243.67658ms]
Sep  6 10:51:30.165: INFO: Got endpoints: latency-svc-78xgl [226.147595ms]
Sep  6 10:51:30.172: INFO: Created: latency-svc-wb7jd
Sep  6 10:51:30.182: INFO: Got endpoints: latency-svc-wb7jd [239.936554ms]
Sep  6 10:51:30.192: INFO: Created: latency-svc-4dkl7
Sep  6 10:51:30.199: INFO: Got endpoints: latency-svc-4dkl7 [232.528994ms]
Sep  6 10:51:30.201: INFO: Created: latency-svc-c65lg
Sep  6 10:51:30.210: INFO: Got endpoints: latency-svc-c65lg [229.304867ms]
Sep  6 10:51:30.211: INFO: Created: latency-svc-tmrc2
Sep  6 10:51:30.217: INFO: Got endpoints: latency-svc-tmrc2 [225.662822ms]
Sep  6 10:51:30.220: INFO: Created: latency-svc-cx8nr
Sep  6 10:51:30.231: INFO: Got endpoints: latency-svc-cx8nr [203.874045ms]
Sep  6 10:51:30.248: INFO: Created: latency-svc-n5lk4
Sep  6 10:51:30.255: INFO: Got endpoints: latency-svc-n5lk4 [224.408364ms]
Sep  6 10:51:30.260: INFO: Created: latency-svc-k22tn
Sep  6 10:51:30.269: INFO: Got endpoints: latency-svc-k22tn [220.414923ms]
Sep  6 10:51:30.277: INFO: Created: latency-svc-mqlgh
Sep  6 10:51:30.285: INFO: Got endpoints: latency-svc-mqlgh [226.193712ms]
Sep  6 10:51:30.290: INFO: Created: latency-svc-hztl9
Sep  6 10:51:30.298: INFO: Got endpoints: latency-svc-hztl9 [228.71622ms]
Sep  6 10:51:30.299: INFO: Created: latency-svc-rkvkx
Sep  6 10:51:30.310: INFO: Got endpoints: latency-svc-rkvkx [222.545351ms]
Sep  6 10:51:30.315: INFO: Created: latency-svc-fsf5j
Sep  6 10:51:30.323: INFO: Got endpoints: latency-svc-fsf5j [221.300446ms]
Sep  6 10:51:30.334: INFO: Created: latency-svc-t4d29
Sep  6 10:51:30.343: INFO: Got endpoints: latency-svc-t4d29 [206.732015ms]
Sep  6 10:51:30.351: INFO: Created: latency-svc-wspg4
Sep  6 10:51:30.356: INFO: Got endpoints: latency-svc-wspg4 [216.239947ms]
Sep  6 10:51:30.356: INFO: Latencies: [33.899429ms 61.012114ms 65.888276ms 81.437797ms 123.698202ms 128.155723ms 140.951377ms 164.431305ms 167.373012ms 168.513053ms 168.6018ms 169.642152ms 171.645141ms 173.371091ms 175.098501ms 175.688662ms 175.726527ms 178.066213ms 180.232384ms 181.538292ms 182.005529ms 182.977353ms 183.737019ms 184.711002ms 184.829407ms 185.010158ms 186.374112ms 186.957267ms 187.545727ms 188.265004ms 188.548722ms 188.670019ms 189.007783ms 189.604611ms 190.322173ms 191.93323ms 191.94839ms 194.325462ms 195.004472ms 195.091763ms 196.286972ms 196.302227ms 197.502856ms 197.937337ms 198.203648ms 198.77797ms 199.252324ms 199.304479ms 199.96001ms 203.111246ms 203.874045ms 203.898073ms 205.082886ms 205.480717ms 205.800363ms 206.732015ms 207.651172ms 208.586973ms 208.616338ms 208.704047ms 209.278516ms 210.393723ms 210.549002ms 210.61204ms 210.672752ms 210.990534ms 212.245655ms 212.549334ms 212.882509ms 213.063092ms 213.418208ms 215.325619ms 215.710325ms 215.72614ms 215.904868ms 216.239947ms 216.804761ms 217.316857ms 217.57723ms 217.776987ms 218.679691ms 220.283435ms 220.414923ms 220.780774ms 220.858259ms 220.966831ms 221.300446ms 221.627202ms 222.050645ms 222.08011ms 222.431423ms 222.545351ms 223.964645ms 224.040583ms 224.045671ms 224.299723ms 224.408364ms 224.463895ms 225.272844ms 225.640764ms 225.662822ms 225.74894ms 225.9984ms 226.020443ms 226.113302ms 226.147595ms 226.193712ms 226.294038ms 226.733963ms 227.241517ms 227.562391ms 228.71622ms 228.886603ms 229.304867ms 229.457154ms 229.871302ms 230.029246ms 230.073032ms 230.786841ms 231.00737ms 231.420303ms 231.598177ms 232.373922ms 232.528994ms 232.969963ms 233.367668ms 233.758081ms 234.900034ms 235.004773ms 236.083061ms 236.970994ms 238.076694ms 238.929129ms 238.93759ms 239.097551ms 239.227734ms 239.936554ms 240.985218ms 241.445719ms 241.635803ms 242.733348ms 243.418985ms 243.67658ms 243.986141ms 244.116016ms 244.381899ms 244.665687ms 245.231429ms 245.555105ms 246.143876ms 247.240543ms 248.833143ms 249.268183ms 249.590082ms 249.602588ms 249.692256ms 249.765639ms 251.169461ms 251.531294ms 254.974814ms 256.497506ms 257.139107ms 257.743345ms 258.77929ms 258.799899ms 259.288837ms 260.037156ms 260.124197ms 260.348523ms 260.633975ms 260.70347ms 261.543463ms 262.226785ms 264.905052ms 265.654079ms 266.701942ms 269.716722ms 270.187967ms 271.894427ms 271.982701ms 272.097132ms 274.476211ms 274.887567ms 275.107428ms 275.329136ms 275.677508ms 277.08061ms 279.981856ms 280.017913ms 280.976173ms 281.453327ms 281.924107ms 282.336912ms 283.380557ms 287.859497ms 288.203153ms 289.750573ms 290.870272ms 300.672519ms 321.927498ms]
Sep  6 10:51:30.356: INFO: 50 %ile: 225.662822ms
Sep  6 10:51:30.356: INFO: 90 %ile: 272.097132ms
Sep  6 10:51:30.356: INFO: 99 %ile: 300.672519ms
Sep  6 10:51:30.356: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:51:30.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6904" for this suite.
•{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":78,"skipped":1254,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:51:30.366: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 10:51:30.419: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8f38ccc-3ab6-4219-bf0e-0bb641d56ca4" in namespace "projected-8100" to be "Succeeded or Failed"
Sep  6 10:51:30.425: INFO: Pod "downwardapi-volume-a8f38ccc-3ab6-4219-bf0e-0bb641d56ca4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.972024ms
Sep  6 10:51:32.428: INFO: Pod "downwardapi-volume-a8f38ccc-3ab6-4219-bf0e-0bb641d56ca4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009666232s
STEP: Saw pod success
Sep  6 10:51:32.428: INFO: Pod "downwardapi-volume-a8f38ccc-3ab6-4219-bf0e-0bb641d56ca4" satisfied condition "Succeeded or Failed"
Sep  6 10:51:32.435: INFO: Trying to get logs from node vm114012 pod downwardapi-volume-a8f38ccc-3ab6-4219-bf0e-0bb641d56ca4 container client-container: <nil>
STEP: delete the pod
Sep  6 10:51:32.454: INFO: Waiting for pod downwardapi-volume-a8f38ccc-3ab6-4219-bf0e-0bb641d56ca4 to disappear
Sep  6 10:51:32.458: INFO: Pod downwardapi-volume-a8f38ccc-3ab6-4219-bf0e-0bb641d56ca4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:51:32.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8100" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":79,"skipped":1264,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:51:32.468: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-792faa95-f4bc-44c0-8d79-33743826af6e
STEP: Creating a pod to test consume secrets
Sep  6 10:51:32.508: INFO: Waiting up to 5m0s for pod "pod-secrets-baf2e92c-14e2-49af-8070-1c8d4d6b52f9" in namespace "secrets-6" to be "Succeeded or Failed"
Sep  6 10:51:32.512: INFO: Pod "pod-secrets-baf2e92c-14e2-49af-8070-1c8d4d6b52f9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.567535ms
Sep  6 10:51:34.516: INFO: Pod "pod-secrets-baf2e92c-14e2-49af-8070-1c8d4d6b52f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008021499s
STEP: Saw pod success
Sep  6 10:51:34.516: INFO: Pod "pod-secrets-baf2e92c-14e2-49af-8070-1c8d4d6b52f9" satisfied condition "Succeeded or Failed"
Sep  6 10:51:34.521: INFO: Trying to get logs from node vm114012 pod pod-secrets-baf2e92c-14e2-49af-8070-1c8d4d6b52f9 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 10:51:34.536: INFO: Waiting for pod pod-secrets-baf2e92c-14e2-49af-8070-1c8d4d6b52f9 to disappear
Sep  6 10:51:34.539: INFO: Pod pod-secrets-baf2e92c-14e2-49af-8070-1c8d4d6b52f9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:51:34.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":80,"skipped":1275,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:51:34.549: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8129
STEP: creating service affinity-nodeport-transition in namespace services-8129
STEP: creating replication controller affinity-nodeport-transition in namespace services-8129
I0906 10:51:34.605854      20 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-8129, replica count: 3
I0906 10:51:37.656427      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 10:51:37.735: INFO: Creating new exec pod
Sep  6 10:51:40.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8129 exec execpod-affinitybj66x -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Sep  6 10:51:41.057: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep  6 10:51:41.057: INFO: stdout: ""
Sep  6 10:51:41.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8129 exec execpod-affinitybj66x -- /bin/sh -x -c nc -zv -t -w 2 10.101.162.28 80'
Sep  6 10:51:41.227: INFO: stderr: "+ nc -zv -t -w 2 10.101.162.28 80\nConnection to 10.101.162.28 80 port [tcp/http] succeeded!\n"
Sep  6 10:51:41.227: INFO: stdout: ""
Sep  6 10:51:41.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8129 exec execpod-affinitybj66x -- /bin/sh -x -c nc -zv -t -w 2 172.16.114.11 31184'
Sep  6 10:51:41.396: INFO: stderr: "+ nc -zv -t -w 2 172.16.114.11 31184\nConnection to 172.16.114.11 31184 port [tcp/31184] succeeded!\n"
Sep  6 10:51:41.396: INFO: stdout: ""
Sep  6 10:51:41.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8129 exec execpod-affinitybj66x -- /bin/sh -x -c nc -zv -t -w 2 172.16.114.13 31184'
Sep  6 10:51:41.541: INFO: stderr: "+ nc -zv -t -w 2 172.16.114.13 31184\nConnection to 172.16.114.13 31184 port [tcp/31184] succeeded!\n"
Sep  6 10:51:41.541: INFO: stdout: ""
Sep  6 10:51:41.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8129 exec execpod-affinitybj66x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.114.11:31184/ ; done'
Sep  6 10:51:41.802: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n"
Sep  6 10:51:41.802: INFO: stdout: "\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m"
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:51:41.802: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:11.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8129 exec execpod-affinitybj66x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.114.11:31184/ ; done'
Sep  6 10:52:12.042: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n"
Sep  6 10:52:12.042: INFO: stdout: "\naffinity-nodeport-transition-k89tt\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-ddjd2\naffinity-nodeport-transition-k89tt\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-ddjd2\naffinity-nodeport-transition-k89tt\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-ddjd2\naffinity-nodeport-transition-k89tt\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-ddjd2\naffinity-nodeport-transition-k89tt\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-ddjd2\naffinity-nodeport-transition-k89tt"
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-k89tt
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-ddjd2
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-k89tt
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-ddjd2
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-k89tt
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-ddjd2
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-k89tt
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-ddjd2
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-k89tt
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-ddjd2
Sep  6 10:52:12.042: INFO: Received response from host: affinity-nodeport-transition-k89tt
Sep  6 10:52:12.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8129 exec execpod-affinitybj66x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.114.11:31184/ ; done'
Sep  6 10:52:12.315: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31184/\n"
Sep  6 10:52:12.315: INFO: stdout: "\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m\naffinity-nodeport-transition-lzg4m"
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Received response from host: affinity-nodeport-transition-lzg4m
Sep  6 10:52:12.315: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8129, will wait for the garbage collector to delete the pods
Sep  6 10:52:12.394: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.48173ms
Sep  6 10:52:12.495: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.242138ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:52:27.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8129" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:53.388 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":81,"skipped":1276,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:52:27.938: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  6 10:52:27.988: INFO: Waiting up to 5m0s for pod "pod-c10f655d-cd44-4c8e-9105-f9d0bfeeb7d6" in namespace "emptydir-3160" to be "Succeeded or Failed"
Sep  6 10:52:27.992: INFO: Pod "pod-c10f655d-cd44-4c8e-9105-f9d0bfeeb7d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.436664ms
Sep  6 10:52:29.996: INFO: Pod "pod-c10f655d-cd44-4c8e-9105-f9d0bfeeb7d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007951718s
STEP: Saw pod success
Sep  6 10:52:29.996: INFO: Pod "pod-c10f655d-cd44-4c8e-9105-f9d0bfeeb7d6" satisfied condition "Succeeded or Failed"
Sep  6 10:52:29.999: INFO: Trying to get logs from node vm114011 pod pod-c10f655d-cd44-4c8e-9105-f9d0bfeeb7d6 container test-container: <nil>
STEP: delete the pod
Sep  6 10:52:30.015: INFO: Waiting for pod pod-c10f655d-cd44-4c8e-9105-f9d0bfeeb7d6 to disappear
Sep  6 10:52:30.017: INFO: Pod pod-c10f655d-cd44-4c8e-9105-f9d0bfeeb7d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:52:30.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3160" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":82,"skipped":1308,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:52:30.027: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 10:52:30.333: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  6 10:52:32.349: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798058350, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798058350, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798058350, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798058350, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 10:52:35.361: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:52:35.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7041" for this suite.
STEP: Destroying namespace "webhook-7041-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.569 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":83,"skipped":1308,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:52:35.597: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 10:52:35.864: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 10:52:38.880: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:52:48.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5530" for this suite.
STEP: Destroying namespace "webhook-5530-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.446 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":84,"skipped":1323,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:52:49.045: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Sep  6 10:52:49.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7908 create -f -'
Sep  6 10:52:49.405: INFO: stderr: ""
Sep  6 10:52:49.405: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 10:52:49.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7908 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 10:52:49.483: INFO: stderr: ""
Sep  6 10:52:49.483: INFO: stdout: "update-demo-nautilus-dtx9x update-demo-nautilus-qr4c4 "
Sep  6 10:52:49.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7908 get pods update-demo-nautilus-dtx9x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 10:52:49.553: INFO: stderr: ""
Sep  6 10:52:49.553: INFO: stdout: ""
Sep  6 10:52:49.553: INFO: update-demo-nautilus-dtx9x is created but not running
Sep  6 10:52:54.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7908 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 10:52:54.627: INFO: stderr: ""
Sep  6 10:52:54.627: INFO: stdout: "update-demo-nautilus-dtx9x update-demo-nautilus-qr4c4 "
Sep  6 10:52:54.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7908 get pods update-demo-nautilus-dtx9x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 10:52:54.687: INFO: stderr: ""
Sep  6 10:52:54.687: INFO: stdout: ""
Sep  6 10:52:54.687: INFO: update-demo-nautilus-dtx9x is created but not running
Sep  6 10:52:59.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7908 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 10:52:59.749: INFO: stderr: ""
Sep  6 10:52:59.749: INFO: stdout: "update-demo-nautilus-dtx9x update-demo-nautilus-qr4c4 "
Sep  6 10:52:59.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7908 get pods update-demo-nautilus-dtx9x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 10:52:59.813: INFO: stderr: ""
Sep  6 10:52:59.813: INFO: stdout: "true"
Sep  6 10:52:59.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7908 get pods update-demo-nautilus-dtx9x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  6 10:52:59.880: INFO: stderr: ""
Sep  6 10:52:59.880: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 10:52:59.880: INFO: validating pod update-demo-nautilus-dtx9x
Sep  6 10:52:59.884: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 10:52:59.884: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 10:52:59.884: INFO: update-demo-nautilus-dtx9x is verified up and running
Sep  6 10:52:59.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7908 get pods update-demo-nautilus-qr4c4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 10:52:59.951: INFO: stderr: ""
Sep  6 10:52:59.951: INFO: stdout: "true"
Sep  6 10:52:59.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7908 get pods update-demo-nautilus-qr4c4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  6 10:53:00.012: INFO: stderr: ""
Sep  6 10:53:00.012: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 10:53:00.012: INFO: validating pod update-demo-nautilus-qr4c4
Sep  6 10:53:00.016: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 10:53:00.016: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 10:53:00.016: INFO: update-demo-nautilus-qr4c4 is verified up and running
STEP: using delete to clean up resources
Sep  6 10:53:00.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7908 delete --grace-period=0 --force -f -'
Sep  6 10:53:00.077: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 10:53:00.077: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  6 10:53:00.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7908 get rc,svc -l name=update-demo --no-headers'
Sep  6 10:53:00.161: INFO: stderr: "No resources found in kubectl-7908 namespace.\n"
Sep  6 10:53:00.161: INFO: stdout: ""
Sep  6 10:53:00.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7908 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 10:53:00.232: INFO: stderr: ""
Sep  6 10:53:00.232: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:53:00.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7908" for this suite.

• [SLOW TEST:11.202 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":85,"skipped":1358,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:53:00.246: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:53:06.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3565" for this suite.
STEP: Destroying namespace "nsdeletetest-2417" for this suite.
Sep  6 10:53:06.402: INFO: Namespace nsdeletetest-2417 was already deleted
STEP: Destroying namespace "nsdeletetest-1631" for this suite.

• [SLOW TEST:6.159 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":86,"skipped":1361,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:53:06.406: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Sep  6 10:53:06.440: INFO: Waiting up to 5m0s for pod "var-expansion-d70e3643-ffbe-41f7-b731-a0999fa176ce" in namespace "var-expansion-9257" to be "Succeeded or Failed"
Sep  6 10:53:06.447: INFO: Pod "var-expansion-d70e3643-ffbe-41f7-b731-a0999fa176ce": Phase="Pending", Reason="", readiness=false. Elapsed: 6.721834ms
Sep  6 10:53:08.451: INFO: Pod "var-expansion-d70e3643-ffbe-41f7-b731-a0999fa176ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011151945s
STEP: Saw pod success
Sep  6 10:53:08.451: INFO: Pod "var-expansion-d70e3643-ffbe-41f7-b731-a0999fa176ce" satisfied condition "Succeeded or Failed"
Sep  6 10:53:08.454: INFO: Trying to get logs from node vm114012 pod var-expansion-d70e3643-ffbe-41f7-b731-a0999fa176ce container dapi-container: <nil>
STEP: delete the pod
Sep  6 10:53:08.472: INFO: Waiting for pod var-expansion-d70e3643-ffbe-41f7-b731-a0999fa176ce to disappear
Sep  6 10:53:08.475: INFO: Pod var-expansion-d70e3643-ffbe-41f7-b731-a0999fa176ce no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:53:08.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9257" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":87,"skipped":1370,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:53:08.483: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Sep  6 10:53:08.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-2781 cluster-info'
Sep  6 10:53:08.568: INFO: stderr: ""
Sep  6 10:53:08.568: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:53:08.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2781" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":88,"skipped":1393,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:53:08.576: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-3584
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3584
STEP: Deleting pre-stop pod
Sep  6 10:53:17.634: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:53:17.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3584" for this suite.

• [SLOW TEST:9.081 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":89,"skipped":1406,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:53:17.657: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-5615/configmap-test-9913f7e9-4b78-43d9-bd38-362d8b17b423
STEP: Creating a pod to test consume configMaps
Sep  6 10:53:17.692: INFO: Waiting up to 5m0s for pod "pod-configmaps-ea748b83-08ae-4074-a608-b463ad8009ee" in namespace "configmap-5615" to be "Succeeded or Failed"
Sep  6 10:53:17.696: INFO: Pod "pod-configmaps-ea748b83-08ae-4074-a608-b463ad8009ee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.205712ms
Sep  6 10:53:19.700: INFO: Pod "pod-configmaps-ea748b83-08ae-4074-a608-b463ad8009ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007201794s
STEP: Saw pod success
Sep  6 10:53:19.700: INFO: Pod "pod-configmaps-ea748b83-08ae-4074-a608-b463ad8009ee" satisfied condition "Succeeded or Failed"
Sep  6 10:53:19.702: INFO: Trying to get logs from node vm114012 pod pod-configmaps-ea748b83-08ae-4074-a608-b463ad8009ee container env-test: <nil>
STEP: delete the pod
Sep  6 10:53:19.722: INFO: Waiting for pod pod-configmaps-ea748b83-08ae-4074-a608-b463ad8009ee to disappear
Sep  6 10:53:19.724: INFO: Pod pod-configmaps-ea748b83-08ae-4074-a608-b463ad8009ee no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:53:19.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5615" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":90,"skipped":1424,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:53:19.738: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Sep  6 10:53:19.764: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  6 10:54:19.788: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:54:19.792: INFO: Starting informer...
STEP: Starting pods...
Sep  6 10:54:20.011: INFO: Pod1 is running on vm114011. Tainting Node
Sep  6 10:54:22.229: INFO: Pod2 is running on vm114011. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Sep  6 10:54:39.904: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep  6 10:54:59.902: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:54:59.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7762" for this suite.

• [SLOW TEST:100.218 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":91,"skipped":1429,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:54:59.956: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  6 10:54:59.986: INFO: Waiting up to 5m0s for pod "pod-d2f6ba38-4672-4957-a539-441df46d4df0" in namespace "emptydir-7925" to be "Succeeded or Failed"
Sep  6 10:54:59.998: INFO: Pod "pod-d2f6ba38-4672-4957-a539-441df46d4df0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.449287ms
Sep  6 10:55:02.002: INFO: Pod "pod-d2f6ba38-4672-4957-a539-441df46d4df0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016057277s
STEP: Saw pod success
Sep  6 10:55:02.002: INFO: Pod "pod-d2f6ba38-4672-4957-a539-441df46d4df0" satisfied condition "Succeeded or Failed"
Sep  6 10:55:02.005: INFO: Trying to get logs from node vm114011 pod pod-d2f6ba38-4672-4957-a539-441df46d4df0 container test-container: <nil>
STEP: delete the pod
Sep  6 10:55:02.024: INFO: Waiting for pod pod-d2f6ba38-4672-4957-a539-441df46d4df0 to disappear
Sep  6 10:55:02.027: INFO: Pod pod-d2f6ba38-4672-4957-a539-441df46d4df0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:55:02.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7925" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":92,"skipped":1461,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:55:02.036: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 10:55:02.070: INFO: Waiting up to 5m0s for pod "downwardapi-volume-190a0c4d-8ebe-45ff-8357-8028d1cb19c3" in namespace "downward-api-580" to be "Succeeded or Failed"
Sep  6 10:55:02.072: INFO: Pod "downwardapi-volume-190a0c4d-8ebe-45ff-8357-8028d1cb19c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18393ms
Sep  6 10:55:04.079: INFO: Pod "downwardapi-volume-190a0c4d-8ebe-45ff-8357-8028d1cb19c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009442503s
Sep  6 10:55:06.083: INFO: Pod "downwardapi-volume-190a0c4d-8ebe-45ff-8357-8028d1cb19c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013690076s
STEP: Saw pod success
Sep  6 10:55:06.084: INFO: Pod "downwardapi-volume-190a0c4d-8ebe-45ff-8357-8028d1cb19c3" satisfied condition "Succeeded or Failed"
Sep  6 10:55:06.086: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-190a0c4d-8ebe-45ff-8357-8028d1cb19c3 container client-container: <nil>
STEP: delete the pod
Sep  6 10:55:06.101: INFO: Waiting for pod downwardapi-volume-190a0c4d-8ebe-45ff-8357-8028d1cb19c3 to disappear
Sep  6 10:55:06.104: INFO: Pod downwardapi-volume-190a0c4d-8ebe-45ff-8357-8028d1cb19c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:55:06.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-580" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":93,"skipped":1471,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:55:06.113: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-976c25f4-d8d1-412a-8207-d468c4eda9fd
STEP: Creating a pod to test consume secrets
Sep  6 10:55:06.150: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-86e1d46e-87ad-4ffa-a773-616561c8f058" in namespace "projected-2566" to be "Succeeded or Failed"
Sep  6 10:55:06.162: INFO: Pod "pod-projected-secrets-86e1d46e-87ad-4ffa-a773-616561c8f058": Phase="Pending", Reason="", readiness=false. Elapsed: 11.683389ms
Sep  6 10:55:08.166: INFO: Pod "pod-projected-secrets-86e1d46e-87ad-4ffa-a773-616561c8f058": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015857244s
STEP: Saw pod success
Sep  6 10:55:08.166: INFO: Pod "pod-projected-secrets-86e1d46e-87ad-4ffa-a773-616561c8f058" satisfied condition "Succeeded or Failed"
Sep  6 10:55:08.168: INFO: Trying to get logs from node vm114011 pod pod-projected-secrets-86e1d46e-87ad-4ffa-a773-616561c8f058 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 10:55:08.186: INFO: Waiting for pod pod-projected-secrets-86e1d46e-87ad-4ffa-a773-616561c8f058 to disappear
Sep  6 10:55:08.189: INFO: Pod pod-projected-secrets-86e1d46e-87ad-4ffa-a773-616561c8f058 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:55:08.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2566" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":94,"skipped":1524,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:55:08.195: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-0f1660f0-0b39-4497-8259-91a0b84ca49b
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:55:12.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2347" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":95,"skipped":1525,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:55:12.266: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Sep  6 10:55:12.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-2376 create -f -'
Sep  6 10:55:12.464: INFO: stderr: ""
Sep  6 10:55:12.464: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Sep  6 10:55:12.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-2376 diff -f -'
Sep  6 10:55:12.714: INFO: rc: 1
Sep  6 10:55:12.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-2376 delete -f -'
Sep  6 10:55:12.782: INFO: stderr: ""
Sep  6 10:55:12.782: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:55:12.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2376" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":96,"skipped":1546,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:55:12.791: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:55:29.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8784" for this suite.

• [SLOW TEST:17.091 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":97,"skipped":1553,"failed":0}
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:55:29.882: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6618.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6618.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6618.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6618.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6618.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6618.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6618.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6618.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6618.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6618.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6618.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 168.8.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.8.168_udp@PTR;check="$$(dig +tcp +noall +answer +search 168.8.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.8.168_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6618.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6618.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6618.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6618.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6618.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6618.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6618.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6618.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6618.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6618.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6618.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 168.8.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.8.168_udp@PTR;check="$$(dig +tcp +noall +answer +search 168.8.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.8.168_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 10:55:31.982: INFO: Unable to read wheezy_udp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:31.986: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:31.992: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:31.996: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:32.007: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:32.012: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:32.024: INFO: Unable to read jessie_udp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:32.027: INFO: Unable to read jessie_tcp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:32.031: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:32.034: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:32.044: INFO: Unable to read jessie_udp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:32.048: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:32.055: INFO: Lookups using dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437 failed for: [wheezy_udp@dns-test-service.dns-6618.svc.cluster.local wheezy_tcp@dns-test-service.dns-6618.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-6618.svc.cluster.local jessie_tcp@dns-test-service.dns-6618.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 10:55:37.060: INFO: Unable to read wheezy_udp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:37.064: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:37.068: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:37.072: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:37.083: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:37.088: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:37.101: INFO: Unable to read jessie_udp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:37.105: INFO: Unable to read jessie_tcp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:37.111: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:37.115: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:37.126: INFO: Unable to read jessie_udp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:37.130: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:37.138: INFO: Lookups using dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437 failed for: [wheezy_udp@dns-test-service.dns-6618.svc.cluster.local wheezy_tcp@dns-test-service.dns-6618.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-6618.svc.cluster.local jessie_tcp@dns-test-service.dns-6618.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 10:55:42.059: INFO: Unable to read wheezy_udp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:42.062: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:42.065: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:42.068: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:42.077: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:42.079: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:42.086: INFO: Unable to read jessie_udp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:42.089: INFO: Unable to read jessie_tcp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:42.091: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:42.094: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:42.101: INFO: Unable to read jessie_udp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:42.103: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:42.110: INFO: Lookups using dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437 failed for: [wheezy_udp@dns-test-service.dns-6618.svc.cluster.local wheezy_tcp@dns-test-service.dns-6618.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-6618.svc.cluster.local jessie_tcp@dns-test-service.dns-6618.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 10:55:47.059: INFO: Unable to read wheezy_udp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:47.062: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:47.066: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:47.069: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:47.079: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:47.082: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:47.091: INFO: Unable to read jessie_udp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:47.095: INFO: Unable to read jessie_tcp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:47.098: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:47.102: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:47.114: INFO: Unable to read jessie_udp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:47.118: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:47.126: INFO: Lookups using dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437 failed for: [wheezy_udp@dns-test-service.dns-6618.svc.cluster.local wheezy_tcp@dns-test-service.dns-6618.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-6618.svc.cluster.local jessie_tcp@dns-test-service.dns-6618.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 10:55:52.058: INFO: Unable to read wheezy_udp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:52.061: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:52.063: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:52.067: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:52.076: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:52.079: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:52.087: INFO: Unable to read jessie_udp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:52.090: INFO: Unable to read jessie_tcp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:52.093: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:52.096: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:52.104: INFO: Unable to read jessie_udp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:52.107: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:52.117: INFO: Lookups using dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437 failed for: [wheezy_udp@dns-test-service.dns-6618.svc.cluster.local wheezy_tcp@dns-test-service.dns-6618.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-6618.svc.cluster.local jessie_tcp@dns-test-service.dns-6618.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 10:55:57.059: INFO: Unable to read wheezy_udp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:57.063: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:57.067: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:57.070: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:57.080: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:57.084: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:57.094: INFO: Unable to read jessie_udp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:57.097: INFO: Unable to read jessie_tcp@dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:57.101: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:57.105: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:57.115: INFO: Unable to read jessie_udp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:57.118: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437: the server could not find the requested resource (get pods dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437)
Sep  6 10:55:57.125: INFO: Lookups using dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437 failed for: [wheezy_udp@dns-test-service.dns-6618.svc.cluster.local wheezy_tcp@dns-test-service.dns-6618.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-6618.svc.cluster.local jessie_tcp@dns-test-service.dns-6618.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6618.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 10:56:02.108: INFO: DNS probes using dns-6618/dns-test-5a4ca1ca-aa4b-470d-bfb0-c74fc0d8e437 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:56:02.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6618" for this suite.

• [SLOW TEST:32.361 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":98,"skipped":1553,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:56:02.244: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  6 10:56:02.290: INFO: Waiting up to 5m0s for pod "pod-b774a2e4-33d0-48a6-93ad-2084afcf8692" in namespace "emptydir-3423" to be "Succeeded or Failed"
Sep  6 10:56:02.294: INFO: Pod "pod-b774a2e4-33d0-48a6-93ad-2084afcf8692": Phase="Pending", Reason="", readiness=false. Elapsed: 3.874894ms
Sep  6 10:56:04.297: INFO: Pod "pod-b774a2e4-33d0-48a6-93ad-2084afcf8692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007241759s
Sep  6 10:56:06.301: INFO: Pod "pod-b774a2e4-33d0-48a6-93ad-2084afcf8692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011365598s
STEP: Saw pod success
Sep  6 10:56:06.302: INFO: Pod "pod-b774a2e4-33d0-48a6-93ad-2084afcf8692" satisfied condition "Succeeded or Failed"
Sep  6 10:56:06.304: INFO: Trying to get logs from node vm114011 pod pod-b774a2e4-33d0-48a6-93ad-2084afcf8692 container test-container: <nil>
STEP: delete the pod
Sep  6 10:56:06.320: INFO: Waiting for pod pod-b774a2e4-33d0-48a6-93ad-2084afcf8692 to disappear
Sep  6 10:56:06.322: INFO: Pod pod-b774a2e4-33d0-48a6-93ad-2084afcf8692 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:56:06.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3423" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":99,"skipped":1553,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:56:06.331: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7427
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7427
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7427
Sep  6 10:56:06.379: INFO: Found 0 stateful pods, waiting for 1
Sep  6 10:56:16.383: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep  6 10:56:16.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7427 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 10:56:16.528: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 10:56:16.528: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 10:56:16.528: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 10:56:16.533: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  6 10:56:26.537: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 10:56:26.537: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 10:56:26.552: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999729s
Sep  6 10:56:27.556: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994318393s
Sep  6 10:56:28.559: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990274526s
Sep  6 10:56:29.563: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986423802s
Sep  6 10:56:30.567: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982529615s
Sep  6 10:56:31.571: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.978562824s
Sep  6 10:56:32.576: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.975365971s
Sep  6 10:56:33.581: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.96958957s
Sep  6 10:56:34.587: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.964246308s
Sep  6 10:56:35.590: INFO: Verifying statefulset ss doesn't scale past 1 for another 959.456433ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7427
Sep  6 10:56:36.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7427 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:56:36.744: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  6 10:56:36.744: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 10:56:36.744: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 10:56:36.749: INFO: Found 1 stateful pods, waiting for 3
Sep  6 10:56:46.753: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 10:56:46.753: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 10:56:46.753: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep  6 10:56:46.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7427 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 10:56:46.908: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 10:56:46.908: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 10:56:46.908: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 10:56:46.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7427 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 10:56:47.056: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 10:56:47.056: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 10:56:47.056: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 10:56:47.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7427 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 10:56:47.224: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 10:56:47.224: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 10:56:47.224: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 10:56:47.224: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 10:56:47.228: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep  6 10:56:57.235: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 10:56:57.235: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 10:56:57.235: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 10:56:57.249: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999629s
Sep  6 10:56:58.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994421338s
Sep  6 10:56:59.256: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98994537s
Sep  6 10:57:00.261: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986566154s
Sep  6 10:57:01.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982418677s
Sep  6 10:57:02.269: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978502642s
Sep  6 10:57:03.273: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.974508059s
Sep  6 10:57:04.278: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970083867s
Sep  6 10:57:05.283: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.965604658s
Sep  6 10:57:06.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.392068ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7427
Sep  6 10:57:07.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7427 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:57:07.459: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  6 10:57:07.459: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 10:57:07.459: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 10:57:07.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7427 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:57:07.610: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  6 10:57:07.610: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 10:57:07.610: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 10:57:07.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-7427 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 10:57:07.756: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  6 10:57:07.756: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 10:57:07.756: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 10:57:07.756: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  6 10:57:37.781: INFO: Deleting all statefulset in ns statefulset-7427
Sep  6 10:57:37.784: INFO: Scaling statefulset ss to 0
Sep  6 10:57:37.790: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 10:57:37.793: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:57:37.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7427" for this suite.

• [SLOW TEST:91.487 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":100,"skipped":1578,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:57:37.820: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-8506
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 10:57:37.847: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  6 10:57:37.890: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 10:57:39.895: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 10:57:41.894: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 10:57:43.894: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 10:57:45.895: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 10:57:47.894: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 10:57:49.896: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep  6 10:57:49.900: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  6 10:57:51.903: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  6 10:57:53.903: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep  6 10:57:53.908: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep  6 10:57:55.932: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.90 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8506 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:57:55.932: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:57:57.030: INFO: Found all expected endpoints: [netserver-0]
Sep  6 10:57:57.034: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.57 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8506 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:57:57.034: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:57:58.126: INFO: Found all expected endpoints: [netserver-1]
Sep  6 10:57:58.130: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.23 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8506 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 10:57:58.130: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 10:57:59.199: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:57:59.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8506" for this suite.

• [SLOW TEST:21.389 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":101,"skipped":1583,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:57:59.208: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:57:59.241: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep  6 10:57:59.253: INFO: Number of nodes with available pods: 0
Sep  6 10:57:59.253: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep  6 10:57:59.272: INFO: Number of nodes with available pods: 0
Sep  6 10:57:59.272: INFO: Node vm114012 is running more than one daemon pod
Sep  6 10:58:00.276: INFO: Number of nodes with available pods: 0
Sep  6 10:58:00.276: INFO: Node vm114012 is running more than one daemon pod
Sep  6 10:58:01.276: INFO: Number of nodes with available pods: 1
Sep  6 10:58:01.276: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep  6 10:58:01.292: INFO: Number of nodes with available pods: 1
Sep  6 10:58:01.292: INFO: Number of running nodes: 0, number of available pods: 1
Sep  6 10:58:02.296: INFO: Number of nodes with available pods: 0
Sep  6 10:58:02.296: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep  6 10:58:02.309: INFO: Number of nodes with available pods: 0
Sep  6 10:58:02.309: INFO: Node vm114012 is running more than one daemon pod
Sep  6 10:58:03.314: INFO: Number of nodes with available pods: 0
Sep  6 10:58:03.314: INFO: Node vm114012 is running more than one daemon pod
Sep  6 10:58:04.333: INFO: Number of nodes with available pods: 0
Sep  6 10:58:04.333: INFO: Node vm114012 is running more than one daemon pod
Sep  6 10:58:05.314: INFO: Number of nodes with available pods: 0
Sep  6 10:58:05.314: INFO: Node vm114012 is running more than one daemon pod
Sep  6 10:58:06.313: INFO: Number of nodes with available pods: 0
Sep  6 10:58:06.313: INFO: Node vm114012 is running more than one daemon pod
Sep  6 10:58:07.314: INFO: Number of nodes with available pods: 0
Sep  6 10:58:07.314: INFO: Node vm114012 is running more than one daemon pod
Sep  6 10:58:08.312: INFO: Number of nodes with available pods: 0
Sep  6 10:58:08.312: INFO: Node vm114012 is running more than one daemon pod
Sep  6 10:58:09.313: INFO: Number of nodes with available pods: 1
Sep  6 10:58:09.313: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6126, will wait for the garbage collector to delete the pods
Sep  6 10:58:09.376: INFO: Deleting DaemonSet.extensions daemon-set took: 5.993171ms
Sep  6 10:58:09.476: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.205802ms
Sep  6 10:58:17.879: INFO: Number of nodes with available pods: 0
Sep  6 10:58:17.880: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 10:58:17.883: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6126/daemonsets","resourceVersion":"15577"},"items":null}

Sep  6 10:58:17.886: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6126/pods","resourceVersion":"15577"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:58:17.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6126" for this suite.

• [SLOW TEST:18.711 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":102,"skipped":1606,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:58:17.919: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep  6 10:58:18.242: INFO: Pod name wrapped-volume-race-8e662428-0d8f-44bf-b6d4-4b169479b3a2: Found 3 pods out of 5
Sep  6 10:58:23.250: INFO: Pod name wrapped-volume-race-8e662428-0d8f-44bf-b6d4-4b169479b3a2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8e662428-0d8f-44bf-b6d4-4b169479b3a2 in namespace emptydir-wrapper-1183, will wait for the garbage collector to delete the pods
Sep  6 10:58:35.332: INFO: Deleting ReplicationController wrapped-volume-race-8e662428-0d8f-44bf-b6d4-4b169479b3a2 took: 5.535433ms
Sep  6 10:58:35.434: INFO: Terminating ReplicationController wrapped-volume-race-8e662428-0d8f-44bf-b6d4-4b169479b3a2 pods took: 102.368444ms
STEP: Creating RC which spawns configmap-volume pods
Sep  6 10:58:48.058: INFO: Pod name wrapped-volume-race-b70e9597-7c32-4c4f-b524-3fd3e0d1955d: Found 0 pods out of 5
Sep  6 10:58:53.063: INFO: Pod name wrapped-volume-race-b70e9597-7c32-4c4f-b524-3fd3e0d1955d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b70e9597-7c32-4c4f-b524-3fd3e0d1955d in namespace emptydir-wrapper-1183, will wait for the garbage collector to delete the pods
Sep  6 10:58:53.139: INFO: Deleting ReplicationController wrapped-volume-race-b70e9597-7c32-4c4f-b524-3fd3e0d1955d took: 5.260016ms
Sep  6 10:58:53.239: INFO: Terminating ReplicationController wrapped-volume-race-b70e9597-7c32-4c4f-b524-3fd3e0d1955d pods took: 100.422344ms
STEP: Creating RC which spawns configmap-volume pods
Sep  6 10:58:57.953: INFO: Pod name wrapped-volume-race-65c2c918-bf9c-4103-b0ab-8eb81bcfa4f2: Found 0 pods out of 5
Sep  6 10:59:02.959: INFO: Pod name wrapped-volume-race-65c2c918-bf9c-4103-b0ab-8eb81bcfa4f2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-65c2c918-bf9c-4103-b0ab-8eb81bcfa4f2 in namespace emptydir-wrapper-1183, will wait for the garbage collector to delete the pods
Sep  6 10:59:03.034: INFO: Deleting ReplicationController wrapped-volume-race-65c2c918-bf9c-4103-b0ab-8eb81bcfa4f2 took: 7.188808ms
Sep  6 10:59:03.135: INFO: Terminating ReplicationController wrapped-volume-race-65c2c918-bf9c-4103-b0ab-8eb81bcfa4f2 pods took: 101.123255ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:59:08.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1183" for this suite.

• [SLOW TEST:50.367 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":103,"skipped":1618,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:59:08.287: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:59:12.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3233" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":104,"skipped":1632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:59:12.339: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:59:12.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3716" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":105,"skipped":1666,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:59:12.408: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep  6 10:59:12.764: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 10:59:15.789: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 10:59:15.795: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:59:16.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6413" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":106,"skipped":1678,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:59:16.928: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  6 10:59:16.976: INFO: Waiting up to 5m0s for pod "pod-e9692be3-5f5a-4aba-a787-8b1dbbef1303" in namespace "emptydir-2198" to be "Succeeded or Failed"
Sep  6 10:59:16.980: INFO: Pod "pod-e9692be3-5f5a-4aba-a787-8b1dbbef1303": Phase="Pending", Reason="", readiness=false. Elapsed: 3.283995ms
Sep  6 10:59:18.983: INFO: Pod "pod-e9692be3-5f5a-4aba-a787-8b1dbbef1303": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006583685s
STEP: Saw pod success
Sep  6 10:59:18.983: INFO: Pod "pod-e9692be3-5f5a-4aba-a787-8b1dbbef1303" satisfied condition "Succeeded or Failed"
Sep  6 10:59:18.986: INFO: Trying to get logs from node vm114012 pod pod-e9692be3-5f5a-4aba-a787-8b1dbbef1303 container test-container: <nil>
STEP: delete the pod
Sep  6 10:59:19.007: INFO: Waiting for pod pod-e9692be3-5f5a-4aba-a787-8b1dbbef1303 to disappear
Sep  6 10:59:19.010: INFO: Pod pod-e9692be3-5f5a-4aba-a787-8b1dbbef1303 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:59:19.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2198" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":107,"skipped":1708,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:59:19.021: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:59:21.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5528" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":108,"skipped":1742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:59:21.094: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:59:37.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-654" for this suite.

• [SLOW TEST:16.126 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":109,"skipped":1785,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:59:37.221: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  6 10:59:39.267: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 10:59:39.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-37" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":110,"skipped":1789,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 10:59:39.291: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Sep  6 10:59:39.314: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  6 11:00:39.329: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:00:39.332: INFO: Starting informer...
STEP: Starting pod...
Sep  6 11:00:39.544: INFO: Pod is running on vm114011. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Sep  6 11:00:39.563: INFO: Pod wasn't evicted. Proceeding
Sep  6 11:00:39.563: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Sep  6 11:01:54.611: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:01:54.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6000" for this suite.

• [SLOW TEST:135.327 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":111,"skipped":1794,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:01:54.619: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep  6 11:01:54.643: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 11:01:54.651: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 11:01:54.654: INFO: 
Logging pods the apiserver thinks is on node vm114011 before test
Sep  6 11:01:54.658: INFO: kube-flannel-ds-7kcnr from kube-system started at 2022-09-06 11:00:41 +0000 UTC (1 container statuses recorded)
Sep  6 11:01:54.658: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 11:01:54.658: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-9wpsp from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 11:01:54.659: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 11:01:54.659: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 11:01:54.659: INFO: taint-eviction-4 from taint-single-pod-6000 started at 2022-09-06 11:00:39 +0000 UTC (1 container statuses recorded)
Sep  6 11:01:54.659: INFO: 	Container pause ready: true, restart count 0
Sep  6 11:01:54.659: INFO: 
Logging pods the apiserver thinks is on node vm114012 before test
Sep  6 11:01:54.664: INFO: kube-flannel-ds-8kz6j from kube-system started at 2022-09-06 10:12:05 +0000 UTC (1 container statuses recorded)
Sep  6 11:01:54.664: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 11:01:54.664: INFO: metrics-server-6f58bc76cc-dv8ws from kube-system started at 2022-09-06 10:13:09 +0000 UTC (1 container statuses recorded)
Sep  6 11:01:54.664: INFO: 	Container metrics-server ready: true, restart count 0
Sep  6 11:01:54.664: INFO: sonobuoy from sonobuoy started at 2022-09-06 10:14:15 +0000 UTC (1 container statuses recorded)
Sep  6 11:01:54.664: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 11:01:54.664: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-tdhsh from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 11:01:54.664: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 11:01:54.664: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 11:01:54.664: INFO: 
Logging pods the apiserver thinks is on node vm114013 before test
Sep  6 11:01:54.670: INFO: coredns-588b5cd46d-jcnzp from kube-system started at 2022-09-06 10:13:07 +0000 UTC (1 container statuses recorded)
Sep  6 11:01:54.670: INFO: 	Container coredns ready: true, restart count 0
Sep  6 11:01:54.670: INFO: kube-flannel-ds-jxdkc from kube-system started at 2022-09-06 10:12:05 +0000 UTC (1 container statuses recorded)
Sep  6 11:01:54.670: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 11:01:54.670: INFO: metrics-server-6f58bc76cc-hppkr from kube-system started at 2022-09-06 10:13:09 +0000 UTC (1 container statuses recorded)
Sep  6 11:01:54.670: INFO: 	Container metrics-server ready: true, restart count 0
Sep  6 11:01:54.670: INFO: sonobuoy-e2e-job-8d290fb976034269 from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 11:01:54.670: INFO: 	Container e2e ready: true, restart count 0
Sep  6 11:01:54.670: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 11:01:54.670: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-lb9kk from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 11:01:54.670: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 11:01:54.670: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-cd4d7ffd-83cd-4593-ba4c-5bcc6c0f54af 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-cd4d7ffd-83cd-4593-ba4c-5bcc6c0f54af off the node vm114012
STEP: verifying the node doesn't have the label kubernetes.io/e2e-cd4d7ffd-83cd-4593-ba4c-5bcc6c0f54af
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:06:58.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-114" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.129 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":112,"skipped":1820,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:06:58.748: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 11:06:58.929: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 11:07:01.958: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:07:01.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7924" for this suite.
STEP: Destroying namespace "webhook-7924-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":113,"skipped":1821,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:07:02.042: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-c9d4e92b-452f-473a-a394-d843c3efc53a
STEP: Creating a pod to test consume configMaps
Sep  6 11:07:02.091: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-501c1d1d-e287-4a5c-9cc4-c37a3ecae649" in namespace "projected-7007" to be "Succeeded or Failed"
Sep  6 11:07:02.093: INFO: Pod "pod-projected-configmaps-501c1d1d-e287-4a5c-9cc4-c37a3ecae649": Phase="Pending", Reason="", readiness=false. Elapsed: 2.874515ms
Sep  6 11:07:04.098: INFO: Pod "pod-projected-configmaps-501c1d1d-e287-4a5c-9cc4-c37a3ecae649": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007287428s
STEP: Saw pod success
Sep  6 11:07:04.098: INFO: Pod "pod-projected-configmaps-501c1d1d-e287-4a5c-9cc4-c37a3ecae649" satisfied condition "Succeeded or Failed"
Sep  6 11:07:04.102: INFO: Trying to get logs from node vm114011 pod pod-projected-configmaps-501c1d1d-e287-4a5c-9cc4-c37a3ecae649 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 11:07:04.136: INFO: Waiting for pod pod-projected-configmaps-501c1d1d-e287-4a5c-9cc4-c37a3ecae649 to disappear
Sep  6 11:07:04.142: INFO: Pod pod-projected-configmaps-501c1d1d-e287-4a5c-9cc4-c37a3ecae649 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:07:04.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7007" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":114,"skipped":1864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:07:04.153: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep  6 11:07:06.708: INFO: Successfully updated pod "adopt-release-lmf5h"
STEP: Checking that the Job readopts the Pod
Sep  6 11:07:06.708: INFO: Waiting up to 15m0s for pod "adopt-release-lmf5h" in namespace "job-4907" to be "adopted"
Sep  6 11:07:06.712: INFO: Pod "adopt-release-lmf5h": Phase="Running", Reason="", readiness=true. Elapsed: 4.473517ms
Sep  6 11:07:08.716: INFO: Pod "adopt-release-lmf5h": Phase="Running", Reason="", readiness=true. Elapsed: 2.008413333s
Sep  6 11:07:08.716: INFO: Pod "adopt-release-lmf5h" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep  6 11:07:09.229: INFO: Successfully updated pod "adopt-release-lmf5h"
STEP: Checking that the Job releases the Pod
Sep  6 11:07:09.229: INFO: Waiting up to 15m0s for pod "adopt-release-lmf5h" in namespace "job-4907" to be "released"
Sep  6 11:07:09.232: INFO: Pod "adopt-release-lmf5h": Phase="Running", Reason="", readiness=true. Elapsed: 3.85563ms
Sep  6 11:07:11.237: INFO: Pod "adopt-release-lmf5h": Phase="Running", Reason="", readiness=true. Elapsed: 2.008474548s
Sep  6 11:07:11.238: INFO: Pod "adopt-release-lmf5h" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:07:11.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4907" for this suite.

• [SLOW TEST:7.094 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":115,"skipped":1937,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:07:11.248: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Sep  6 11:07:11.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 create -f -'
Sep  6 11:07:11.524: INFO: stderr: ""
Sep  6 11:07:11.524: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 11:07:11.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 11:07:11.606: INFO: stderr: ""
Sep  6 11:07:11.606: INFO: stdout: "update-demo-nautilus-7m78w update-demo-nautilus-whztw "
Sep  6 11:07:11.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods update-demo-nautilus-7m78w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 11:07:11.665: INFO: stderr: ""
Sep  6 11:07:11.665: INFO: stdout: ""
Sep  6 11:07:11.665: INFO: update-demo-nautilus-7m78w is created but not running
Sep  6 11:07:16.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 11:07:16.731: INFO: stderr: ""
Sep  6 11:07:16.731: INFO: stdout: "update-demo-nautilus-7m78w update-demo-nautilus-whztw "
Sep  6 11:07:16.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods update-demo-nautilus-7m78w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 11:07:16.802: INFO: stderr: ""
Sep  6 11:07:16.802: INFO: stdout: ""
Sep  6 11:07:16.802: INFO: update-demo-nautilus-7m78w is created but not running
Sep  6 11:07:21.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 11:07:21.870: INFO: stderr: ""
Sep  6 11:07:21.870: INFO: stdout: "update-demo-nautilus-7m78w update-demo-nautilus-whztw "
Sep  6 11:07:21.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods update-demo-nautilus-7m78w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 11:07:21.937: INFO: stderr: ""
Sep  6 11:07:21.937: INFO: stdout: "true"
Sep  6 11:07:21.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods update-demo-nautilus-7m78w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  6 11:07:21.998: INFO: stderr: ""
Sep  6 11:07:21.998: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 11:07:21.998: INFO: validating pod update-demo-nautilus-7m78w
Sep  6 11:07:22.005: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 11:07:22.005: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 11:07:22.005: INFO: update-demo-nautilus-7m78w is verified up and running
Sep  6 11:07:22.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods update-demo-nautilus-whztw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 11:07:22.068: INFO: stderr: ""
Sep  6 11:07:22.068: INFO: stdout: "true"
Sep  6 11:07:22.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods update-demo-nautilus-whztw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  6 11:07:22.134: INFO: stderr: ""
Sep  6 11:07:22.134: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 11:07:22.134: INFO: validating pod update-demo-nautilus-whztw
Sep  6 11:07:22.145: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 11:07:22.145: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 11:07:22.145: INFO: update-demo-nautilus-whztw is verified up and running
STEP: scaling down the replication controller
Sep  6 11:07:22.147: INFO: scanned /root for discovery docs: <nil>
Sep  6 11:07:22.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Sep  6 11:07:23.220: INFO: stderr: ""
Sep  6 11:07:23.220: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 11:07:23.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 11:07:23.299: INFO: stderr: ""
Sep  6 11:07:23.299: INFO: stdout: "update-demo-nautilus-7m78w update-demo-nautilus-whztw "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  6 11:07:28.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 11:07:28.365: INFO: stderr: ""
Sep  6 11:07:28.365: INFO: stdout: "update-demo-nautilus-whztw "
Sep  6 11:07:28.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods update-demo-nautilus-whztw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 11:07:28.436: INFO: stderr: ""
Sep  6 11:07:28.437: INFO: stdout: "true"
Sep  6 11:07:28.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods update-demo-nautilus-whztw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  6 11:07:28.501: INFO: stderr: ""
Sep  6 11:07:28.501: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 11:07:28.501: INFO: validating pod update-demo-nautilus-whztw
Sep  6 11:07:28.507: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 11:07:28.507: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 11:07:28.507: INFO: update-demo-nautilus-whztw is verified up and running
STEP: scaling up the replication controller
Sep  6 11:07:28.508: INFO: scanned /root for discovery docs: <nil>
Sep  6 11:07:28.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Sep  6 11:07:29.591: INFO: stderr: ""
Sep  6 11:07:29.591: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 11:07:29.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  6 11:07:29.661: INFO: stderr: ""
Sep  6 11:07:29.661: INFO: stdout: "update-demo-nautilus-w8ghb update-demo-nautilus-whztw "
Sep  6 11:07:29.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods update-demo-nautilus-w8ghb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 11:07:29.719: INFO: stderr: ""
Sep  6 11:07:29.720: INFO: stdout: "true"
Sep  6 11:07:29.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods update-demo-nautilus-w8ghb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  6 11:07:29.784: INFO: stderr: ""
Sep  6 11:07:29.784: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 11:07:29.784: INFO: validating pod update-demo-nautilus-w8ghb
Sep  6 11:07:29.789: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 11:07:29.789: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 11:07:29.789: INFO: update-demo-nautilus-w8ghb is verified up and running
Sep  6 11:07:29.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods update-demo-nautilus-whztw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  6 11:07:29.853: INFO: stderr: ""
Sep  6 11:07:29.853: INFO: stdout: "true"
Sep  6 11:07:29.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods update-demo-nautilus-whztw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  6 11:07:29.919: INFO: stderr: ""
Sep  6 11:07:29.919: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 11:07:29.919: INFO: validating pod update-demo-nautilus-whztw
Sep  6 11:07:29.922: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 11:07:29.922: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 11:07:29.922: INFO: update-demo-nautilus-whztw is verified up and running
STEP: using delete to clean up resources
Sep  6 11:07:29.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 delete --grace-period=0 --force -f -'
Sep  6 11:07:29.983: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 11:07:29.983: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  6 11:07:29.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get rc,svc -l name=update-demo --no-headers'
Sep  6 11:07:30.061: INFO: stderr: "No resources found in kubectl-7018 namespace.\n"
Sep  6 11:07:30.061: INFO: stdout: ""
Sep  6 11:07:30.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-7018 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 11:07:30.122: INFO: stderr: ""
Sep  6 11:07:30.123: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:07:30.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7018" for this suite.

• [SLOW TEST:18.891 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":116,"skipped":1946,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:07:30.139: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 11:07:30.177: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98ac631d-1662-4de1-a659-ce05fc76678e" in namespace "projected-7763" to be "Succeeded or Failed"
Sep  6 11:07:30.181: INFO: Pod "downwardapi-volume-98ac631d-1662-4de1-a659-ce05fc76678e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.866789ms
Sep  6 11:07:32.185: INFO: Pod "downwardapi-volume-98ac631d-1662-4de1-a659-ce05fc76678e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007433057s
Sep  6 11:07:34.188: INFO: Pod "downwardapi-volume-98ac631d-1662-4de1-a659-ce05fc76678e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01045183s
STEP: Saw pod success
Sep  6 11:07:34.188: INFO: Pod "downwardapi-volume-98ac631d-1662-4de1-a659-ce05fc76678e" satisfied condition "Succeeded or Failed"
Sep  6 11:07:34.193: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-98ac631d-1662-4de1-a659-ce05fc76678e container client-container: <nil>
STEP: delete the pod
Sep  6 11:07:34.219: INFO: Waiting for pod downwardapi-volume-98ac631d-1662-4de1-a659-ce05fc76678e to disappear
Sep  6 11:07:34.222: INFO: Pod downwardapi-volume-98ac631d-1662-4de1-a659-ce05fc76678e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:07:34.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7763" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":117,"skipped":1952,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:07:34.244: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  6 11:07:38.315: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 11:07:38.319: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 11:07:40.320: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 11:07:40.325: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 11:07:42.320: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 11:07:42.325: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 11:07:44.320: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 11:07:44.324: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 11:07:46.320: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 11:07:46.326: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 11:07:48.320: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 11:07:48.324: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:07:48.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-694" for this suite.

• [SLOW TEST:14.106 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":118,"skipped":1964,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:07:48.351: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Sep  6 11:07:48.380: INFO: Waiting up to 5m0s for pod "var-expansion-3602d0d1-9f1b-42a0-bdf4-0d6b0f33a803" in namespace "var-expansion-9126" to be "Succeeded or Failed"
Sep  6 11:07:48.388: INFO: Pod "var-expansion-3602d0d1-9f1b-42a0-bdf4-0d6b0f33a803": Phase="Pending", Reason="", readiness=false. Elapsed: 8.056049ms
Sep  6 11:07:50.392: INFO: Pod "var-expansion-3602d0d1-9f1b-42a0-bdf4-0d6b0f33a803": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011186306s
STEP: Saw pod success
Sep  6 11:07:50.392: INFO: Pod "var-expansion-3602d0d1-9f1b-42a0-bdf4-0d6b0f33a803" satisfied condition "Succeeded or Failed"
Sep  6 11:07:50.395: INFO: Trying to get logs from node vm114012 pod var-expansion-3602d0d1-9f1b-42a0-bdf4-0d6b0f33a803 container dapi-container: <nil>
STEP: delete the pod
Sep  6 11:07:50.409: INFO: Waiting for pod var-expansion-3602d0d1-9f1b-42a0-bdf4-0d6b0f33a803 to disappear
Sep  6 11:07:50.412: INFO: Pod var-expansion-3602d0d1-9f1b-42a0-bdf4-0d6b0f33a803 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:07:50.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9126" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":119,"skipped":1983,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:07:50.421: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:07:50.927: INFO: Checking APIGroup: apiregistration.k8s.io
Sep  6 11:07:50.928: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep  6 11:07:50.928: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Sep  6 11:07:50.928: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep  6 11:07:50.928: INFO: Checking APIGroup: extensions
Sep  6 11:07:50.930: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Sep  6 11:07:50.930: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Sep  6 11:07:50.930: INFO: extensions/v1beta1 matches extensions/v1beta1
Sep  6 11:07:50.930: INFO: Checking APIGroup: apps
Sep  6 11:07:50.932: INFO: PreferredVersion.GroupVersion: apps/v1
Sep  6 11:07:50.932: INFO: Versions found [{apps/v1 v1}]
Sep  6 11:07:50.932: INFO: apps/v1 matches apps/v1
Sep  6 11:07:50.932: INFO: Checking APIGroup: events.k8s.io
Sep  6 11:07:50.933: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep  6 11:07:50.933: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Sep  6 11:07:50.933: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep  6 11:07:50.933: INFO: Checking APIGroup: authentication.k8s.io
Sep  6 11:07:50.934: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep  6 11:07:50.934: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Sep  6 11:07:50.934: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep  6 11:07:50.934: INFO: Checking APIGroup: authorization.k8s.io
Sep  6 11:07:50.936: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep  6 11:07:50.936: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Sep  6 11:07:50.936: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep  6 11:07:50.936: INFO: Checking APIGroup: autoscaling
Sep  6 11:07:50.937: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Sep  6 11:07:50.937: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Sep  6 11:07:50.937: INFO: autoscaling/v1 matches autoscaling/v1
Sep  6 11:07:50.937: INFO: Checking APIGroup: batch
Sep  6 11:07:50.938: INFO: PreferredVersion.GroupVersion: batch/v1
Sep  6 11:07:50.938: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1} {batch/v2alpha1 v2alpha1}]
Sep  6 11:07:50.938: INFO: batch/v1 matches batch/v1
Sep  6 11:07:50.938: INFO: Checking APIGroup: certificates.k8s.io
Sep  6 11:07:50.939: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep  6 11:07:50.939: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Sep  6 11:07:50.939: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep  6 11:07:50.939: INFO: Checking APIGroup: networking.k8s.io
Sep  6 11:07:50.940: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep  6 11:07:50.941: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Sep  6 11:07:50.941: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep  6 11:07:50.941: INFO: Checking APIGroup: policy
Sep  6 11:07:50.942: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Sep  6 11:07:50.942: INFO: Versions found [{policy/v1beta1 v1beta1}]
Sep  6 11:07:50.942: INFO: policy/v1beta1 matches policy/v1beta1
Sep  6 11:07:50.942: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep  6 11:07:50.943: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep  6 11:07:50.943: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1} {rbac.authorization.k8s.io/v1alpha1 v1alpha1}]
Sep  6 11:07:50.943: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep  6 11:07:50.943: INFO: Checking APIGroup: settings.k8s.io
Sep  6 11:07:50.945: INFO: PreferredVersion.GroupVersion: settings.k8s.io/v1alpha1
Sep  6 11:07:50.945: INFO: Versions found [{settings.k8s.io/v1alpha1 v1alpha1}]
Sep  6 11:07:50.945: INFO: settings.k8s.io/v1alpha1 matches settings.k8s.io/v1alpha1
Sep  6 11:07:50.945: INFO: Checking APIGroup: storage.k8s.io
Sep  6 11:07:50.946: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep  6 11:07:50.947: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1} {storage.k8s.io/v1alpha1 v1alpha1}]
Sep  6 11:07:50.947: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep  6 11:07:50.947: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep  6 11:07:50.948: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep  6 11:07:50.948: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Sep  6 11:07:50.948: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep  6 11:07:50.948: INFO: Checking APIGroup: apiextensions.k8s.io
Sep  6 11:07:50.949: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep  6 11:07:50.949: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Sep  6 11:07:50.949: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep  6 11:07:50.949: INFO: Checking APIGroup: scheduling.k8s.io
Sep  6 11:07:50.952: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep  6 11:07:50.952: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1} {scheduling.k8s.io/v1alpha1 v1alpha1}]
Sep  6 11:07:50.952: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep  6 11:07:50.952: INFO: Checking APIGroup: coordination.k8s.io
Sep  6 11:07:50.954: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep  6 11:07:50.954: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Sep  6 11:07:50.954: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep  6 11:07:50.954: INFO: Checking APIGroup: node.k8s.io
Sep  6 11:07:50.956: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Sep  6 11:07:50.956: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1} {node.k8s.io/v1alpha1 v1alpha1}]
Sep  6 11:07:50.956: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Sep  6 11:07:50.956: INFO: Checking APIGroup: discovery.k8s.io
Sep  6 11:07:50.957: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Sep  6 11:07:50.957: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Sep  6 11:07:50.957: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Sep  6 11:07:50.957: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep  6 11:07:50.958: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1alpha1
Sep  6 11:07:50.958: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1alpha1 v1alpha1}]
Sep  6 11:07:50.958: INFO: flowcontrol.apiserver.k8s.io/v1alpha1 matches flowcontrol.apiserver.k8s.io/v1alpha1
Sep  6 11:07:50.958: INFO: Checking APIGroup: metrics.k8s.io
Sep  6 11:07:50.959: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Sep  6 11:07:50.959: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Sep  6 11:07:50.959: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:07:50.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-908" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":120,"skipped":2050,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:07:50.972: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  6 11:07:51.009: INFO: Waiting up to 5m0s for pod "pod-78243a3c-2e2b-44db-9e27-6d53d28e6368" in namespace "emptydir-7162" to be "Succeeded or Failed"
Sep  6 11:07:51.014: INFO: Pod "pod-78243a3c-2e2b-44db-9e27-6d53d28e6368": Phase="Pending", Reason="", readiness=false. Elapsed: 4.40629ms
Sep  6 11:07:53.017: INFO: Pod "pod-78243a3c-2e2b-44db-9e27-6d53d28e6368": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007612883s
Sep  6 11:07:55.021: INFO: Pod "pod-78243a3c-2e2b-44db-9e27-6d53d28e6368": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011290971s
STEP: Saw pod success
Sep  6 11:07:55.021: INFO: Pod "pod-78243a3c-2e2b-44db-9e27-6d53d28e6368" satisfied condition "Succeeded or Failed"
Sep  6 11:07:55.024: INFO: Trying to get logs from node vm114011 pod pod-78243a3c-2e2b-44db-9e27-6d53d28e6368 container test-container: <nil>
STEP: delete the pod
Sep  6 11:07:55.038: INFO: Waiting for pod pod-78243a3c-2e2b-44db-9e27-6d53d28e6368 to disappear
Sep  6 11:07:55.042: INFO: Pod pod-78243a3c-2e2b-44db-9e27-6d53d28e6368 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:07:55.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7162" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":121,"skipped":2057,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:07:55.051: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:07:55.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3978" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":122,"skipped":2064,"failed":0}
SSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:07:55.085: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:07:58.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3116" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":123,"skipped":2067,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:07:58.152: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  6 11:08:00.708: INFO: Successfully updated pod "pod-update-activedeadlineseconds-dbfe1ff6-51c2-4ed7-8d91-3fd21026b36c"
Sep  6 11:08:00.708: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-dbfe1ff6-51c2-4ed7-8d91-3fd21026b36c" in namespace "pods-5675" to be "terminated due to deadline exceeded"
Sep  6 11:08:00.712: INFO: Pod "pod-update-activedeadlineseconds-dbfe1ff6-51c2-4ed7-8d91-3fd21026b36c": Phase="Running", Reason="", readiness=true. Elapsed: 3.589115ms
Sep  6 11:08:02.715: INFO: Pod "pod-update-activedeadlineseconds-dbfe1ff6-51c2-4ed7-8d91-3fd21026b36c": Phase="Running", Reason="", readiness=true. Elapsed: 2.006800107s
Sep  6 11:08:04.719: INFO: Pod "pod-update-activedeadlineseconds-dbfe1ff6-51c2-4ed7-8d91-3fd21026b36c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.010230378s
Sep  6 11:08:04.719: INFO: Pod "pod-update-activedeadlineseconds-dbfe1ff6-51c2-4ed7-8d91-3fd21026b36c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:08:04.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5675" for this suite.

• [SLOW TEST:6.576 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":124,"skipped":2093,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:08:04.728: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep  6 11:08:04.775: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6129 /api/v1/namespaces/watch-6129/configmaps/e2e-watch-test-label-changed da3de0d1-41c7-4f16-9ab9-42589a14f5a9 18271 0 2022-09-06 11:08:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-09-06 11:08:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 11:08:04.776: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6129 /api/v1/namespaces/watch-6129/configmaps/e2e-watch-test-label-changed da3de0d1-41c7-4f16-9ab9-42589a14f5a9 18272 0 2022-09-06 11:08:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-09-06 11:08:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 11:08:04.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6129 /api/v1/namespaces/watch-6129/configmaps/e2e-watch-test-label-changed da3de0d1-41c7-4f16-9ab9-42589a14f5a9 18273 0 2022-09-06 11:08:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-09-06 11:08:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep  6 11:08:14.805: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6129 /api/v1/namespaces/watch-6129/configmaps/e2e-watch-test-label-changed da3de0d1-41c7-4f16-9ab9-42589a14f5a9 18315 0 2022-09-06 11:08:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-09-06 11:08:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 11:08:14.805: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6129 /api/v1/namespaces/watch-6129/configmaps/e2e-watch-test-label-changed da3de0d1-41c7-4f16-9ab9-42589a14f5a9 18316 0 2022-09-06 11:08:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-09-06 11:08:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 11:08:14.805: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6129 /api/v1/namespaces/watch-6129/configmaps/e2e-watch-test-label-changed da3de0d1-41c7-4f16-9ab9-42589a14f5a9 18317 0 2022-09-06 11:08:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-09-06 11:08:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:08:14.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6129" for this suite.

• [SLOW TEST:10.086 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":125,"skipped":2104,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:08:14.814: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:08:14.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4786 version'
Sep  6 11:08:14.903: INFO: stderr: ""
Sep  6 11:08:14.903: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.16\", GitCommit:\"e37e4ab4cc8dcda84f1344dda47a97bb1927d074\", GitTreeState:\"clean\", BuildDate:\"2021-10-27T16:25:59Z\", GoVersion:\"go1.15.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.16\", GitCommit:\"e37e4ab4cc8dcda84f1344dda47a97bb1927d074\", GitTreeState:\"clean\", BuildDate:\"2021-10-27T16:20:18Z\", GoVersion:\"go1.15.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:08:14.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4786" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":126,"skipped":2114,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:08:14.912: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-f647dfec-ef30-4b56-9ab0-ad64c364c578 in namespace container-probe-4273
Sep  6 11:08:16.950: INFO: Started pod test-webserver-f647dfec-ef30-4b56-9ab0-ad64c364c578 in namespace container-probe-4273
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 11:08:16.953: INFO: Initial restart count of pod test-webserver-f647dfec-ef30-4b56-9ab0-ad64c364c578 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:12:17.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4273" for this suite.

• [SLOW TEST:242.550 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":127,"skipped":2132,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:12:17.463: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 11:12:17.543: INFO: Number of nodes with available pods: 0
Sep  6 11:12:17.543: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:18.550: INFO: Number of nodes with available pods: 0
Sep  6 11:12:18.552: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:19.550: INFO: Number of nodes with available pods: 3
Sep  6 11:12:19.550: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep  6 11:12:19.567: INFO: Number of nodes with available pods: 2
Sep  6 11:12:19.567: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:20.575: INFO: Number of nodes with available pods: 2
Sep  6 11:12:20.575: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:21.575: INFO: Number of nodes with available pods: 2
Sep  6 11:12:21.575: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:22.574: INFO: Number of nodes with available pods: 2
Sep  6 11:12:22.574: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:23.574: INFO: Number of nodes with available pods: 2
Sep  6 11:12:23.574: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:24.575: INFO: Number of nodes with available pods: 2
Sep  6 11:12:24.575: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:25.575: INFO: Number of nodes with available pods: 2
Sep  6 11:12:25.575: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:26.576: INFO: Number of nodes with available pods: 2
Sep  6 11:12:26.576: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:27.575: INFO: Number of nodes with available pods: 2
Sep  6 11:12:27.575: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:28.575: INFO: Number of nodes with available pods: 2
Sep  6 11:12:28.575: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:29.574: INFO: Number of nodes with available pods: 2
Sep  6 11:12:29.574: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:30.575: INFO: Number of nodes with available pods: 2
Sep  6 11:12:30.575: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:31.575: INFO: Number of nodes with available pods: 2
Sep  6 11:12:31.575: INFO: Node vm114011 is running more than one daemon pod
Sep  6 11:12:32.575: INFO: Number of nodes with available pods: 3
Sep  6 11:12:32.575: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2796, will wait for the garbage collector to delete the pods
Sep  6 11:12:32.639: INFO: Deleting DaemonSet.extensions daemon-set took: 6.698152ms
Sep  6 11:12:32.740: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.41018ms
Sep  6 11:12:39.943: INFO: Number of nodes with available pods: 0
Sep  6 11:12:39.943: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 11:12:39.947: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2796/daemonsets","resourceVersion":"19080"},"items":null}

Sep  6 11:12:39.950: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2796/pods","resourceVersion":"19080"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:12:39.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2796" for this suite.

• [SLOW TEST:22.512 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":128,"skipped":2148,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:12:39.976: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9794
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Sep  6 11:12:40.021: INFO: Found 0 stateful pods, waiting for 3
Sep  6 11:12:50.025: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:12:50.025: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:12:50.025: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep  6 11:12:50.055: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep  6 11:13:00.087: INFO: Updating stateful set ss2
Sep  6 11:13:00.099: INFO: Waiting for Pod statefulset-9794/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Sep  6 11:13:10.180: INFO: Found 1 stateful pods, waiting for 3
Sep  6 11:13:20.184: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:13:20.184: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:13:20.184: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  6 11:13:30.184: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:13:30.184: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:13:30.184: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  6 11:13:40.184: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:13:40.184: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:13:40.184: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep  6 11:13:40.208: INFO: Updating stateful set ss2
Sep  6 11:13:40.216: INFO: Waiting for Pod statefulset-9794/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep  6 11:13:50.240: INFO: Updating stateful set ss2
Sep  6 11:13:50.249: INFO: Waiting for StatefulSet statefulset-9794/ss2 to complete update
Sep  6 11:13:50.249: INFO: Waiting for Pod statefulset-9794/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep  6 11:14:00.255: INFO: Waiting for StatefulSet statefulset-9794/ss2 to complete update
Sep  6 11:14:10.254: INFO: Waiting for StatefulSet statefulset-9794/ss2 to complete update
Sep  6 11:14:20.255: INFO: Waiting for StatefulSet statefulset-9794/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  6 11:14:30.254: INFO: Deleting all statefulset in ns statefulset-9794
Sep  6 11:14:30.257: INFO: Scaling statefulset ss2 to 0
Sep  6 11:14:40.280: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 11:14:40.282: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:14:40.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9794" for this suite.

• [SLOW TEST:120.345 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":129,"skipped":2229,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:14:40.321: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 11:14:40.354: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2bd2e9d-62a8-4c5d-a3af-5f74fa5b6872" in namespace "projected-6910" to be "Succeeded or Failed"
Sep  6 11:14:40.356: INFO: Pod "downwardapi-volume-e2bd2e9d-62a8-4c5d-a3af-5f74fa5b6872": Phase="Pending", Reason="", readiness=false. Elapsed: 2.379112ms
Sep  6 11:14:42.366: INFO: Pod "downwardapi-volume-e2bd2e9d-62a8-4c5d-a3af-5f74fa5b6872": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011843738s
Sep  6 11:14:44.369: INFO: Pod "downwardapi-volume-e2bd2e9d-62a8-4c5d-a3af-5f74fa5b6872": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01547282s
STEP: Saw pod success
Sep  6 11:14:44.369: INFO: Pod "downwardapi-volume-e2bd2e9d-62a8-4c5d-a3af-5f74fa5b6872" satisfied condition "Succeeded or Failed"
Sep  6 11:14:44.372: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-e2bd2e9d-62a8-4c5d-a3af-5f74fa5b6872 container client-container: <nil>
STEP: delete the pod
Sep  6 11:14:44.393: INFO: Waiting for pod downwardapi-volume-e2bd2e9d-62a8-4c5d-a3af-5f74fa5b6872 to disappear
Sep  6 11:14:44.399: INFO: Pod downwardapi-volume-e2bd2e9d-62a8-4c5d-a3af-5f74fa5b6872 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:14:44.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6910" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":130,"skipped":2236,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:14:44.409: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-328f908d-bc45-4d62-87ab-70fe63e88a2c
STEP: Creating a pod to test consume configMaps
Sep  6 11:14:44.452: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bb3efa52-643b-4055-b14b-be2708e59106" in namespace "projected-5644" to be "Succeeded or Failed"
Sep  6 11:14:44.464: INFO: Pod "pod-projected-configmaps-bb3efa52-643b-4055-b14b-be2708e59106": Phase="Pending", Reason="", readiness=false. Elapsed: 11.495618ms
Sep  6 11:14:46.467: INFO: Pod "pod-projected-configmaps-bb3efa52-643b-4055-b14b-be2708e59106": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015124581s
STEP: Saw pod success
Sep  6 11:14:46.467: INFO: Pod "pod-projected-configmaps-bb3efa52-643b-4055-b14b-be2708e59106" satisfied condition "Succeeded or Failed"
Sep  6 11:14:46.470: INFO: Trying to get logs from node vm114011 pod pod-projected-configmaps-bb3efa52-643b-4055-b14b-be2708e59106 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 11:14:46.485: INFO: Waiting for pod pod-projected-configmaps-bb3efa52-643b-4055-b14b-be2708e59106 to disappear
Sep  6 11:14:46.487: INFO: Pod pod-projected-configmaps-bb3efa52-643b-4055-b14b-be2708e59106 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:14:46.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5644" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":131,"skipped":2237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:14:46.496: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Sep  6 11:14:46.524: INFO: created test-pod-1
Sep  6 11:14:46.540: INFO: created test-pod-2
Sep  6 11:14:46.552: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:14:46.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6989" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":132,"skipped":2271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:14:46.610: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-2410f617-e397-4229-893a-b9898a7e4ccf
STEP: Creating secret with name secret-projected-all-test-volume-186a085b-5eb9-44c4-a92d-3cf5236e1ad0
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep  6 11:14:46.652: INFO: Waiting up to 5m0s for pod "projected-volume-78be0488-87af-4443-8990-7183524b4e4d" in namespace "projected-7380" to be "Succeeded or Failed"
Sep  6 11:14:46.656: INFO: Pod "projected-volume-78be0488-87af-4443-8990-7183524b4e4d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.700131ms
Sep  6 11:14:48.659: INFO: Pod "projected-volume-78be0488-87af-4443-8990-7183524b4e4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00725089s
STEP: Saw pod success
Sep  6 11:14:48.659: INFO: Pod "projected-volume-78be0488-87af-4443-8990-7183524b4e4d" satisfied condition "Succeeded or Failed"
Sep  6 11:14:48.662: INFO: Trying to get logs from node vm114011 pod projected-volume-78be0488-87af-4443-8990-7183524b4e4d container projected-all-volume-test: <nil>
STEP: delete the pod
Sep  6 11:14:48.678: INFO: Waiting for pod projected-volume-78be0488-87af-4443-8990-7183524b4e4d to disappear
Sep  6 11:14:48.681: INFO: Pod projected-volume-78be0488-87af-4443-8990-7183524b4e4d no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:14:48.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7380" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":133,"skipped":2313,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:14:48.689: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:14:48.711: INFO: Creating deployment "test-recreate-deployment"
Sep  6 11:14:48.715: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  6 11:14:48.722: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep  6 11:14:50.727: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  6 11:14:50.729: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  6 11:14:50.736: INFO: Updating deployment test-recreate-deployment
Sep  6 11:14:50.736: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep  6 11:14:50.816: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5375 /apis/apps/v1/namespaces/deployment-5375/deployments/test-recreate-deployment 893a4de3-37af-484b-b31b-e8a5a7949124 19833 2 2022-09-06 11:14:48 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-09-06 11:14:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-09-06 11:14:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00416c1c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-09-06 11:14:50 +0000 UTC,LastTransitionTime:2022-09-06 11:14:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2022-09-06 11:14:50 +0000 UTC,LastTransitionTime:2022-09-06 11:14:48 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep  6 11:14:50.830: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-5375 /apis/apps/v1/namespaces/deployment-5375/replicasets/test-recreate-deployment-f79dd4667 de454b6e-56ed-4e8b-bdfa-d4e7be548025 19832 1 2022-09-06 11:14:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 893a4de3-37af-484b-b31b-e8a5a7949124 0xc00416c6e0 0xc00416c6e1}] []  [{kube-controller-manager Update apps/v1 2022-09-06 11:14:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893a4de3-37af-484b-b31b-e8a5a7949124\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00416c758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 11:14:50.830: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  6 11:14:50.830: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-5375 /apis/apps/v1/namespaces/deployment-5375/replicasets/test-recreate-deployment-c96cf48f 4281a734-0a7d-4bcd-8625-cc754de4be95 19823 2 2022-09-06 11:14:48 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 893a4de3-37af-484b-b31b-e8a5a7949124 0xc00416c5ef 0xc00416c600}] []  [{kube-controller-manager Update apps/v1 2022-09-06 11:14:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"893a4de3-37af-484b-b31b-e8a5a7949124\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00416c678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 11:14:50.836: INFO: Pod "test-recreate-deployment-f79dd4667-mqbw2" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-mqbw2 test-recreate-deployment-f79dd4667- deployment-5375 /api/v1/namespaces/deployment-5375/pods/test-recreate-deployment-f79dd4667-mqbw2 bb483475-ed98-4c6d-8a82-35b17f9ea3eb 19834 0 2022-09-06 11:14:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 de454b6e-56ed-4e8b-bdfa-d4e7be548025 0xc00416cc70 0xc00416cc71}] []  [{kube-controller-manager Update v1 2022-09-06 11:14:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de454b6e-56ed-4e8b-bdfa-d4e7be548025\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 11:14:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jbzpb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jbzpb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jbzpb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114011,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 11:14:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 11:14:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 11:14:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 11:14:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.11,PodIP:,StartTime:2022-09-06 11:14:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:14:50.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5375" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":134,"skipped":2328,"failed":0}

------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:14:50.852: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep  6 11:14:50.936: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6783 /api/v1/namespaces/watch-6783/configmaps/e2e-watch-test-resource-version 1c21e7e0-d80b-4dee-85f9-fe1f786c6869 19843 0 2022-09-06 11:14:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-09-06 11:14:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 11:14:50.936: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6783 /api/v1/namespaces/watch-6783/configmaps/e2e-watch-test-resource-version 1c21e7e0-d80b-4dee-85f9-fe1f786c6869 19844 0 2022-09-06 11:14:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-09-06 11:14:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:14:50.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6783" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":135,"skipped":2328,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:14:50.946: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep  6 11:14:53.522: INFO: Successfully updated pod "labelsupdatef1de850b-d399-4a26-9aba-d7c469b2e6f0"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:14:57.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1350" for this suite.

• [SLOW TEST:6.605 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":136,"skipped":2348,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:14:57.555: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-f4ae85af-df3a-47fc-8953-019608120455
STEP: Creating a pod to test consume secrets
Sep  6 11:14:57.589: INFO: Waiting up to 5m0s for pod "pod-secrets-9f67de87-039b-4514-b38d-e48a4dd7e387" in namespace "secrets-2136" to be "Succeeded or Failed"
Sep  6 11:14:57.592: INFO: Pod "pod-secrets-9f67de87-039b-4514-b38d-e48a4dd7e387": Phase="Pending", Reason="", readiness=false. Elapsed: 3.044101ms
Sep  6 11:14:59.595: INFO: Pod "pod-secrets-9f67de87-039b-4514-b38d-e48a4dd7e387": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006419902s
Sep  6 11:15:01.598: INFO: Pod "pod-secrets-9f67de87-039b-4514-b38d-e48a4dd7e387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009814383s
STEP: Saw pod success
Sep  6 11:15:01.598: INFO: Pod "pod-secrets-9f67de87-039b-4514-b38d-e48a4dd7e387" satisfied condition "Succeeded or Failed"
Sep  6 11:15:01.601: INFO: Trying to get logs from node vm114011 pod pod-secrets-9f67de87-039b-4514-b38d-e48a4dd7e387 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 11:15:01.612: INFO: Waiting for pod pod-secrets-9f67de87-039b-4514-b38d-e48a4dd7e387 to disappear
Sep  6 11:15:01.615: INFO: Pod pod-secrets-9f67de87-039b-4514-b38d-e48a4dd7e387 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:15:01.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2136" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":137,"skipped":2354,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:15:01.624: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 11:15:02.009: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  6 11:15:04.017: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798059702, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798059702, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798059702, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798059702, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 11:15:07.031: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:15:07.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1246" for this suite.
STEP: Destroying namespace "webhook-1246-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.517 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":138,"skipped":2364,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:15:07.143: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:15:07.206: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-0aa871db-b140-4021-9293-19bacae2f4d3" in namespace "security-context-test-2941" to be "Succeeded or Failed"
Sep  6 11:15:07.219: INFO: Pod "busybox-readonly-false-0aa871db-b140-4021-9293-19bacae2f4d3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.030793ms
Sep  6 11:15:09.225: INFO: Pod "busybox-readonly-false-0aa871db-b140-4021-9293-19bacae2f4d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018605891s
Sep  6 11:15:09.225: INFO: Pod "busybox-readonly-false-0aa871db-b140-4021-9293-19bacae2f4d3" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:15:09.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2941" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":139,"skipped":2373,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:15:09.236: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 11:15:09.269: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae2f2b81-df0f-43b1-a403-763e7e52ce70" in namespace "projected-2122" to be "Succeeded or Failed"
Sep  6 11:15:09.275: INFO: Pod "downwardapi-volume-ae2f2b81-df0f-43b1-a403-763e7e52ce70": Phase="Pending", Reason="", readiness=false. Elapsed: 5.838958ms
Sep  6 11:15:11.278: INFO: Pod "downwardapi-volume-ae2f2b81-df0f-43b1-a403-763e7e52ce70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009192755s
STEP: Saw pod success
Sep  6 11:15:11.278: INFO: Pod "downwardapi-volume-ae2f2b81-df0f-43b1-a403-763e7e52ce70" satisfied condition "Succeeded or Failed"
Sep  6 11:15:11.281: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-ae2f2b81-df0f-43b1-a403-763e7e52ce70 container client-container: <nil>
STEP: delete the pod
Sep  6 11:15:11.302: INFO: Waiting for pod downwardapi-volume-ae2f2b81-df0f-43b1-a403-763e7e52ce70 to disappear
Sep  6 11:15:11.307: INFO: Pod downwardapi-volume-ae2f2b81-df0f-43b1-a403-763e7e52ce70 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:15:11.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2122" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":140,"skipped":2379,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:15:11.323: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:15:11.345: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep  6 11:15:12.373: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:15:13.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8762" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":141,"skipped":2384,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:15:13.389: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6196 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6196;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6196 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6196;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6196.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6196.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6196.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6196.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6196.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6196.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6196.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6196.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6196.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6196.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6196.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6196.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6196.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 238.60.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.60.238_udp@PTR;check="$$(dig +tcp +noall +answer +search 238.60.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.60.238_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6196 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6196;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6196 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6196;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6196.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6196.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6196.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6196.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6196.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6196.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6196.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6196.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6196.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6196.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6196.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6196.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6196.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 238.60.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.60.238_udp@PTR;check="$$(dig +tcp +noall +answer +search 238.60.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.60.238_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 11:15:15.474: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.477: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.480: INFO: Unable to read wheezy_udp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.484: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.486: INFO: Unable to read wheezy_udp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.489: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.494: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.498: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.508: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.510: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.520: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.524: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.528: INFO: Unable to read jessie_udp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.531: INFO: Unable to read jessie_tcp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.534: INFO: Unable to read jessie_udp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.537: INFO: Unable to read jessie_tcp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.539: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.543: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.552: INFO: Unable to read jessie_udp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.555: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:15.563: INFO: Lookups using dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6196 wheezy_tcp@dns-test-service.dns-6196 wheezy_udp@dns-test-service.dns-6196.svc wheezy_tcp@dns-test-service.dns-6196.svc wheezy_udp@_http._tcp.dns-test-service.dns-6196.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6196.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6196 jessie_tcp@dns-test-service.dns-6196 jessie_udp@dns-test-service.dns-6196.svc jessie_tcp@dns-test-service.dns-6196.svc jessie_udp@_http._tcp.dns-test-service.dns-6196.svc jessie_tcp@_http._tcp.dns-test-service.dns-6196.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 11:15:20.569: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.573: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.576: INFO: Unable to read wheezy_udp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.581: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.584: INFO: Unable to read wheezy_udp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.588: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.591: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.596: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.605: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.608: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.616: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.619: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.622: INFO: Unable to read jessie_udp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.625: INFO: Unable to read jessie_tcp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.634: INFO: Unable to read jessie_udp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.638: INFO: Unable to read jessie_tcp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.641: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.645: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.654: INFO: Unable to read jessie_udp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.657: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:20.663: INFO: Lookups using dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6196 wheezy_tcp@dns-test-service.dns-6196 wheezy_udp@dns-test-service.dns-6196.svc wheezy_tcp@dns-test-service.dns-6196.svc wheezy_udp@_http._tcp.dns-test-service.dns-6196.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6196.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6196 jessie_tcp@dns-test-service.dns-6196 jessie_udp@dns-test-service.dns-6196.svc jessie_tcp@dns-test-service.dns-6196.svc jessie_udp@_http._tcp.dns-test-service.dns-6196.svc jessie_tcp@_http._tcp.dns-test-service.dns-6196.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 11:15:25.569: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.572: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.576: INFO: Unable to read wheezy_udp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.579: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.581: INFO: Unable to read wheezy_udp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.584: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.587: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.590: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.598: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.601: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.609: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.612: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.615: INFO: Unable to read jessie_udp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.617: INFO: Unable to read jessie_tcp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.620: INFO: Unable to read jessie_udp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.623: INFO: Unable to read jessie_tcp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.626: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.630: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.640: INFO: Unable to read jessie_udp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.645: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:25.651: INFO: Lookups using dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6196 wheezy_tcp@dns-test-service.dns-6196 wheezy_udp@dns-test-service.dns-6196.svc wheezy_tcp@dns-test-service.dns-6196.svc wheezy_udp@_http._tcp.dns-test-service.dns-6196.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6196.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6196 jessie_tcp@dns-test-service.dns-6196 jessie_udp@dns-test-service.dns-6196.svc jessie_tcp@dns-test-service.dns-6196.svc jessie_udp@_http._tcp.dns-test-service.dns-6196.svc jessie_tcp@_http._tcp.dns-test-service.dns-6196.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 11:15:30.568: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.572: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.575: INFO: Unable to read wheezy_udp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.579: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.582: INFO: Unable to read wheezy_udp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.585: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.590: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.595: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.604: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.608: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.618: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.623: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.628: INFO: Unable to read jessie_udp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.635: INFO: Unable to read jessie_tcp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.640: INFO: Unable to read jessie_udp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.646: INFO: Unable to read jessie_tcp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.651: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.656: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.668: INFO: Unable to read jessie_udp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.673: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:30.685: INFO: Lookups using dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6196 wheezy_tcp@dns-test-service.dns-6196 wheezy_udp@dns-test-service.dns-6196.svc wheezy_tcp@dns-test-service.dns-6196.svc wheezy_udp@_http._tcp.dns-test-service.dns-6196.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6196.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6196 jessie_tcp@dns-test-service.dns-6196 jessie_udp@dns-test-service.dns-6196.svc jessie_tcp@dns-test-service.dns-6196.svc jessie_udp@_http._tcp.dns-test-service.dns-6196.svc jessie_tcp@_http._tcp.dns-test-service.dns-6196.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 11:15:35.568: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.572: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.576: INFO: Unable to read wheezy_udp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.580: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.583: INFO: Unable to read wheezy_udp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.586: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.588: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.591: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.598: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.601: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.609: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.612: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.614: INFO: Unable to read jessie_udp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.617: INFO: Unable to read jessie_tcp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.620: INFO: Unable to read jessie_udp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.623: INFO: Unable to read jessie_tcp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.626: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.630: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.638: INFO: Unable to read jessie_udp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.642: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:35.648: INFO: Lookups using dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6196 wheezy_tcp@dns-test-service.dns-6196 wheezy_udp@dns-test-service.dns-6196.svc wheezy_tcp@dns-test-service.dns-6196.svc wheezy_udp@_http._tcp.dns-test-service.dns-6196.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6196.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6196 jessie_tcp@dns-test-service.dns-6196 jessie_udp@dns-test-service.dns-6196.svc jessie_tcp@dns-test-service.dns-6196.svc jessie_udp@_http._tcp.dns-test-service.dns-6196.svc jessie_tcp@_http._tcp.dns-test-service.dns-6196.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 11:15:40.568: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.578: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.581: INFO: Unable to read wheezy_udp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.585: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.588: INFO: Unable to read wheezy_udp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.591: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.594: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.598: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.606: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.609: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.617: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.620: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.624: INFO: Unable to read jessie_udp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.628: INFO: Unable to read jessie_tcp@dns-test-service.dns-6196 from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.634: INFO: Unable to read jessie_udp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.638: INFO: Unable to read jessie_tcp@dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.641: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.645: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6196.svc from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.653: INFO: Unable to read jessie_udp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.657: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682: the server could not find the requested resource (get pods dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682)
Sep  6 11:15:40.663: INFO: Lookups using dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6196 wheezy_tcp@dns-test-service.dns-6196 wheezy_udp@dns-test-service.dns-6196.svc wheezy_tcp@dns-test-service.dns-6196.svc wheezy_udp@_http._tcp.dns-test-service.dns-6196.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6196.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6196 jessie_tcp@dns-test-service.dns-6196 jessie_udp@dns-test-service.dns-6196.svc jessie_tcp@dns-test-service.dns-6196.svc jessie_udp@_http._tcp.dns-test-service.dns-6196.svc jessie_tcp@_http._tcp.dns-test-service.dns-6196.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 11:15:45.635: INFO: DNS probes using dns-6196/dns-test-7b28f74f-17b2-4dde-bc6f-e954f548a682 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:15:45.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6196" for this suite.

• [SLOW TEST:32.386 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":142,"skipped":2417,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:15:45.777: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 11:15:45.850: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e9018fff-4cf5-49f0-b99d-716d394236ea" in namespace "downward-api-8658" to be "Succeeded or Failed"
Sep  6 11:15:45.862: INFO: Pod "downwardapi-volume-e9018fff-4cf5-49f0-b99d-716d394236ea": Phase="Pending", Reason="", readiness=false. Elapsed: 12.595316ms
Sep  6 11:15:47.866: INFO: Pod "downwardapi-volume-e9018fff-4cf5-49f0-b99d-716d394236ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016200067s
STEP: Saw pod success
Sep  6 11:15:47.866: INFO: Pod "downwardapi-volume-e9018fff-4cf5-49f0-b99d-716d394236ea" satisfied condition "Succeeded or Failed"
Sep  6 11:15:47.869: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-e9018fff-4cf5-49f0-b99d-716d394236ea container client-container: <nil>
STEP: delete the pod
Sep  6 11:15:47.885: INFO: Waiting for pod downwardapi-volume-e9018fff-4cf5-49f0-b99d-716d394236ea to disappear
Sep  6 11:15:47.887: INFO: Pod downwardapi-volume-e9018fff-4cf5-49f0-b99d-716d394236ea no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:15:47.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8658" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":143,"skipped":2484,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:15:47.905: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:15:47.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4096" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":144,"skipped":2489,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:15:48.001: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:15:48.023: INFO: Creating ReplicaSet my-hostname-basic-5b90ec87-e66c-4eb7-9de2-49189bf26f88
Sep  6 11:15:48.031: INFO: Pod name my-hostname-basic-5b90ec87-e66c-4eb7-9de2-49189bf26f88: Found 0 pods out of 1
Sep  6 11:15:53.035: INFO: Pod name my-hostname-basic-5b90ec87-e66c-4eb7-9de2-49189bf26f88: Found 1 pods out of 1
Sep  6 11:15:53.035: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5b90ec87-e66c-4eb7-9de2-49189bf26f88" is running
Sep  6 11:15:53.039: INFO: Pod "my-hostname-basic-5b90ec87-e66c-4eb7-9de2-49189bf26f88-fccx4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-06 11:15:48 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-06 11:15:49 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-06 11:15:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-06 11:15:48 +0000 UTC Reason: Message:}])
Sep  6 11:15:53.039: INFO: Trying to dial the pod
Sep  6 11:15:58.049: INFO: Controller my-hostname-basic-5b90ec87-e66c-4eb7-9de2-49189bf26f88: Got expected result from replica 1 [my-hostname-basic-5b90ec87-e66c-4eb7-9de2-49189bf26f88-fccx4]: "my-hostname-basic-5b90ec87-e66c-4eb7-9de2-49189bf26f88-fccx4", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:15:58.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3266" for this suite.

• [SLOW TEST:10.057 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":145,"skipped":2491,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:15:58.058: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 11:15:58.256: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 11:16:01.274: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:16:01.278: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-283-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:16:02.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2622" for this suite.
STEP: Destroying namespace "webhook-2622-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":146,"skipped":2497,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:16:02.437: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8982
Sep  6 11:16:04.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8982 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep  6 11:16:04.645: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Sep  6 11:16:04.645: INFO: stdout: "ipvs"
Sep  6 11:16:04.645: INFO: proxyMode: ipvs
Sep  6 11:16:04.653: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep  6 11:16:04.657: INFO: Pod kube-proxy-mode-detector still exists
Sep  6 11:16:06.657: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep  6 11:16:06.660: INFO: Pod kube-proxy-mode-detector still exists
Sep  6 11:16:08.657: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep  6 11:16:08.660: INFO: Pod kube-proxy-mode-detector still exists
Sep  6 11:16:10.657: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep  6 11:16:10.660: INFO: Pod kube-proxy-mode-detector still exists
Sep  6 11:16:12.657: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep  6 11:16:12.661: INFO: Pod kube-proxy-mode-detector still exists
Sep  6 11:16:14.657: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep  6 11:16:14.661: INFO: Pod kube-proxy-mode-detector still exists
Sep  6 11:16:16.657: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep  6 11:16:16.661: INFO: Pod kube-proxy-mode-detector still exists
Sep  6 11:16:18.657: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep  6 11:16:18.660: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-8982
STEP: creating replication controller affinity-nodeport-timeout in namespace services-8982
I0906 11:16:18.691782      20 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-8982, replica count: 3
I0906 11:16:21.742536      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 11:16:21.751: INFO: Creating new exec pod
Sep  6 11:16:24.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8982 exec execpod-affinitypws7v -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Sep  6 11:16:24.941: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Sep  6 11:16:24.942: INFO: stdout: ""
Sep  6 11:16:24.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8982 exec execpod-affinitypws7v -- /bin/sh -x -c nc -zv -t -w 2 10.98.20.20 80'
Sep  6 11:16:25.072: INFO: stderr: "+ nc -zv -t -w 2 10.98.20.20 80\nConnection to 10.98.20.20 80 port [tcp/http] succeeded!\n"
Sep  6 11:16:25.072: INFO: stdout: ""
Sep  6 11:16:25.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8982 exec execpod-affinitypws7v -- /bin/sh -x -c nc -zv -t -w 2 172.16.114.11 31926'
Sep  6 11:16:25.226: INFO: stderr: "+ nc -zv -t -w 2 172.16.114.11 31926\nConnection to 172.16.114.11 31926 port [tcp/31926] succeeded!\n"
Sep  6 11:16:25.227: INFO: stdout: ""
Sep  6 11:16:25.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8982 exec execpod-affinitypws7v -- /bin/sh -x -c nc -zv -t -w 2 172.16.114.13 31926'
Sep  6 11:16:25.382: INFO: stderr: "+ nc -zv -t -w 2 172.16.114.13 31926\nConnection to 172.16.114.13 31926 port [tcp/31926] succeeded!\n"
Sep  6 11:16:25.382: INFO: stdout: ""
Sep  6 11:16:25.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8982 exec execpod-affinitypws7v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.114.11:31926/ ; done'
Sep  6 11:16:25.568: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n"
Sep  6 11:16:25.568: INFO: stdout: "\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9\naffinity-nodeport-timeout-wxzb9"
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Received response from host: affinity-nodeport-timeout-wxzb9
Sep  6 11:16:25.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8982 exec execpod-affinitypws7v -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.114.11:31926/'
Sep  6 11:16:25.721: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n"
Sep  6 11:16:25.721: INFO: stdout: "affinity-nodeport-timeout-wxzb9"
Sep  6 11:18:30.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8982 exec execpod-affinitypws7v -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.114.11:31926/'
Sep  6 11:18:30.943: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n"
Sep  6 11:18:30.943: INFO: stdout: "affinity-nodeport-timeout-wxzb9"
Sep  6 11:20:35.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8982 exec execpod-affinitypws7v -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.114.11:31926/'
Sep  6 11:20:36.104: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n"
Sep  6 11:20:36.104: INFO: stdout: "affinity-nodeport-timeout-wxzb9"
Sep  6 11:22:41.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8982 exec execpod-affinitypws7v -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.114.11:31926/'
Sep  6 11:22:41.268: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n"
Sep  6 11:22:41.269: INFO: stdout: "affinity-nodeport-timeout-wxzb9"
Sep  6 11:24:46.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8982 exec execpod-affinitypws7v -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.114.11:31926/'
Sep  6 11:24:46.450: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n"
Sep  6 11:24:46.450: INFO: stdout: "affinity-nodeport-timeout-wxzb9"
Sep  6 11:26:51.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8982 exec execpod-affinitypws7v -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.114.11:31926/'
Sep  6 11:26:51.622: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n"
Sep  6 11:26:51.622: INFO: stdout: "affinity-nodeport-timeout-wxzb9"
Sep  6 11:28:56.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8982 exec execpod-affinitypws7v -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.114.11:31926/'
Sep  6 11:28:56.863: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n"
Sep  6 11:28:56.863: INFO: stdout: "affinity-nodeport-timeout-wxzb9"
Sep  6 11:31:01.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8982 exec execpod-affinitypws7v -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.114.11:31926/'
Sep  6 11:31:02.129: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.114.11:31926/\n"
Sep  6 11:31:02.129: INFO: stdout: "affinity-nodeport-timeout-zv8nb"
Sep  6 11:31:02.129: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-8982, will wait for the garbage collector to delete the pods
Sep  6 11:31:02.238: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.859901ms
Sep  6 11:31:02.339: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.414397ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:31:09.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8982" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:907.576 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":147,"skipped":2513,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:31:10.014: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:31:10.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-170" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":148,"skipped":2520,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:31:10.137: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-6682
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6682 to expose endpoints map[]
Sep  6 11:31:10.188: INFO: successfully validated that service endpoint-test2 in namespace services-6682 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6682
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6682 to expose endpoints map[pod1:[80]]
Sep  6 11:31:12.208: INFO: successfully validated that service endpoint-test2 in namespace services-6682 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-6682
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6682 to expose endpoints map[pod1:[80] pod2:[80]]
Sep  6 11:31:14.236: INFO: successfully validated that service endpoint-test2 in namespace services-6682 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-6682
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6682 to expose endpoints map[pod2:[80]]
Sep  6 11:31:14.284: INFO: successfully validated that service endpoint-test2 in namespace services-6682 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-6682
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6682 to expose endpoints map[]
Sep  6 11:31:14.327: INFO: successfully validated that service endpoint-test2 in namespace services-6682 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:31:14.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6682" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":149,"skipped":2523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:31:14.384: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 11:31:14.454: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88230455-8c10-46f6-ace0-c04cbdda6002" in namespace "projected-2173" to be "Succeeded or Failed"
Sep  6 11:31:14.459: INFO: Pod "downwardapi-volume-88230455-8c10-46f6-ace0-c04cbdda6002": Phase="Pending", Reason="", readiness=false. Elapsed: 4.879472ms
Sep  6 11:31:16.462: INFO: Pod "downwardapi-volume-88230455-8c10-46f6-ace0-c04cbdda6002": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008090433s
STEP: Saw pod success
Sep  6 11:31:16.462: INFO: Pod "downwardapi-volume-88230455-8c10-46f6-ace0-c04cbdda6002" satisfied condition "Succeeded or Failed"
Sep  6 11:31:16.465: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-88230455-8c10-46f6-ace0-c04cbdda6002 container client-container: <nil>
STEP: delete the pod
Sep  6 11:31:16.492: INFO: Waiting for pod downwardapi-volume-88230455-8c10-46f6-ace0-c04cbdda6002 to disappear
Sep  6 11:31:16.495: INFO: Pod downwardapi-volume-88230455-8c10-46f6-ace0-c04cbdda6002 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:31:16.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2173" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":150,"skipped":2553,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:31:16.506: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-9a852d27-b5e2-4347-8e7b-045438f175ac
STEP: Creating a pod to test consume secrets
Sep  6 11:31:16.550: INFO: Waiting up to 5m0s for pod "pod-secrets-3e53f0c8-e6e9-4b81-b1ec-7acbe0a9a4ec" in namespace "secrets-8673" to be "Succeeded or Failed"
Sep  6 11:31:16.555: INFO: Pod "pod-secrets-3e53f0c8-e6e9-4b81-b1ec-7acbe0a9a4ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.199092ms
Sep  6 11:31:18.560: INFO: Pod "pod-secrets-3e53f0c8-e6e9-4b81-b1ec-7acbe0a9a4ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009949985s
STEP: Saw pod success
Sep  6 11:31:18.561: INFO: Pod "pod-secrets-3e53f0c8-e6e9-4b81-b1ec-7acbe0a9a4ec" satisfied condition "Succeeded or Failed"
Sep  6 11:31:18.566: INFO: Trying to get logs from node vm114011 pod pod-secrets-3e53f0c8-e6e9-4b81-b1ec-7acbe0a9a4ec container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 11:31:18.582: INFO: Waiting for pod pod-secrets-3e53f0c8-e6e9-4b81-b1ec-7acbe0a9a4ec to disappear
Sep  6 11:31:18.585: INFO: Pod pod-secrets-3e53f0c8-e6e9-4b81-b1ec-7acbe0a9a4ec no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:31:18.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8673" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":151,"skipped":2559,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:31:18.595: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-657
STEP: creating replication controller nodeport-test in namespace services-657
I0906 11:31:18.642226      20 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-657, replica count: 2
I0906 11:31:21.693417      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 11:31:21.693: INFO: Creating new exec pod
Sep  6 11:31:24.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-657 exec execpoddclgd -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Sep  6 11:31:24.957: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep  6 11:31:24.957: INFO: stdout: ""
Sep  6 11:31:24.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-657 exec execpoddclgd -- /bin/sh -x -c nc -zv -t -w 2 10.103.52.3 80'
Sep  6 11:31:25.221: INFO: stderr: "+ nc -zv -t -w 2 10.103.52.3 80\nConnection to 10.103.52.3 80 port [tcp/http] succeeded!\n"
Sep  6 11:31:25.221: INFO: stdout: ""
Sep  6 11:31:25.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-657 exec execpoddclgd -- /bin/sh -x -c nc -zv -t -w 2 172.16.114.12 32046'
Sep  6 11:31:25.435: INFO: stderr: "+ nc -zv -t -w 2 172.16.114.12 32046\nConnection to 172.16.114.12 32046 port [tcp/32046] succeeded!\n"
Sep  6 11:31:25.436: INFO: stdout: ""
Sep  6 11:31:25.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-657 exec execpoddclgd -- /bin/sh -x -c nc -zv -t -w 2 172.16.114.11 32046'
Sep  6 11:31:25.672: INFO: stderr: "+ nc -zv -t -w 2 172.16.114.11 32046\nConnection to 172.16.114.11 32046 port [tcp/32046] succeeded!\n"
Sep  6 11:31:25.672: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:31:25.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-657" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.097 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":152,"skipped":2572,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:31:25.693: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep  6 11:31:35.765: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0906 11:31:35.765317      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0906 11:31:35.765352      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0906 11:31:35.765356      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:31:35.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6527" for this suite.

• [SLOW TEST:10.082 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":153,"skipped":2623,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:31:35.775: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6711, will wait for the garbage collector to delete the pods
Sep  6 11:31:37.889: INFO: Deleting Job.batch foo took: 6.630183ms
Sep  6 11:31:37.990: INFO: Terminating Job.batch foo pods took: 100.724617ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:32:19.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6711" for this suite.

• [SLOW TEST:44.226 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":154,"skipped":2626,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:32:20.002: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2097.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2097.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 11:32:24.076: INFO: DNS probes using dns-test-4d0611fb-a8bb-4177-9cd0-0912214e3918 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2097.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2097.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 11:32:26.159: INFO: File wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local from pod  dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 11:32:26.165: INFO: File jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local from pod  dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 11:32:26.165: INFO: Lookups using dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 failed for: [wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local]

Sep  6 11:32:31.172: INFO: File wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local from pod  dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 11:32:31.177: INFO: File jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local from pod  dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 11:32:31.177: INFO: Lookups using dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 failed for: [wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local]

Sep  6 11:32:36.171: INFO: File wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local from pod  dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 11:32:36.175: INFO: File jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local from pod  dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 11:32:36.175: INFO: Lookups using dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 failed for: [wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local]

Sep  6 11:32:41.170: INFO: File wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local from pod  dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 11:32:41.176: INFO: File jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local from pod  dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 11:32:41.176: INFO: Lookups using dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 failed for: [wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local]

Sep  6 11:32:46.170: INFO: File wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local from pod  dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 11:32:46.174: INFO: File jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local from pod  dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 11:32:46.174: INFO: Lookups using dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 failed for: [wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local]

Sep  6 11:32:51.169: INFO: File wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local from pod  dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 11:32:51.172: INFO: File jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local from pod  dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 11:32:51.172: INFO: Lookups using dns-2097/dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 failed for: [wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local]

Sep  6 11:32:56.176: INFO: DNS probes using dns-test-b9212b2d-53e3-419c-a4d3-37fced556920 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2097.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2097.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2097.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2097.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 11:32:58.262: INFO: DNS probes using dns-test-c3ca4e1a-dbb5-47e4-9795-567066236164 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:32:58.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2097" for this suite.

• [SLOW TEST:38.307 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":155,"skipped":2635,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:32:58.310: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1385
STEP: creating an pod
Sep  6 11:32:58.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4294 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep  6 11:32:58.454: INFO: stderr: ""
Sep  6 11:32:58.454: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Sep  6 11:32:58.454: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep  6 11:32:58.454: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4294" to be "running and ready, or succeeded"
Sep  6 11:32:58.466: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.683979ms
Sep  6 11:33:00.469: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.0150943s
Sep  6 11:33:00.469: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep  6 11:33:00.469: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep  6 11:33:00.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4294 logs logs-generator logs-generator'
Sep  6 11:33:00.546: INFO: stderr: ""
Sep  6 11:33:00.547: INFO: stdout: "I0906 11:32:59.212601       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/zwj 545\nI0906 11:32:59.413217       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/lcq5 595\nI0906 11:32:59.613009       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/hrg 408\nI0906 11:32:59.812916       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/9vrm 434\nI0906 11:33:00.012814       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/5fw 496\nI0906 11:33:00.213108       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/46vk 527\nI0906 11:33:00.412856       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/4hfz 599\n"
Sep  6 11:33:02.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4294 logs logs-generator logs-generator'
Sep  6 11:33:02.647: INFO: stderr: ""
Sep  6 11:33:02.647: INFO: stdout: "I0906 11:32:59.212601       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/zwj 545\nI0906 11:32:59.413217       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/lcq5 595\nI0906 11:32:59.613009       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/hrg 408\nI0906 11:32:59.812916       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/9vrm 434\nI0906 11:33:00.012814       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/5fw 496\nI0906 11:33:00.213108       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/46vk 527\nI0906 11:33:00.412856       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/4hfz 599\nI0906 11:33:00.612639       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/d65 369\nI0906 11:33:00.813142       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/cf5 314\nI0906 11:33:01.013214       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/q8x 244\nI0906 11:33:01.213141       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/zmq9 503\nI0906 11:33:01.412793       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/hmr 413\nI0906 11:33:01.612917       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/dv5c 283\nI0906 11:33:01.812749       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/687 540\nI0906 11:33:02.013148       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/g5x5 213\nI0906 11:33:02.212915       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/mkf 275\nI0906 11:33:02.412798       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/ds8 326\nI0906 11:33:02.613044       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/nbtr 449\n"
STEP: limiting log lines
Sep  6 11:33:02.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4294 logs logs-generator logs-generator --tail=1'
Sep  6 11:33:02.728: INFO: stderr: ""
Sep  6 11:33:02.728: INFO: stdout: "I0906 11:33:02.613044       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/nbtr 449\n"
Sep  6 11:33:02.728: INFO: got output "I0906 11:33:02.613044       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/nbtr 449\n"
STEP: limiting log bytes
Sep  6 11:33:02.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4294 logs logs-generator logs-generator --limit-bytes=1'
Sep  6 11:33:02.802: INFO: stderr: ""
Sep  6 11:33:02.802: INFO: stdout: "I"
Sep  6 11:33:02.802: INFO: got output "I"
STEP: exposing timestamps
Sep  6 11:33:02.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4294 logs logs-generator logs-generator --tail=1 --timestamps'
Sep  6 11:33:02.872: INFO: stderr: ""
Sep  6 11:33:02.872: INFO: stdout: "2022-09-06T11:33:02.812902748Z I0906 11:33:02.812681       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/vn6m 575\n"
Sep  6 11:33:02.872: INFO: got output "2022-09-06T11:33:02.812902748Z I0906 11:33:02.812681       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/vn6m 575\n"
STEP: restricting to a time range
Sep  6 11:33:05.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4294 logs logs-generator logs-generator --since=1s'
Sep  6 11:33:05.441: INFO: stderr: ""
Sep  6 11:33:05.441: INFO: stdout: "I0906 11:33:04.612877       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/m64j 251\nI0906 11:33:04.813104       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/jt2x 570\nI0906 11:33:05.013118       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/f79 401\nI0906 11:33:05.212830       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/5pz 326\nI0906 11:33:05.412803       1 logs_generator.go:76] 31 GET /api/v1/namespaces/ns/pods/kftp 446\n"
Sep  6 11:33:05.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4294 logs logs-generator logs-generator --since=24h'
Sep  6 11:33:05.509: INFO: stderr: ""
Sep  6 11:33:05.509: INFO: stdout: "I0906 11:32:59.212601       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/zwj 545\nI0906 11:32:59.413217       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/lcq5 595\nI0906 11:32:59.613009       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/hrg 408\nI0906 11:32:59.812916       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/9vrm 434\nI0906 11:33:00.012814       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/5fw 496\nI0906 11:33:00.213108       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/46vk 527\nI0906 11:33:00.412856       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/4hfz 599\nI0906 11:33:00.612639       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/d65 369\nI0906 11:33:00.813142       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/cf5 314\nI0906 11:33:01.013214       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/q8x 244\nI0906 11:33:01.213141       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/zmq9 503\nI0906 11:33:01.412793       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/hmr 413\nI0906 11:33:01.612917       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/dv5c 283\nI0906 11:33:01.812749       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/687 540\nI0906 11:33:02.013148       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/g5x5 213\nI0906 11:33:02.212915       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/mkf 275\nI0906 11:33:02.412798       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/ds8 326\nI0906 11:33:02.613044       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/nbtr 449\nI0906 11:33:02.812681       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/vn6m 575\nI0906 11:33:03.013209       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/2m4f 595\nI0906 11:33:03.212792       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/lx89 474\nI0906 11:33:03.412820       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/cs9t 272\nI0906 11:33:03.612891       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/p4rd 439\nI0906 11:33:03.812849       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/zztg 485\nI0906 11:33:04.012978       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/jrct 435\nI0906 11:33:04.212659       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/b9p 246\nI0906 11:33:04.412889       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/84pc 240\nI0906 11:33:04.612877       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/m64j 251\nI0906 11:33:04.813104       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/jt2x 570\nI0906 11:33:05.013118       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/f79 401\nI0906 11:33:05.212830       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/5pz 326\nI0906 11:33:05.412803       1 logs_generator.go:76] 31 GET /api/v1/namespaces/ns/pods/kftp 446\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
Sep  6 11:33:05.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4294 delete pod logs-generator'
Sep  6 11:33:09.907: INFO: stderr: ""
Sep  6 11:33:09.907: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:33:09.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4294" for this suite.

• [SLOW TEST:11.609 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":156,"skipped":2637,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:33:09.919: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  6 11:33:09.955: INFO: Waiting up to 5m0s for pod "pod-d2721774-093b-4d4f-b55a-ad41d974fb73" in namespace "emptydir-2241" to be "Succeeded or Failed"
Sep  6 11:33:09.967: INFO: Pod "pod-d2721774-093b-4d4f-b55a-ad41d974fb73": Phase="Pending", Reason="", readiness=false. Elapsed: 11.609616ms
Sep  6 11:33:11.970: INFO: Pod "pod-d2721774-093b-4d4f-b55a-ad41d974fb73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014891563s
Sep  6 11:33:13.974: INFO: Pod "pod-d2721774-093b-4d4f-b55a-ad41d974fb73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018572505s
STEP: Saw pod success
Sep  6 11:33:13.974: INFO: Pod "pod-d2721774-093b-4d4f-b55a-ad41d974fb73" satisfied condition "Succeeded or Failed"
Sep  6 11:33:13.977: INFO: Trying to get logs from node vm114011 pod pod-d2721774-093b-4d4f-b55a-ad41d974fb73 container test-container: <nil>
STEP: delete the pod
Sep  6 11:33:13.999: INFO: Waiting for pod pod-d2721774-093b-4d4f-b55a-ad41d974fb73 to disappear
Sep  6 11:33:14.003: INFO: Pod pod-d2721774-093b-4d4f-b55a-ad41d974fb73 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:33:14.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2241" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":157,"skipped":2638,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:33:14.013: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 11:33:14.251: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  6 11:33:16.262: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798060794, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798060794, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798060794, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798060794, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:33:18.266: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798060794, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798060794, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798060794, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798060794, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 11:33:21.281: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:33:21.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6693" for this suite.
STEP: Destroying namespace "webhook-6693-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.407 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":158,"skipped":2647,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:33:21.420: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Sep  6 11:33:22.000: INFO: created pod pod-service-account-defaultsa
Sep  6 11:33:22.000: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  6 11:33:22.005: INFO: created pod pod-service-account-mountsa
Sep  6 11:33:22.006: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  6 11:33:22.017: INFO: created pod pod-service-account-nomountsa
Sep  6 11:33:22.017: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  6 11:33:22.026: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  6 11:33:22.026: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  6 11:33:22.040: INFO: created pod pod-service-account-mountsa-mountspec
Sep  6 11:33:22.040: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  6 11:33:22.056: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  6 11:33:22.056: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  6 11:33:22.070: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  6 11:33:22.070: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  6 11:33:22.082: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  6 11:33:22.082: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  6 11:33:22.095: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  6 11:33:22.095: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:33:22.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8406" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":159,"skipped":2674,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:33:22.129: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-3164a27d-debb-4e1a-bd9c-2bc550e1879d
STEP: Creating a pod to test consume secrets
Sep  6 11:33:22.252: INFO: Waiting up to 5m0s for pod "pod-secrets-9abca6d4-fd2d-411d-a30f-69c7c45b14e0" in namespace "secrets-3532" to be "Succeeded or Failed"
Sep  6 11:33:22.276: INFO: Pod "pod-secrets-9abca6d4-fd2d-411d-a30f-69c7c45b14e0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.796709ms
Sep  6 11:33:24.280: INFO: Pod "pod-secrets-9abca6d4-fd2d-411d-a30f-69c7c45b14e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02735154s
Sep  6 11:33:26.284: INFO: Pod "pod-secrets-9abca6d4-fd2d-411d-a30f-69c7c45b14e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031326431s
STEP: Saw pod success
Sep  6 11:33:26.284: INFO: Pod "pod-secrets-9abca6d4-fd2d-411d-a30f-69c7c45b14e0" satisfied condition "Succeeded or Failed"
Sep  6 11:33:26.286: INFO: Trying to get logs from node vm114012 pod pod-secrets-9abca6d4-fd2d-411d-a30f-69c7c45b14e0 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 11:33:26.305: INFO: Waiting for pod pod-secrets-9abca6d4-fd2d-411d-a30f-69c7c45b14e0 to disappear
Sep  6 11:33:26.308: INFO: Pod pod-secrets-9abca6d4-fd2d-411d-a30f-69c7c45b14e0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:33:26.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3532" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":160,"skipped":2692,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:33:26.318: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Sep  6 11:33:26.345: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:33:42.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8114" for this suite.

• [SLOW TEST:15.742 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":161,"skipped":2698,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:33:42.061: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6667
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Sep  6 11:33:42.109: INFO: Found 0 stateful pods, waiting for 3
Sep  6 11:33:52.113: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:33:52.114: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:33:52.114: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 11:33:52.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-6667 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 11:33:52.273: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 11:33:52.273: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 11:33:52.273: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep  6 11:34:02.304: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep  6 11:34:12.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-6667 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 11:34:12.452: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  6 11:34:12.452: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 11:34:12.452: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Sep  6 11:34:32.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-6667 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  6 11:34:32.629: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  6 11:34:32.629: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  6 11:34:32.629: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  6 11:34:42.667: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep  6 11:34:52.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=statefulset-6667 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  6 11:34:52.836: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  6 11:34:52.836: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  6 11:34:52.836: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  6 11:35:02.858: INFO: Waiting for StatefulSet statefulset-6667/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  6 11:35:12.865: INFO: Deleting all statefulset in ns statefulset-6667
Sep  6 11:35:12.870: INFO: Scaling statefulset ss2 to 0
Sep  6 11:35:22.890: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 11:35:22.893: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:35:22.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6667" for this suite.

• [SLOW TEST:100.859 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":162,"skipped":2727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:35:22.920: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep  6 11:35:23.378: INFO: starting watch
STEP: patching
STEP: updating
Sep  6 11:35:23.391: INFO: waiting for watch events with expected annotations
Sep  6 11:35:23.391: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:35:23.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7812" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":163,"skipped":2795,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:35:23.450: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep  6 11:35:25.492: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8938 PodName:pod-sharedvolume-5433914d-2afa-4c98-a4a0-ed63bb9f6203 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 11:35:25.492: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 11:35:25.565: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:35:25.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8938" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":164,"skipped":2818,"failed":0}
SS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:35:25.575: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:35:25.602: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-485529b3-ad49-4547-80fa-53806ed94f47" in namespace "security-context-test-5277" to be "Succeeded or Failed"
Sep  6 11:35:25.608: INFO: Pod "alpine-nnp-false-485529b3-ad49-4547-80fa-53806ed94f47": Phase="Pending", Reason="", readiness=false. Elapsed: 5.886125ms
Sep  6 11:35:27.614: INFO: Pod "alpine-nnp-false-485529b3-ad49-4547-80fa-53806ed94f47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01156077s
Sep  6 11:35:29.620: INFO: Pod "alpine-nnp-false-485529b3-ad49-4547-80fa-53806ed94f47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017344072s
Sep  6 11:35:31.624: INFO: Pod "alpine-nnp-false-485529b3-ad49-4547-80fa-53806ed94f47": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021251085s
Sep  6 11:35:33.627: INFO: Pod "alpine-nnp-false-485529b3-ad49-4547-80fa-53806ed94f47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024688301s
Sep  6 11:35:33.627: INFO: Pod "alpine-nnp-false-485529b3-ad49-4547-80fa-53806ed94f47" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:35:33.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5277" for this suite.

• [SLOW TEST:8.071 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":165,"skipped":2820,"failed":0}
SSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:35:33.646: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:35:33.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3042" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":166,"skipped":2823,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:35:33.704: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:35:49.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-124" for this suite.

• [SLOW TEST:16.123 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":167,"skipped":2842,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:35:49.827: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7572
STEP: creating service affinity-nodeport in namespace services-7572
STEP: creating replication controller affinity-nodeport in namespace services-7572
I0906 11:35:49.884798      20 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-7572, replica count: 3
I0906 11:35:52.935867      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 11:35:52.946: INFO: Creating new exec pod
Sep  6 11:35:55.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-7572 exec execpod-affinityhcp76 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Sep  6 11:35:56.103: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep  6 11:35:56.103: INFO: stdout: ""
Sep  6 11:35:56.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-7572 exec execpod-affinityhcp76 -- /bin/sh -x -c nc -zv -t -w 2 10.107.80.87 80'
Sep  6 11:35:56.263: INFO: stderr: "+ nc -zv -t -w 2 10.107.80.87 80\nConnection to 10.107.80.87 80 port [tcp/http] succeeded!\n"
Sep  6 11:35:56.263: INFO: stdout: ""
Sep  6 11:35:56.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-7572 exec execpod-affinityhcp76 -- /bin/sh -x -c nc -zv -t -w 2 172.16.114.13 30932'
Sep  6 11:35:56.409: INFO: stderr: "+ nc -zv -t -w 2 172.16.114.13 30932\nConnection to 172.16.114.13 30932 port [tcp/30932] succeeded!\n"
Sep  6 11:35:56.409: INFO: stdout: ""
Sep  6 11:35:56.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-7572 exec execpod-affinityhcp76 -- /bin/sh -x -c nc -zv -t -w 2 172.16.114.11 30932'
Sep  6 11:35:56.554: INFO: stderr: "+ nc -zv -t -w 2 172.16.114.11 30932\nConnection to 172.16.114.11 30932 port [tcp/30932] succeeded!\n"
Sep  6 11:35:56.554: INFO: stdout: ""
Sep  6 11:35:56.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-7572 exec execpod-affinityhcp76 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.114.11:30932/ ; done'
Sep  6 11:35:56.808: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.114.11:30932/\n"
Sep  6 11:35:56.808: INFO: stdout: "\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm\naffinity-nodeport-v6dgm"
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Received response from host: affinity-nodeport-v6dgm
Sep  6 11:35:56.808: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7572, will wait for the garbage collector to delete the pods
Sep  6 11:35:56.881: INFO: Deleting ReplicationController affinity-nodeport took: 6.058716ms
Sep  6 11:35:56.982: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.496048ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:36:08.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7572" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:18.197 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":168,"skipped":2862,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:36:08.025: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  6 11:36:10.100: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:36:10.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9782" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":169,"skipped":2877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:36:10.117: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep  6 11:36:13.173: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:36:14.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2086" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":170,"skipped":2902,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:36:14.203: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6778
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6778
I0906 11:36:14.269275      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6778, replica count: 2
Sep  6 11:36:17.320: INFO: Creating new exec pod
I0906 11:36:17.320549      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 11:36:20.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-6778 exec execpodn2gxt -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep  6 11:36:20.483: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  6 11:36:20.484: INFO: stdout: ""
Sep  6 11:36:20.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-6778 exec execpodn2gxt -- /bin/sh -x -c nc -zv -t -w 2 10.101.80.186 80'
Sep  6 11:36:20.632: INFO: stderr: "+ nc -zv -t -w 2 10.101.80.186 80\nConnection to 10.101.80.186 80 port [tcp/http] succeeded!\n"
Sep  6 11:36:20.632: INFO: stdout: ""
Sep  6 11:36:20.632: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:36:20.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6778" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.465 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":171,"skipped":2902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:36:20.669: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 11:36:20.718: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b46529a-469e-4dd5-aba4-09968f70ded8" in namespace "downward-api-1379" to be "Succeeded or Failed"
Sep  6 11:36:20.737: INFO: Pod "downwardapi-volume-1b46529a-469e-4dd5-aba4-09968f70ded8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.263629ms
Sep  6 11:36:22.741: INFO: Pod "downwardapi-volume-1b46529a-469e-4dd5-aba4-09968f70ded8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021969122s
STEP: Saw pod success
Sep  6 11:36:22.741: INFO: Pod "downwardapi-volume-1b46529a-469e-4dd5-aba4-09968f70ded8" satisfied condition "Succeeded or Failed"
Sep  6 11:36:22.745: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-1b46529a-469e-4dd5-aba4-09968f70ded8 container client-container: <nil>
STEP: delete the pod
Sep  6 11:36:22.763: INFO: Waiting for pod downwardapi-volume-1b46529a-469e-4dd5-aba4-09968f70ded8 to disappear
Sep  6 11:36:22.773: INFO: Pod downwardapi-volume-1b46529a-469e-4dd5-aba4-09968f70ded8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:36:22.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1379" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":172,"skipped":2960,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:36:22.781: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 11:36:22.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b37e7f4-a134-4f7e-8a09-d42fa2759041" in namespace "downward-api-1892" to be "Succeeded or Failed"
Sep  6 11:36:22.812: INFO: Pod "downwardapi-volume-3b37e7f4-a134-4f7e-8a09-d42fa2759041": Phase="Pending", Reason="", readiness=false. Elapsed: 5.370197ms
Sep  6 11:36:24.816: INFO: Pod "downwardapi-volume-3b37e7f4-a134-4f7e-8a09-d42fa2759041": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00901578s
STEP: Saw pod success
Sep  6 11:36:24.816: INFO: Pod "downwardapi-volume-3b37e7f4-a134-4f7e-8a09-d42fa2759041" satisfied condition "Succeeded or Failed"
Sep  6 11:36:24.818: INFO: Trying to get logs from node vm114012 pod downwardapi-volume-3b37e7f4-a134-4f7e-8a09-d42fa2759041 container client-container: <nil>
STEP: delete the pod
Sep  6 11:36:24.839: INFO: Waiting for pod downwardapi-volume-3b37e7f4-a134-4f7e-8a09-d42fa2759041 to disappear
Sep  6 11:36:24.842: INFO: Pod downwardapi-volume-3b37e7f4-a134-4f7e-8a09-d42fa2759041 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:36:24.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1892" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":173,"skipped":2978,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:36:24.853: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:36:24.887: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:36:25.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1743" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":174,"skipped":2994,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:36:25.924: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep  6 11:36:29.978: INFO: &Pod{ObjectMeta:{send-events-61a5efae-65c4-4039-a39f-ac9d4e9a70c6  events-1741 /api/v1/namespaces/events-1741/pods/send-events-61a5efae-65c4-4039-a39f-ac9d4e9a70c6 6b04899f-bac1-4055-87d3-4b4013c0354a 25266 0 2022-09-06 11:36:25 +0000 UTC <nil> <nil> map[name:foo time:954567283] map[] [] []  [{e2e.test Update v1 2022-09-06 11:36:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 11:36:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cswz8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cswz8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cswz8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114011,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 11:36:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 11:36:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 11:36:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 11:36:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.11,PodIP:10.244.0.143,StartTime:2022-09-06 11:36:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-06 11:36:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://dfe99dca548ca0957e44415e2c2999420fe9ac4f93d66eec61f7759f8efda889,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep  6 11:36:31.982: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep  6 11:36:33.985: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:36:33.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1741" for this suite.

• [SLOW TEST:8.093 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":175,"skipped":2998,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:36:34.019: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep  6 11:36:34.043: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 11:36:36.831: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:36:47.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5003" for this suite.

• [SLOW TEST:13.473 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":176,"skipped":3040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:36:47.492: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:36:47.516: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:36:53.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4437" for this suite.

• [SLOW TEST:6.426 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":177,"skipped":3102,"failed":0}
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:36:53.919: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep  6 11:36:53.949: INFO: Waiting up to 5m0s for pod "downward-api-ba105630-9f1f-47bd-8ec8-7dd1fc86a694" in namespace "downward-api-2948" to be "Succeeded or Failed"
Sep  6 11:36:53.955: INFO: Pod "downward-api-ba105630-9f1f-47bd-8ec8-7dd1fc86a694": Phase="Pending", Reason="", readiness=false. Elapsed: 5.883769ms
Sep  6 11:36:55.959: INFO: Pod "downward-api-ba105630-9f1f-47bd-8ec8-7dd1fc86a694": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009875094s
STEP: Saw pod success
Sep  6 11:36:55.960: INFO: Pod "downward-api-ba105630-9f1f-47bd-8ec8-7dd1fc86a694" satisfied condition "Succeeded or Failed"
Sep  6 11:36:55.963: INFO: Trying to get logs from node vm114012 pod downward-api-ba105630-9f1f-47bd-8ec8-7dd1fc86a694 container dapi-container: <nil>
STEP: delete the pod
Sep  6 11:36:55.980: INFO: Waiting for pod downward-api-ba105630-9f1f-47bd-8ec8-7dd1fc86a694 to disappear
Sep  6 11:36:55.982: INFO: Pod downward-api-ba105630-9f1f-47bd-8ec8-7dd1fc86a694 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:36:55.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2948" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":178,"skipped":3102,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:36:55.991: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6081
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6081
STEP: Creating statefulset with conflicting port in namespace statefulset-6081
STEP: Waiting until pod test-pod will start running in namespace statefulset-6081
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6081
Sep  6 11:36:58.070: INFO: Observed stateful pod in namespace: statefulset-6081, name: ss-0, uid: 45dbd1d7-23d3-4eac-9181-bab57597ee28, status phase: Pending. Waiting for statefulset controller to delete.
Sep  6 11:36:58.107: INFO: Observed stateful pod in namespace: statefulset-6081, name: ss-0, uid: 45dbd1d7-23d3-4eac-9181-bab57597ee28, status phase: Failed. Waiting for statefulset controller to delete.
Sep  6 11:36:58.113: INFO: Observed stateful pod in namespace: statefulset-6081, name: ss-0, uid: 45dbd1d7-23d3-4eac-9181-bab57597ee28, status phase: Failed. Waiting for statefulset controller to delete.
Sep  6 11:36:58.116: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6081
STEP: Removing pod with conflicting port in namespace statefulset-6081
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6081 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  6 11:37:00.140: INFO: Deleting all statefulset in ns statefulset-6081
Sep  6 11:37:00.144: INFO: Scaling statefulset ss to 0
Sep  6 11:37:10.164: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 11:37:10.166: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:37:10.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6081" for this suite.

• [SLOW TEST:14.200 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":179,"skipped":3105,"failed":0}
SS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:37:10.190: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep  6 11:37:10.212: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Sep  6 11:37:10.403: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  6 11:37:12.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:37:14.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:37:16.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:37:18.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:37:20.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:37:22.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:37:24.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:37:26.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:37:28.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:37:30.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:37:32.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:37:34.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:37:36.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:37:38.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798061030, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 11:37:41.086: INFO: Waited 615.05185ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:37:42.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8306" for this suite.

• [SLOW TEST:31.942 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":180,"skipped":3107,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:37:42.134: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-qp6r
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 11:37:42.173: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qp6r" in namespace "subpath-8788" to be "Succeeded or Failed"
Sep  6 11:37:42.183: INFO: Pod "pod-subpath-test-configmap-qp6r": Phase="Pending", Reason="", readiness=false. Elapsed: 9.356042ms
Sep  6 11:37:44.186: INFO: Pod "pod-subpath-test-configmap-qp6r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012971796s
Sep  6 11:37:46.190: INFO: Pod "pod-subpath-test-configmap-qp6r": Phase="Running", Reason="", readiness=true. Elapsed: 4.016562326s
Sep  6 11:37:48.195: INFO: Pod "pod-subpath-test-configmap-qp6r": Phase="Running", Reason="", readiness=true. Elapsed: 6.021521884s
Sep  6 11:37:50.199: INFO: Pod "pod-subpath-test-configmap-qp6r": Phase="Running", Reason="", readiness=true. Elapsed: 8.025281707s
Sep  6 11:37:52.202: INFO: Pod "pod-subpath-test-configmap-qp6r": Phase="Running", Reason="", readiness=true. Elapsed: 10.028617469s
Sep  6 11:37:54.205: INFO: Pod "pod-subpath-test-configmap-qp6r": Phase="Running", Reason="", readiness=true. Elapsed: 12.031616128s
Sep  6 11:37:56.208: INFO: Pod "pod-subpath-test-configmap-qp6r": Phase="Running", Reason="", readiness=true. Elapsed: 14.035004969s
Sep  6 11:37:58.215: INFO: Pod "pod-subpath-test-configmap-qp6r": Phase="Running", Reason="", readiness=true. Elapsed: 16.04151448s
Sep  6 11:38:00.219: INFO: Pod "pod-subpath-test-configmap-qp6r": Phase="Running", Reason="", readiness=true. Elapsed: 18.045969448s
Sep  6 11:38:02.223: INFO: Pod "pod-subpath-test-configmap-qp6r": Phase="Running", Reason="", readiness=true. Elapsed: 20.049218114s
Sep  6 11:38:04.226: INFO: Pod "pod-subpath-test-configmap-qp6r": Phase="Running", Reason="", readiness=true. Elapsed: 22.052468189s
Sep  6 11:38:06.230: INFO: Pod "pod-subpath-test-configmap-qp6r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.056206918s
STEP: Saw pod success
Sep  6 11:38:06.230: INFO: Pod "pod-subpath-test-configmap-qp6r" satisfied condition "Succeeded or Failed"
Sep  6 11:38:06.233: INFO: Trying to get logs from node vm114011 pod pod-subpath-test-configmap-qp6r container test-container-subpath-configmap-qp6r: <nil>
STEP: delete the pod
Sep  6 11:38:06.258: INFO: Waiting for pod pod-subpath-test-configmap-qp6r to disappear
Sep  6 11:38:06.261: INFO: Pod pod-subpath-test-configmap-qp6r no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qp6r
Sep  6 11:38:06.261: INFO: Deleting pod "pod-subpath-test-configmap-qp6r" in namespace "subpath-8788"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:38:06.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8788" for this suite.

• [SLOW TEST:24.138 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":181,"skipped":3152,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:38:06.272: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Sep  6 11:38:06.301: INFO: created test-podtemplate-1
Sep  6 11:38:06.305: INFO: created test-podtemplate-2
Sep  6 11:38:06.310: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Sep  6 11:38:06.313: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Sep  6 11:38:06.321: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:38:06.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9103" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":182,"skipped":3175,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:38:06.330: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  6 11:38:08.376: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:38:08.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-793" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":183,"skipped":3176,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:38:08.397: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep  6 11:38:12.951: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1779 pod-service-account-fb63cf3b-699e-4295-963a-be12a986a5a6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep  6 11:38:13.094: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1779 pod-service-account-fb63cf3b-699e-4295-963a-be12a986a5a6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep  6 11:38:13.238: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1779 pod-service-account-fb63cf3b-699e-4295-963a-be12a986a5a6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:38:13.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1779" for this suite.

• [SLOW TEST:5.003 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":184,"skipped":3199,"failed":0}
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:38:13.401: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:38:15.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4642" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":185,"skipped":3199,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:38:15.508: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:38:15.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8240" for this suite.
STEP: Destroying namespace "nspatchtest-b010a8e9-c51c-49f2-9479-90dcca094d60-7916" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":186,"skipped":3200,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:38:15.576: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-0bfb83f1-49a8-49fe-bed7-30722f30d93f
STEP: Creating a pod to test consume secrets
Sep  6 11:38:15.610: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-16a690ed-cf38-4314-a4a4-bd341fff8eff" in namespace "projected-7135" to be "Succeeded or Failed"
Sep  6 11:38:15.616: INFO: Pod "pod-projected-secrets-16a690ed-cf38-4314-a4a4-bd341fff8eff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.810033ms
Sep  6 11:38:17.620: INFO: Pod "pod-projected-secrets-16a690ed-cf38-4314-a4a4-bd341fff8eff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010197793s
STEP: Saw pod success
Sep  6 11:38:17.620: INFO: Pod "pod-projected-secrets-16a690ed-cf38-4314-a4a4-bd341fff8eff" satisfied condition "Succeeded or Failed"
Sep  6 11:38:17.623: INFO: Trying to get logs from node vm114012 pod pod-projected-secrets-16a690ed-cf38-4314-a4a4-bd341fff8eff container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 11:38:17.639: INFO: Waiting for pod pod-projected-secrets-16a690ed-cf38-4314-a4a4-bd341fff8eff to disappear
Sep  6 11:38:17.643: INFO: Pod pod-projected-secrets-16a690ed-cf38-4314-a4a4-bd341fff8eff no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:38:17.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7135" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":187,"skipped":3204,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:38:17.653: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:38:17.677: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:38:19.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8047" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":188,"skipped":3210,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:38:19.714: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:39:19.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-681" for this suite.

• [SLOW TEST:60.042 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":189,"skipped":3215,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:39:19.758: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:39:19.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7575" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":190,"skipped":3259,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:39:19.806: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep  6 11:39:19.834: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 11:39:21.609: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:39:29.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5184" for this suite.

• [SLOW TEST:9.573 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":191,"skipped":3297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:39:29.382: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  6 11:39:31.425: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:39:31.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2619" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":192,"skipped":3352,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:39:31.445: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Sep  6 11:39:31.471: INFO: Waiting up to 5m0s for pod "var-expansion-baeb5c1e-d248-4d4a-98a7-2700d03c581d" in namespace "var-expansion-6416" to be "Succeeded or Failed"
Sep  6 11:39:31.474: INFO: Pod "var-expansion-baeb5c1e-d248-4d4a-98a7-2700d03c581d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.650661ms
Sep  6 11:39:33.478: INFO: Pod "var-expansion-baeb5c1e-d248-4d4a-98a7-2700d03c581d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007401884s
STEP: Saw pod success
Sep  6 11:39:33.478: INFO: Pod "var-expansion-baeb5c1e-d248-4d4a-98a7-2700d03c581d" satisfied condition "Succeeded or Failed"
Sep  6 11:39:33.482: INFO: Trying to get logs from node vm114011 pod var-expansion-baeb5c1e-d248-4d4a-98a7-2700d03c581d container dapi-container: <nil>
STEP: delete the pod
Sep  6 11:39:33.499: INFO: Waiting for pod var-expansion-baeb5c1e-d248-4d4a-98a7-2700d03c581d to disappear
Sep  6 11:39:33.502: INFO: Pod var-expansion-baeb5c1e-d248-4d4a-98a7-2700d03c581d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:39:33.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6416" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":193,"skipped":3355,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:39:33.511: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Sep  6 11:39:33.543: INFO: Waiting up to 5m0s for pod "pod-f517e2ca-bbd8-4f1c-9df3-b54ffc5726bf" in namespace "emptydir-9851" to be "Succeeded or Failed"
Sep  6 11:39:33.547: INFO: Pod "pod-f517e2ca-bbd8-4f1c-9df3-b54ffc5726bf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.645471ms
Sep  6 11:39:35.559: INFO: Pod "pod-f517e2ca-bbd8-4f1c-9df3-b54ffc5726bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015719735s
STEP: Saw pod success
Sep  6 11:39:35.559: INFO: Pod "pod-f517e2ca-bbd8-4f1c-9df3-b54ffc5726bf" satisfied condition "Succeeded or Failed"
Sep  6 11:39:35.564: INFO: Trying to get logs from node vm114011 pod pod-f517e2ca-bbd8-4f1c-9df3-b54ffc5726bf container test-container: <nil>
STEP: delete the pod
Sep  6 11:39:35.577: INFO: Waiting for pod pod-f517e2ca-bbd8-4f1c-9df3-b54ffc5726bf to disappear
Sep  6 11:39:35.581: INFO: Pod pod-f517e2ca-bbd8-4f1c-9df3-b54ffc5726bf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:39:35.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9851" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":194,"skipped":3381,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:39:35.587: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:39:35.615: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Creating first CR 
Sep  6 11:39:36.160: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-06T11:39:36Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-06T11:39:36Z]] name:name1 resourceVersion:26594 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:529be659-f41d-4e8c-bda3-1b61c365a76e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep  6 11:39:46.164: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-06T11:39:46Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-06T11:39:46Z]] name:name2 resourceVersion:26648 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:4b881d92-2337-4d3d-b102-e77b88027da9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep  6 11:39:56.170: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-06T11:39:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-06T11:39:56Z]] name:name1 resourceVersion:26676 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:529be659-f41d-4e8c-bda3-1b61c365a76e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep  6 11:40:06.176: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-06T11:39:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-06T11:40:06Z]] name:name2 resourceVersion:26701 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:4b881d92-2337-4d3d-b102-e77b88027da9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep  6 11:40:16.183: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-06T11:39:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-06T11:39:56Z]] name:name1 resourceVersion:26727 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:529be659-f41d-4e8c-bda3-1b61c365a76e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep  6 11:40:26.190: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-09-06T11:39:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-09-06T11:40:06Z]] name:name2 resourceVersion:26752 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:4b881d92-2337-4d3d-b102-e77b88027da9] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:40:36.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1233" for this suite.

• [SLOW TEST:61.122 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":195,"skipped":3394,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:40:36.710: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-b3df3f46-0bae-41b3-8ba6-19b180483f10
STEP: Creating a pod to test consume configMaps
Sep  6 11:40:36.742: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba9db9aa-51fd-439d-9281-651bb1dc344e" in namespace "configmap-6911" to be "Succeeded or Failed"
Sep  6 11:40:36.746: INFO: Pod "pod-configmaps-ba9db9aa-51fd-439d-9281-651bb1dc344e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.289507ms
Sep  6 11:40:38.750: INFO: Pod "pod-configmaps-ba9db9aa-51fd-439d-9281-651bb1dc344e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007482066s
STEP: Saw pod success
Sep  6 11:40:38.750: INFO: Pod "pod-configmaps-ba9db9aa-51fd-439d-9281-651bb1dc344e" satisfied condition "Succeeded or Failed"
Sep  6 11:40:38.754: INFO: Trying to get logs from node vm114011 pod pod-configmaps-ba9db9aa-51fd-439d-9281-651bb1dc344e container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 11:40:38.768: INFO: Waiting for pod pod-configmaps-ba9db9aa-51fd-439d-9281-651bb1dc344e to disappear
Sep  6 11:40:38.773: INFO: Pod pod-configmaps-ba9db9aa-51fd-439d-9281-651bb1dc344e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:40:38.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6911" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":196,"skipped":3409,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:40:38.782: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  6 11:40:44.846: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 11:40:44.850: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 11:40:46.850: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 11:40:46.854: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 11:40:48.851: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 11:40:48.854: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:40:48.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6108" for this suite.

• [SLOW TEST:10.087 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":197,"skipped":3443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:40:48.870: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:40:54.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6408" for this suite.

• [SLOW TEST:6.039 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":198,"skipped":3485,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:40:54.910: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:40:54.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3805" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":199,"skipped":3491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:40:54.966: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9444.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9444.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9444.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9444.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9444.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9444.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 11:40:57.020: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:40:57.024: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:40:57.035: INFO: Unable to read jessie_udp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:40:57.039: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:40:57.039: INFO: Lookups using dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 11:41:02.050: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:02.053: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:02.061: INFO: Unable to read jessie_udp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:02.064: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:02.064: INFO: Lookups using dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 11:41:07.048: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:07.052: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:07.061: INFO: Unable to read jessie_udp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:07.065: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:07.065: INFO: Lookups using dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 11:41:12.049: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:12.052: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:12.060: INFO: Unable to read jessie_udp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:12.062: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:12.062: INFO: Lookups using dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 11:41:17.048: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:17.051: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:17.061: INFO: Unable to read jessie_udp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:17.065: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:17.065: INFO: Lookups using dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 11:41:22.049: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:22.053: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:22.061: INFO: Unable to read jessie_udp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:22.064: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b: the server could not find the requested resource (get pods dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b)
Sep  6 11:41:22.064: INFO: Lookups using dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 11:41:27.062: INFO: DNS probes using dns-9444/dns-test-ceadb5ea-12f7-44af-badf-639ac95f355b succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:41:27.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9444" for this suite.

• [SLOW TEST:32.120 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":200,"skipped":3514,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:41:27.086: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1307
STEP: creating the pod
Sep  6 11:41:27.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4093 create -f -'
Sep  6 11:41:27.444: INFO: stderr: ""
Sep  6 11:41:27.444: INFO: stdout: "pod/pause created\n"
Sep  6 11:41:27.444: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  6 11:41:27.444: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4093" to be "running and ready"
Sep  6 11:41:27.453: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.889162ms
Sep  6 11:41:29.459: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.015230208s
Sep  6 11:41:29.459: INFO: Pod "pause" satisfied condition "running and ready"
Sep  6 11:41:29.459: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Sep  6 11:41:29.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4093 label pods pause testing-label=testing-label-value'
Sep  6 11:41:29.541: INFO: stderr: ""
Sep  6 11:41:29.541: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep  6 11:41:29.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4093 get pod pause -L testing-label'
Sep  6 11:41:29.604: INFO: stderr: ""
Sep  6 11:41:29.605: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep  6 11:41:29.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4093 label pods pause testing-label-'
Sep  6 11:41:29.676: INFO: stderr: ""
Sep  6 11:41:29.676: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep  6 11:41:29.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4093 get pod pause -L testing-label'
Sep  6 11:41:29.741: INFO: stderr: ""
Sep  6 11:41:29.742: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1313
STEP: using delete to clean up resources
Sep  6 11:41:29.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4093 delete --grace-period=0 --force -f -'
Sep  6 11:41:29.825: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 11:41:29.825: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  6 11:41:29.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4093 get rc,svc -l name=pause --no-headers'
Sep  6 11:41:29.899: INFO: stderr: "No resources found in kubectl-4093 namespace.\n"
Sep  6 11:41:29.899: INFO: stdout: ""
Sep  6 11:41:29.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-4093 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 11:41:29.967: INFO: stderr: ""
Sep  6 11:41:29.967: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:41:29.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4093" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":201,"skipped":3528,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:41:29.976: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  6 11:41:32.552: INFO: Successfully updated pod "pod-update-220bfb61-d4d6-48fa-8e46-365bbd0ff2b5"
STEP: verifying the updated pod is in kubernetes
Sep  6 11:41:32.558: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:41:32.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4397" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":202,"skipped":3543,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:41:32.573: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Sep  6 11:41:32.610: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  6 11:42:32.634: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Sep  6 11:42:32.652: INFO: Created pod: pod0-sched-preemption-low-priority
Sep  6 11:42:32.667: INFO: Created pod: pod1-sched-preemption-medium-priority
Sep  6 11:42:32.686: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:42:56.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9261" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:84.206 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":203,"skipped":3580,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:42:56.780: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Sep  6 11:42:56.819: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep  6 11:42:56.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-6103 create -f -'
Sep  6 11:42:57.017: INFO: stderr: ""
Sep  6 11:42:57.017: INFO: stdout: "service/agnhost-replica created\n"
Sep  6 11:42:57.017: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep  6 11:42:57.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-6103 create -f -'
Sep  6 11:42:57.255: INFO: stderr: ""
Sep  6 11:42:57.255: INFO: stdout: "service/agnhost-primary created\n"
Sep  6 11:42:57.255: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  6 11:42:57.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-6103 create -f -'
Sep  6 11:42:57.425: INFO: stderr: ""
Sep  6 11:42:57.425: INFO: stdout: "service/frontend created\n"
Sep  6 11:42:57.426: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep  6 11:42:57.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-6103 create -f -'
Sep  6 11:42:57.582: INFO: stderr: ""
Sep  6 11:42:57.582: INFO: stdout: "deployment.apps/frontend created\n"
Sep  6 11:42:57.582: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  6 11:42:57.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-6103 create -f -'
Sep  6 11:42:57.739: INFO: stderr: ""
Sep  6 11:42:57.739: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep  6 11:42:57.739: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  6 11:42:57.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-6103 create -f -'
Sep  6 11:42:57.927: INFO: stderr: ""
Sep  6 11:42:57.927: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Sep  6 11:42:57.927: INFO: Waiting for all frontend pods to be Running.
Sep  6 11:43:02.978: INFO: Waiting for frontend to serve content.
Sep  6 11:43:02.988: INFO: Trying to add a new entry to the guestbook.
Sep  6 11:43:02.996: INFO: Verifying that added entry can be retrieved.
Sep  6 11:43:03.003: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Sep  6 11:43:08.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-6103 delete --grace-period=0 --force -f -'
Sep  6 11:43:08.117: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 11:43:08.117: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 11:43:08.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-6103 delete --grace-period=0 --force -f -'
Sep  6 11:43:08.256: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 11:43:08.257: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 11:43:08.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-6103 delete --grace-period=0 --force -f -'
Sep  6 11:43:08.343: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 11:43:08.343: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 11:43:08.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-6103 delete --grace-period=0 --force -f -'
Sep  6 11:43:08.408: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 11:43:08.408: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 11:43:08.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-6103 delete --grace-period=0 --force -f -'
Sep  6 11:43:08.554: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 11:43:08.554: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 11:43:08.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-6103 delete --grace-period=0 --force -f -'
Sep  6 11:43:08.663: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 11:43:08.664: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:43:08.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6103" for this suite.

• [SLOW TEST:11.902 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":204,"skipped":3591,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:43:08.682: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Sep  6 11:43:08.757: INFO: Major version: 1
STEP: Confirm minor version
Sep  6 11:43:08.757: INFO: cleanMinorVersion: 19
Sep  6 11:43:08.757: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:43:08.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-3746" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":205,"skipped":3621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:43:08.774: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-ec9e78db-4513-40df-860a-7fcd2f6b527d
STEP: Creating secret with name s-test-opt-upd-6777c7cd-d639-49eb-949b-ad86bfcd51fd
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ec9e78db-4513-40df-860a-7fcd2f6b527d
STEP: Updating secret s-test-opt-upd-6777c7cd-d639-49eb-949b-ad86bfcd51fd
STEP: Creating secret with name s-test-opt-create-1dd6b6cc-b85c-4d1c-838d-aaa27af5f079
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:43:14.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4655" for this suite.

• [SLOW TEST:6.181 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":206,"skipped":3646,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:43:14.954: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-10a738bb-027e-4672-aaf9-dba9b9dd7a9b
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-10a738bb-027e-4672-aaf9-dba9b9dd7a9b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:43:19.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2127" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":207,"skipped":3661,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:43:19.039: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:43:21.107: INFO: Waiting up to 5m0s for pod "client-envvars-3b2e5681-1b1b-4444-ad9f-2d0798381593" in namespace "pods-4164" to be "Succeeded or Failed"
Sep  6 11:43:21.112: INFO: Pod "client-envvars-3b2e5681-1b1b-4444-ad9f-2d0798381593": Phase="Pending", Reason="", readiness=false. Elapsed: 5.003492ms
Sep  6 11:43:23.115: INFO: Pod "client-envvars-3b2e5681-1b1b-4444-ad9f-2d0798381593": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008563814s
STEP: Saw pod success
Sep  6 11:43:23.116: INFO: Pod "client-envvars-3b2e5681-1b1b-4444-ad9f-2d0798381593" satisfied condition "Succeeded or Failed"
Sep  6 11:43:23.119: INFO: Trying to get logs from node vm114012 pod client-envvars-3b2e5681-1b1b-4444-ad9f-2d0798381593 container env3cont: <nil>
STEP: delete the pod
Sep  6 11:43:23.135: INFO: Waiting for pod client-envvars-3b2e5681-1b1b-4444-ad9f-2d0798381593 to disappear
Sep  6 11:43:23.140: INFO: Pod client-envvars-3b2e5681-1b1b-4444-ad9f-2d0798381593 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:43:23.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4164" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":208,"skipped":3674,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:43:23.151: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  6 11:43:23.180: INFO: Waiting up to 5m0s for pod "pod-0dccc379-ec6a-4708-b4d8-de5e94319342" in namespace "emptydir-5065" to be "Succeeded or Failed"
Sep  6 11:43:23.182: INFO: Pod "pod-0dccc379-ec6a-4708-b4d8-de5e94319342": Phase="Pending", Reason="", readiness=false. Elapsed: 2.346858ms
Sep  6 11:43:25.186: INFO: Pod "pod-0dccc379-ec6a-4708-b4d8-de5e94319342": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006385459s
STEP: Saw pod success
Sep  6 11:43:25.186: INFO: Pod "pod-0dccc379-ec6a-4708-b4d8-de5e94319342" satisfied condition "Succeeded or Failed"
Sep  6 11:43:25.189: INFO: Trying to get logs from node vm114012 pod pod-0dccc379-ec6a-4708-b4d8-de5e94319342 container test-container: <nil>
STEP: delete the pod
Sep  6 11:43:25.202: INFO: Waiting for pod pod-0dccc379-ec6a-4708-b4d8-de5e94319342 to disappear
Sep  6 11:43:25.211: INFO: Pod pod-0dccc379-ec6a-4708-b4d8-de5e94319342 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:43:25.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5065" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":209,"skipped":3676,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:43:25.247: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Sep  6 11:43:25.292: INFO: Waiting up to 5m0s for pod "var-expansion-b16ef8ea-0433-45d9-8827-1cadc06fe064" in namespace "var-expansion-8611" to be "Succeeded or Failed"
Sep  6 11:43:25.304: INFO: Pod "var-expansion-b16ef8ea-0433-45d9-8827-1cadc06fe064": Phase="Pending", Reason="", readiness=false. Elapsed: 11.559408ms
Sep  6 11:43:27.307: INFO: Pod "var-expansion-b16ef8ea-0433-45d9-8827-1cadc06fe064": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014760119s
STEP: Saw pod success
Sep  6 11:43:27.307: INFO: Pod "var-expansion-b16ef8ea-0433-45d9-8827-1cadc06fe064" satisfied condition "Succeeded or Failed"
Sep  6 11:43:27.310: INFO: Trying to get logs from node vm114011 pod var-expansion-b16ef8ea-0433-45d9-8827-1cadc06fe064 container dapi-container: <nil>
STEP: delete the pod
Sep  6 11:43:27.326: INFO: Waiting for pod var-expansion-b16ef8ea-0433-45d9-8827-1cadc06fe064 to disappear
Sep  6 11:43:27.330: INFO: Pod var-expansion-b16ef8ea-0433-45d9-8827-1cadc06fe064 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:43:27.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8611" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":210,"skipped":3692,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:43:27.346: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-f1a9a17b-1127-4d48-b711-e9dd51b4f4c6
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:43:27.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7713" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":211,"skipped":3694,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:43:27.378: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2468
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-2468
Sep  6 11:43:27.415: INFO: Found 0 stateful pods, waiting for 1
Sep  6 11:43:37.419: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  6 11:43:37.437: INFO: Deleting all statefulset in ns statefulset-2468
Sep  6 11:43:37.443: INFO: Scaling statefulset ss to 0
Sep  6 11:43:57.466: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 11:43:57.468: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:43:57.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2468" for this suite.

• [SLOW TEST:30.120 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":212,"skipped":3713,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:43:57.499: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-tzh8
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 11:43:57.535: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tzh8" in namespace "subpath-134" to be "Succeeded or Failed"
Sep  6 11:43:57.549: INFO: Pod "pod-subpath-test-configmap-tzh8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.210642ms
Sep  6 11:43:59.552: INFO: Pod "pod-subpath-test-configmap-tzh8": Phase="Running", Reason="", readiness=true. Elapsed: 2.017115053s
Sep  6 11:44:01.556: INFO: Pod "pod-subpath-test-configmap-tzh8": Phase="Running", Reason="", readiness=true. Elapsed: 4.02075676s
Sep  6 11:44:03.560: INFO: Pod "pod-subpath-test-configmap-tzh8": Phase="Running", Reason="", readiness=true. Elapsed: 6.025131075s
Sep  6 11:44:05.564: INFO: Pod "pod-subpath-test-configmap-tzh8": Phase="Running", Reason="", readiness=true. Elapsed: 8.028910865s
Sep  6 11:44:07.568: INFO: Pod "pod-subpath-test-configmap-tzh8": Phase="Running", Reason="", readiness=true. Elapsed: 10.033125251s
Sep  6 11:44:09.572: INFO: Pod "pod-subpath-test-configmap-tzh8": Phase="Running", Reason="", readiness=true. Elapsed: 12.037064365s
Sep  6 11:44:11.576: INFO: Pod "pod-subpath-test-configmap-tzh8": Phase="Running", Reason="", readiness=true. Elapsed: 14.040265299s
Sep  6 11:44:13.579: INFO: Pod "pod-subpath-test-configmap-tzh8": Phase="Running", Reason="", readiness=true. Elapsed: 16.043895572s
Sep  6 11:44:15.583: INFO: Pod "pod-subpath-test-configmap-tzh8": Phase="Running", Reason="", readiness=true. Elapsed: 18.047778017s
Sep  6 11:44:17.587: INFO: Pod "pod-subpath-test-configmap-tzh8": Phase="Running", Reason="", readiness=true. Elapsed: 20.051301246s
Sep  6 11:44:19.590: INFO: Pod "pod-subpath-test-configmap-tzh8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.054838091s
STEP: Saw pod success
Sep  6 11:44:19.590: INFO: Pod "pod-subpath-test-configmap-tzh8" satisfied condition "Succeeded or Failed"
Sep  6 11:44:19.593: INFO: Trying to get logs from node vm114011 pod pod-subpath-test-configmap-tzh8 container test-container-subpath-configmap-tzh8: <nil>
STEP: delete the pod
Sep  6 11:44:19.612: INFO: Waiting for pod pod-subpath-test-configmap-tzh8 to disappear
Sep  6 11:44:19.615: INFO: Pod pod-subpath-test-configmap-tzh8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tzh8
Sep  6 11:44:19.615: INFO: Deleting pod "pod-subpath-test-configmap-tzh8" in namespace "subpath-134"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:44:19.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-134" for this suite.

• [SLOW TEST:22.129 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":213,"skipped":3738,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:44:19.628: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0906 11:44:20.306047      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0906 11:44:20.306219      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0906 11:44:20.306250      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep  6 11:44:20.306: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:44:20.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4364" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":214,"skipped":3747,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:44:20.320: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:44:31.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4835" for this suite.

• [SLOW TEST:11.095 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":215,"skipped":3751,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:44:31.414: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  6 11:44:31.446: INFO: Waiting up to 5m0s for pod "pod-781b3257-c310-447e-89b1-2417c306fb6a" in namespace "emptydir-7357" to be "Succeeded or Failed"
Sep  6 11:44:31.463: INFO: Pod "pod-781b3257-c310-447e-89b1-2417c306fb6a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.518783ms
Sep  6 11:44:33.466: INFO: Pod "pod-781b3257-c310-447e-89b1-2417c306fb6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019860815s
STEP: Saw pod success
Sep  6 11:44:33.466: INFO: Pod "pod-781b3257-c310-447e-89b1-2417c306fb6a" satisfied condition "Succeeded or Failed"
Sep  6 11:44:33.469: INFO: Trying to get logs from node vm114011 pod pod-781b3257-c310-447e-89b1-2417c306fb6a container test-container: <nil>
STEP: delete the pod
Sep  6 11:44:33.486: INFO: Waiting for pod pod-781b3257-c310-447e-89b1-2417c306fb6a to disappear
Sep  6 11:44:33.490: INFO: Pod pod-781b3257-c310-447e-89b1-2417c306fb6a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:44:33.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7357" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":216,"skipped":3753,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:44:33.498: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep  6 11:44:33.519: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:44:36.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7558" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":217,"skipped":3754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:44:36.611: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 11:44:36.653: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6361d9b-122c-421a-b12b-f46af28a7cc0" in namespace "projected-9721" to be "Succeeded or Failed"
Sep  6 11:44:36.657: INFO: Pod "downwardapi-volume-b6361d9b-122c-421a-b12b-f46af28a7cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.890967ms
Sep  6 11:44:38.666: INFO: Pod "downwardapi-volume-b6361d9b-122c-421a-b12b-f46af28a7cc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011905795s
STEP: Saw pod success
Sep  6 11:44:38.666: INFO: Pod "downwardapi-volume-b6361d9b-122c-421a-b12b-f46af28a7cc0" satisfied condition "Succeeded or Failed"
Sep  6 11:44:38.668: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-b6361d9b-122c-421a-b12b-f46af28a7cc0 container client-container: <nil>
STEP: delete the pod
Sep  6 11:44:38.683: INFO: Waiting for pod downwardapi-volume-b6361d9b-122c-421a-b12b-f46af28a7cc0 to disappear
Sep  6 11:44:38.686: INFO: Pod downwardapi-volume-b6361d9b-122c-421a-b12b-f46af28a7cc0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:44:38.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9721" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":218,"skipped":3802,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:44:38.695: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Sep  6 11:44:38.715: INFO: namespace kubectl-9608
Sep  6 11:44:38.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-9608 create -f -'
Sep  6 11:44:38.863: INFO: stderr: ""
Sep  6 11:44:38.863: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep  6 11:44:39.867: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 11:44:39.867: INFO: Found 1 / 1
Sep  6 11:44:39.867: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 11:44:39.870: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 11:44:39.870: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 11:44:39.870: INFO: wait on agnhost-primary startup in kubectl-9608 
Sep  6 11:44:39.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-9608 logs agnhost-primary-twc64 agnhost-primary'
Sep  6 11:44:39.948: INFO: stderr: ""
Sep  6 11:44:39.948: INFO: stdout: "Paused\n"
STEP: exposing RC
Sep  6 11:44:39.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-9608 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Sep  6 11:44:40.025: INFO: stderr: ""
Sep  6 11:44:40.025: INFO: stdout: "service/rm2 exposed\n"
Sep  6 11:44:40.031: INFO: Service rm2 in namespace kubectl-9608 found.
STEP: exposing service
Sep  6 11:44:42.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-9608 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Sep  6 11:44:42.106: INFO: stderr: ""
Sep  6 11:44:42.106: INFO: stdout: "service/rm3 exposed\n"
Sep  6 11:44:42.110: INFO: Service rm3 in namespace kubectl-9608 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:44:44.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9608" for this suite.

• [SLOW TEST:5.431 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1222
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":219,"skipped":3803,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:44:44.125: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:44:44.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5213" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":220,"skipped":3805,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:44:44.197: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:44:44.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3897" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":221,"skipped":3823,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:44:44.273: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:44:44.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2767" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":222,"skipped":3831,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:44:44.352: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:44:44.378: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep  6 11:44:46.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-7788 --namespace=crd-publish-openapi-7788 create -f -'
Sep  6 11:44:46.376: INFO: stderr: ""
Sep  6 11:44:46.377: INFO: stdout: "e2e-test-crd-publish-openapi-3245-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  6 11:44:46.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-7788 --namespace=crd-publish-openapi-7788 delete e2e-test-crd-publish-openapi-3245-crds test-cr'
Sep  6 11:44:46.451: INFO: stderr: ""
Sep  6 11:44:46.451: INFO: stdout: "e2e-test-crd-publish-openapi-3245-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep  6 11:44:46.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-7788 --namespace=crd-publish-openapi-7788 apply -f -'
Sep  6 11:44:46.611: INFO: stderr: ""
Sep  6 11:44:46.612: INFO: stdout: "e2e-test-crd-publish-openapi-3245-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  6 11:44:46.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-7788 --namespace=crd-publish-openapi-7788 delete e2e-test-crd-publish-openapi-3245-crds test-cr'
Sep  6 11:44:46.679: INFO: stderr: ""
Sep  6 11:44:46.679: INFO: stdout: "e2e-test-crd-publish-openapi-3245-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep  6 11:44:46.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-7788 explain e2e-test-crd-publish-openapi-3245-crds'
Sep  6 11:44:46.821: INFO: stderr: ""
Sep  6 11:44:46.821: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3245-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:44:49.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7788" for this suite.

• [SLOW TEST:5.250 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":223,"skipped":3853,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:44:49.604: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Sep  6 11:44:49.641: INFO: Waiting up to 5m0s for pod "client-containers-d0e18a73-6edb-4271-9804-0798b4ce73cd" in namespace "containers-9764" to be "Succeeded or Failed"
Sep  6 11:44:49.650: INFO: Pod "client-containers-d0e18a73-6edb-4271-9804-0798b4ce73cd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.091659ms
Sep  6 11:44:51.653: INFO: Pod "client-containers-d0e18a73-6edb-4271-9804-0798b4ce73cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012407648s
STEP: Saw pod success
Sep  6 11:44:51.653: INFO: Pod "client-containers-d0e18a73-6edb-4271-9804-0798b4ce73cd" satisfied condition "Succeeded or Failed"
Sep  6 11:44:51.656: INFO: Trying to get logs from node vm114012 pod client-containers-d0e18a73-6edb-4271-9804-0798b4ce73cd container test-container: <nil>
STEP: delete the pod
Sep  6 11:44:51.672: INFO: Waiting for pod client-containers-d0e18a73-6edb-4271-9804-0798b4ce73cd to disappear
Sep  6 11:44:51.675: INFO: Pod client-containers-d0e18a73-6edb-4271-9804-0798b4ce73cd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:44:51.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9764" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":224,"skipped":3864,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:44:51.688: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0906 11:45:01.776120      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0906 11:45:01.776180      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0906 11:45:01.776191      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep  6 11:45:01.776: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  6 11:45:01.776: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qmcz" in namespace "gc-4714"
Sep  6 11:45:01.786: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdgb7" in namespace "gc-4714"
Sep  6 11:45:01.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-ct5kb" in namespace "gc-4714"
Sep  6 11:45:01.819: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmh7l" in namespace "gc-4714"
Sep  6 11:45:01.840: INFO: Deleting pod "simpletest-rc-to-be-deleted-kp2p9" in namespace "gc-4714"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:45:01.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4714" for this suite.

• [SLOW TEST:10.199 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":225,"skipped":3877,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:45:01.888: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-9816
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 11:45:01.952: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  6 11:45:02.022: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:45:04.026: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:45:06.025: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:45:08.026: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:45:10.026: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:45:12.025: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:45:14.026: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:45:16.025: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:45:18.026: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:45:20.025: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:45:22.025: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep  6 11:45:22.029: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  6 11:45:24.033: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep  6 11:45:24.038: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep  6 11:45:26.059: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.132:8080/dial?request=hostname&protocol=udp&host=10.244.0.174&port=8081&tries=1'] Namespace:pod-network-test-9816 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 11:45:26.059: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 11:45:26.141: INFO: Waiting for responses: map[]
Sep  6 11:45:26.144: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.132:8080/dial?request=hostname&protocol=udp&host=10.244.1.131&port=8081&tries=1'] Namespace:pod-network-test-9816 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 11:45:26.144: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 11:45:26.234: INFO: Waiting for responses: map[]
Sep  6 11:45:26.238: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.132:8080/dial?request=hostname&protocol=udp&host=10.244.2.57&port=8081&tries=1'] Namespace:pod-network-test-9816 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 11:45:26.238: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 11:45:26.328: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:45:26.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9816" for this suite.

• [SLOW TEST:24.454 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":226,"skipped":3891,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:45:26.342: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:45:28.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7428" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":227,"skipped":3894,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:45:28.401: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 11:45:28.676: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 11:45:31.696: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:45:31.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-906" for this suite.
STEP: Destroying namespace "webhook-906-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":228,"skipped":3933,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:45:31.816: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:45:38.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8304" for this suite.

• [SLOW TEST:7.102 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":229,"skipped":3944,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:45:38.918: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-e12dfb6d-c787-40d4-9b7e-700913c9bef5 in namespace container-probe-4090
Sep  6 11:45:40.963: INFO: Started pod liveness-e12dfb6d-c787-40d4-9b7e-700913c9bef5 in namespace container-probe-4090
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 11:45:40.966: INFO: Initial restart count of pod liveness-e12dfb6d-c787-40d4-9b7e-700913c9bef5 is 0
Sep  6 11:45:58.999: INFO: Restart count of pod container-probe-4090/liveness-e12dfb6d-c787-40d4-9b7e-700913c9bef5 is now 1 (18.033711058s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:45:59.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4090" for this suite.

• [SLOW TEST:20.109 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":230,"skipped":3949,"failed":0}
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:45:59.027: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:45:59.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5370" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":231,"skipped":3949,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:45:59.086: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  6 11:45:59.130: INFO: Waiting up to 5m0s for pod "pod-1348d443-7e25-430a-80ab-4d30a7b93eb1" in namespace "emptydir-1660" to be "Succeeded or Failed"
Sep  6 11:45:59.137: INFO: Pod "pod-1348d443-7e25-430a-80ab-4d30a7b93eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.857588ms
Sep  6 11:46:01.142: INFO: Pod "pod-1348d443-7e25-430a-80ab-4d30a7b93eb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011667472s
STEP: Saw pod success
Sep  6 11:46:01.142: INFO: Pod "pod-1348d443-7e25-430a-80ab-4d30a7b93eb1" satisfied condition "Succeeded or Failed"
Sep  6 11:46:01.145: INFO: Trying to get logs from node vm114012 pod pod-1348d443-7e25-430a-80ab-4d30a7b93eb1 container test-container: <nil>
STEP: delete the pod
Sep  6 11:46:01.159: INFO: Waiting for pod pod-1348d443-7e25-430a-80ab-4d30a7b93eb1 to disappear
Sep  6 11:46:01.161: INFO: Pod pod-1348d443-7e25-430a-80ab-4d30a7b93eb1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:46:01.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1660" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":232,"skipped":3969,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:46:01.170: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  6 11:46:05.246: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 11:46:05.249: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 11:46:07.250: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 11:46:07.255: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 11:46:09.250: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 11:46:09.257: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:46:09.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1705" for this suite.

• [SLOW TEST:8.096 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":233,"skipped":4015,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:46:09.267: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:46:22.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9726" for this suite.

• [SLOW TEST:13.088 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":234,"skipped":4036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:46:22.355: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-a2a2c11a-ca69-4703-81f8-ceb5e30142dc
STEP: Creating a pod to test consume configMaps
Sep  6 11:46:22.391: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ed02e1c5-f2f1-4a0f-8b9e-8a1efc1169cf" in namespace "projected-8736" to be "Succeeded or Failed"
Sep  6 11:46:22.393: INFO: Pod "pod-projected-configmaps-ed02e1c5-f2f1-4a0f-8b9e-8a1efc1169cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.484462ms
Sep  6 11:46:24.397: INFO: Pod "pod-projected-configmaps-ed02e1c5-f2f1-4a0f-8b9e-8a1efc1169cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006768187s
STEP: Saw pod success
Sep  6 11:46:24.398: INFO: Pod "pod-projected-configmaps-ed02e1c5-f2f1-4a0f-8b9e-8a1efc1169cf" satisfied condition "Succeeded or Failed"
Sep  6 11:46:24.400: INFO: Trying to get logs from node vm114011 pod pod-projected-configmaps-ed02e1c5-f2f1-4a0f-8b9e-8a1efc1169cf container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 11:46:24.416: INFO: Waiting for pod pod-projected-configmaps-ed02e1c5-f2f1-4a0f-8b9e-8a1efc1169cf to disappear
Sep  6 11:46:24.419: INFO: Pod pod-projected-configmaps-ed02e1c5-f2f1-4a0f-8b9e-8a1efc1169cf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:46:24.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8736" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":235,"skipped":4070,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:46:24.432: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Sep  6 11:46:24.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-8218 create -f -'
Sep  6 11:46:24.637: INFO: stderr: ""
Sep  6 11:46:24.637: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep  6 11:46:25.641: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 11:46:25.641: INFO: Found 0 / 1
Sep  6 11:46:26.649: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 11:46:26.649: INFO: Found 0 / 1
Sep  6 11:46:27.641: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 11:46:27.641: INFO: Found 1 / 1
Sep  6 11:46:27.641: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep  6 11:46:27.644: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 11:46:27.644: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 11:46:27.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-8218 patch pod agnhost-primary-cn9cd -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  6 11:46:27.717: INFO: stderr: ""
Sep  6 11:46:27.717: INFO: stdout: "pod/agnhost-primary-cn9cd patched\n"
STEP: checking annotations
Sep  6 11:46:27.721: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 11:46:27.721: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:46:27.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8218" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":236,"skipped":4073,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:46:27.728: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:46:38.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6610" for this suite.

• [SLOW TEST:11.062 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":237,"skipped":4073,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:46:38.790: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep  6 11:46:38.826: INFO: Waiting up to 5m0s for pod "downward-api-0b23121e-2d4c-4433-9357-a0e5a1572d9e" in namespace "downward-api-923" to be "Succeeded or Failed"
Sep  6 11:46:38.837: INFO: Pod "downward-api-0b23121e-2d4c-4433-9357-a0e5a1572d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.823126ms
Sep  6 11:46:40.841: INFO: Pod "downward-api-0b23121e-2d4c-4433-9357-a0e5a1572d9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014447044s
STEP: Saw pod success
Sep  6 11:46:40.841: INFO: Pod "downward-api-0b23121e-2d4c-4433-9357-a0e5a1572d9e" satisfied condition "Succeeded or Failed"
Sep  6 11:46:40.845: INFO: Trying to get logs from node vm114011 pod downward-api-0b23121e-2d4c-4433-9357-a0e5a1572d9e container dapi-container: <nil>
STEP: delete the pod
Sep  6 11:46:40.871: INFO: Waiting for pod downward-api-0b23121e-2d4c-4433-9357-a0e5a1572d9e to disappear
Sep  6 11:46:40.874: INFO: Pod downward-api-0b23121e-2d4c-4433-9357-a0e5a1572d9e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:46:40.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-923" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":238,"skipped":4079,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:46:40.889: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-83eeab82-c2d1-4799-ba25-8ede577daa9e
STEP: Creating a pod to test consume secrets
Sep  6 11:46:40.921: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0cf51afb-ea03-47d2-ba63-39f5b884f31f" in namespace "projected-3533" to be "Succeeded or Failed"
Sep  6 11:46:40.924: INFO: Pod "pod-projected-secrets-0cf51afb-ea03-47d2-ba63-39f5b884f31f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.421156ms
Sep  6 11:46:42.928: INFO: Pod "pod-projected-secrets-0cf51afb-ea03-47d2-ba63-39f5b884f31f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00685439s
Sep  6 11:46:44.932: INFO: Pod "pod-projected-secrets-0cf51afb-ea03-47d2-ba63-39f5b884f31f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010083262s
STEP: Saw pod success
Sep  6 11:46:44.932: INFO: Pod "pod-projected-secrets-0cf51afb-ea03-47d2-ba63-39f5b884f31f" satisfied condition "Succeeded or Failed"
Sep  6 11:46:44.934: INFO: Trying to get logs from node vm114011 pod pod-projected-secrets-0cf51afb-ea03-47d2-ba63-39f5b884f31f container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 11:46:44.951: INFO: Waiting for pod pod-projected-secrets-0cf51afb-ea03-47d2-ba63-39f5b884f31f to disappear
Sep  6 11:46:44.954: INFO: Pod pod-projected-secrets-0cf51afb-ea03-47d2-ba63-39f5b884f31f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:46:44.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3533" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":239,"skipped":4130,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:46:44.962: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:46:44.988: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  6 11:46:45.002: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 11:46:47.021: INFO: Creating deployment "test-rolling-update-deployment"
Sep  6 11:46:47.026: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  6 11:46:47.034: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  6 11:46:49.039: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  6 11:46:49.041: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep  6 11:46:49.048: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9320 /apis/apps/v1/namespaces/deployment-9320/deployments/test-rolling-update-deployment 1b4da014-f5fc-4aae-8d85-a927609563a9 29636 1 2022-09-06 11:46:47 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-09-06 11:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-09-06 11:46:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004847158 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-09-06 11:46:47 +0000 UTC,LastTransitionTime:2022-09-06 11:46:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2022-09-06 11:46:48 +0000 UTC,LastTransitionTime:2022-09-06 11:46:47 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 11:46:49.052: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-9320 /apis/apps/v1/namespaces/deployment-9320/replicasets/test-rolling-update-deployment-c4cb8d6d9 991eadbb-7340-447c-9fdb-ab27b16140b5 29626 1 2022-09-06 11:46:47 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 1b4da014-f5fc-4aae-8d85-a927609563a9 0xc0048476e0 0xc0048476e1}] []  [{kube-controller-manager Update apps/v1 2022-09-06 11:46:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b4da014-f5fc-4aae-8d85-a927609563a9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004847758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  6 11:46:49.052: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  6 11:46:49.052: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9320 /apis/apps/v1/namespaces/deployment-9320/replicasets/test-rolling-update-controller b8d6a569-a380-49c9-a8cb-86f856e0c8c8 29635 2 2022-09-06 11:46:44 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 1b4da014-f5fc-4aae-8d85-a927609563a9 0xc0048475d7 0xc0048475d8}] []  [{e2e.test Update apps/v1 2022-09-06 11:46:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-09-06 11:46:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b4da014-f5fc-4aae-8d85-a927609563a9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004847678 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 11:46:49.056: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-vds49" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-vds49 test-rolling-update-deployment-c4cb8d6d9- deployment-9320 /api/v1/namespaces/deployment-9320/pods/test-rolling-update-deployment-c4cb8d6d9-vds49 6363e9aa-8a71-4800-b284-7b8f3061cffc 29625 0 2022-09-06 11:46:47 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 991eadbb-7340-447c-9fdb-ab27b16140b5 0xc006586d20 0xc006586d21}] []  [{kube-controller-manager Update v1 2022-09-06 11:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991eadbb-7340-447c-9fdb-ab27b16140b5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-09-06 11:46:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xmmp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xmmp5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xmmp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vm114012,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 11:46:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 11:46:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 11:46:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-09-06 11:46:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.114.12,PodIP:10.244.1.137,StartTime:2022-09-06 11:46:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-09-06 11:46:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://741819605d67431114fefbc060882670bf9df47c1b411a8392ead9e439d07d92,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:46:49.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9320" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":240,"skipped":4149,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:46:49.063: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep  6 11:46:49.087: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:46:52.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6722" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":241,"skipped":4154,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:46:52.604: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Sep  6 11:46:52.636: INFO: Created pod &Pod{ObjectMeta:{dns-1379  dns-1379 /api/v1/namespaces/dns-1379/pods/dns-1379 4ff31392-8f60-4548-bbc1-264fd1fad879 29684 0 2022-09-06 11:46:52 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-09-06 11:46:52 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zztd6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zztd6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zztd6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 11:46:52.640: INFO: The status of Pod dns-1379 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:46:54.643: INFO: The status of Pod dns-1379 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Sep  6 11:46:54.643: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1379 PodName:dns-1379 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 11:46:54.643: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Verifying customized DNS server is configured on pod...
Sep  6 11:46:54.727: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1379 PodName:dns-1379 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 11:46:54.727: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 11:46:54.812: INFO: Deleting pod dns-1379...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:46:54.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1379" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":242,"skipped":4163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:46:54.837: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  6 11:46:54.874: INFO: Waiting up to 5m0s for pod "pod-8aa6c4d2-724f-43f9-9197-0fe6378a7907" in namespace "emptydir-1116" to be "Succeeded or Failed"
Sep  6 11:46:54.878: INFO: Pod "pod-8aa6c4d2-724f-43f9-9197-0fe6378a7907": Phase="Pending", Reason="", readiness=false. Elapsed: 3.380731ms
Sep  6 11:46:56.881: INFO: Pod "pod-8aa6c4d2-724f-43f9-9197-0fe6378a7907": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006993664s
STEP: Saw pod success
Sep  6 11:46:56.881: INFO: Pod "pod-8aa6c4d2-724f-43f9-9197-0fe6378a7907" satisfied condition "Succeeded or Failed"
Sep  6 11:46:56.885: INFO: Trying to get logs from node vm114011 pod pod-8aa6c4d2-724f-43f9-9197-0fe6378a7907 container test-container: <nil>
STEP: delete the pod
Sep  6 11:46:56.910: INFO: Waiting for pod pod-8aa6c4d2-724f-43f9-9197-0fe6378a7907 to disappear
Sep  6 11:46:56.914: INFO: Pod pod-8aa6c4d2-724f-43f9-9197-0fe6378a7907 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:46:56.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1116" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":243,"skipped":4190,"failed":0}
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:46:56.922: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  6 11:47:00.990: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 11:47:00.993: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 11:47:02.995: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 11:47:02.999: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 11:47:04.995: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 11:47:04.999: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:47:04.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8994" for this suite.

• [SLOW TEST:8.090 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":244,"skipped":4194,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:47:05.012: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2834
Sep  6 11:47:07.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-2834 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep  6 11:47:07.210: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Sep  6 11:47:07.210: INFO: stdout: "ipvs"
Sep  6 11:47:07.210: INFO: proxyMode: ipvs
Sep  6 11:47:07.216: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep  6 11:47:07.221: INFO: Pod kube-proxy-mode-detector still exists
Sep  6 11:47:09.222: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep  6 11:47:09.225: INFO: Pod kube-proxy-mode-detector still exists
Sep  6 11:47:11.221: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep  6 11:47:11.225: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-2834
STEP: creating replication controller affinity-clusterip-timeout in namespace services-2834
I0906 11:47:11.248306      20 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-2834, replica count: 3
I0906 11:47:14.299249      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 11:47:14.305: INFO: Creating new exec pod
Sep  6 11:47:17.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-2834 exec execpod-affinitys9kfc -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Sep  6 11:47:17.461: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Sep  6 11:47:17.461: INFO: stdout: ""
Sep  6 11:47:17.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-2834 exec execpod-affinitys9kfc -- /bin/sh -x -c nc -zv -t -w 2 10.102.16.28 80'
Sep  6 11:47:17.611: INFO: stderr: "+ nc -zv -t -w 2 10.102.16.28 80\nConnection to 10.102.16.28 80 port [tcp/http] succeeded!\n"
Sep  6 11:47:17.611: INFO: stdout: ""
Sep  6 11:47:17.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-2834 exec execpod-affinitys9kfc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.16.28:80/ ; done'
Sep  6 11:47:17.800: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n"
Sep  6 11:47:17.800: INFO: stdout: "\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq\naffinity-clusterip-timeout-hjmlq"
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Received response from host: affinity-clusterip-timeout-hjmlq
Sep  6 11:47:17.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-2834 exec execpod-affinitys9kfc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.102.16.28:80/'
Sep  6 11:47:17.962: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n"
Sep  6 11:47:17.962: INFO: stdout: "affinity-clusterip-timeout-hjmlq"
Sep  6 11:49:22.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-2834 exec execpod-affinitys9kfc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.102.16.28:80/'
Sep  6 11:49:23.109: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n"
Sep  6 11:49:23.109: INFO: stdout: "affinity-clusterip-timeout-hjmlq"
Sep  6 11:51:28.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-2834 exec execpod-affinitys9kfc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.102.16.28:80/'
Sep  6 11:51:28.258: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n"
Sep  6 11:51:28.258: INFO: stdout: "affinity-clusterip-timeout-hjmlq"
Sep  6 11:53:33.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-2834 exec execpod-affinitys9kfc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.102.16.28:80/'
Sep  6 11:53:33.398: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n"
Sep  6 11:53:33.398: INFO: stdout: "affinity-clusterip-timeout-hjmlq"
Sep  6 11:55:38.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-2834 exec execpod-affinitys9kfc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.102.16.28:80/'
Sep  6 11:55:38.633: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.102.16.28:80/\n"
Sep  6 11:55:38.633: INFO: stdout: "affinity-clusterip-timeout-665ws"
Sep  6 11:55:38.633: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-2834, will wait for the garbage collector to delete the pods
Sep  6 11:55:38.704: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 4.939409ms
Sep  6 11:55:38.805: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.501038ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:55:47.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2834" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:522.935 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":245,"skipped":4214,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:55:47.948: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep  6 11:55:48.006: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:55:49.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8766" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":246,"skipped":4246,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:55:49.066: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-2f277610-5181-4652-a4e3-325d45582912
STEP: Creating a pod to test consume secrets
Sep  6 11:55:49.102: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-16a9d241-7926-459a-a43b-626600c7820e" in namespace "projected-4732" to be "Succeeded or Failed"
Sep  6 11:55:49.106: INFO: Pod "pod-projected-secrets-16a9d241-7926-459a-a43b-626600c7820e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.918257ms
Sep  6 11:55:51.109: INFO: Pod "pod-projected-secrets-16a9d241-7926-459a-a43b-626600c7820e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007188839s
STEP: Saw pod success
Sep  6 11:55:51.109: INFO: Pod "pod-projected-secrets-16a9d241-7926-459a-a43b-626600c7820e" satisfied condition "Succeeded or Failed"
Sep  6 11:55:51.113: INFO: Trying to get logs from node vm114011 pod pod-projected-secrets-16a9d241-7926-459a-a43b-626600c7820e container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 11:55:51.132: INFO: Waiting for pod pod-projected-secrets-16a9d241-7926-459a-a43b-626600c7820e to disappear
Sep  6 11:55:51.137: INFO: Pod pod-projected-secrets-16a9d241-7926-459a-a43b-626600c7820e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:55:51.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4732" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":247,"skipped":4250,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:55:51.146: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-7czr
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 11:55:51.178: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-7czr" in namespace "subpath-8530" to be "Succeeded or Failed"
Sep  6 11:55:51.182: INFO: Pod "pod-subpath-test-secret-7czr": Phase="Pending", Reason="", readiness=false. Elapsed: 3.980399ms
Sep  6 11:55:53.187: INFO: Pod "pod-subpath-test-secret-7czr": Phase="Running", Reason="", readiness=true. Elapsed: 2.008800254s
Sep  6 11:55:55.190: INFO: Pod "pod-subpath-test-secret-7czr": Phase="Running", Reason="", readiness=true. Elapsed: 4.012286062s
Sep  6 11:55:57.195: INFO: Pod "pod-subpath-test-secret-7czr": Phase="Running", Reason="", readiness=true. Elapsed: 6.017107646s
Sep  6 11:55:59.202: INFO: Pod "pod-subpath-test-secret-7czr": Phase="Running", Reason="", readiness=true. Elapsed: 8.024169157s
Sep  6 11:56:01.206: INFO: Pod "pod-subpath-test-secret-7czr": Phase="Running", Reason="", readiness=true. Elapsed: 10.027949112s
Sep  6 11:56:03.210: INFO: Pod "pod-subpath-test-secret-7czr": Phase="Running", Reason="", readiness=true. Elapsed: 12.032004092s
Sep  6 11:56:05.213: INFO: Pod "pod-subpath-test-secret-7czr": Phase="Running", Reason="", readiness=true. Elapsed: 14.035659204s
Sep  6 11:56:07.217: INFO: Pod "pod-subpath-test-secret-7czr": Phase="Running", Reason="", readiness=true. Elapsed: 16.038902836s
Sep  6 11:56:09.220: INFO: Pod "pod-subpath-test-secret-7czr": Phase="Running", Reason="", readiness=true. Elapsed: 18.041753132s
Sep  6 11:56:11.224: INFO: Pod "pod-subpath-test-secret-7czr": Phase="Running", Reason="", readiness=true. Elapsed: 20.045871648s
Sep  6 11:56:13.227: INFO: Pod "pod-subpath-test-secret-7czr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.049509083s
STEP: Saw pod success
Sep  6 11:56:13.227: INFO: Pod "pod-subpath-test-secret-7czr" satisfied condition "Succeeded or Failed"
Sep  6 11:56:13.230: INFO: Trying to get logs from node vm114012 pod pod-subpath-test-secret-7czr container test-container-subpath-secret-7czr: <nil>
STEP: delete the pod
Sep  6 11:56:13.250: INFO: Waiting for pod pod-subpath-test-secret-7czr to disappear
Sep  6 11:56:13.255: INFO: Pod pod-subpath-test-secret-7czr no longer exists
STEP: Deleting pod pod-subpath-test-secret-7czr
Sep  6 11:56:13.255: INFO: Deleting pod "pod-subpath-test-secret-7czr" in namespace "subpath-8530"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:56:13.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8530" for this suite.

• [SLOW TEST:22.129 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":248,"skipped":4281,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:56:13.274: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0906 11:56:19.334681      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0906 11:56:19.334877      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0906 11:56:19.334962      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep  6 11:56:19.335: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:56:19.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-784" for this suite.

• [SLOW TEST:6.073 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":249,"skipped":4301,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:56:19.348: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7721
STEP: creating service affinity-clusterip-transition in namespace services-7721
STEP: creating replication controller affinity-clusterip-transition in namespace services-7721
I0906 11:56:19.395567      20 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-7721, replica count: 3
I0906 11:56:22.446723      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 11:56:25.447248      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 11:56:25.453: INFO: Creating new exec pod
Sep  6 11:56:28.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-7721 exec execpod-affinitynq8nt -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Sep  6 11:56:28.617: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep  6 11:56:28.617: INFO: stdout: ""
Sep  6 11:56:28.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-7721 exec execpod-affinitynq8nt -- /bin/sh -x -c nc -zv -t -w 2 10.102.153.139 80'
Sep  6 11:56:28.757: INFO: stderr: "+ nc -zv -t -w 2 10.102.153.139 80\nConnection to 10.102.153.139 80 port [tcp/http] succeeded!\n"
Sep  6 11:56:28.757: INFO: stdout: ""
Sep  6 11:56:28.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-7721 exec execpod-affinitynq8nt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.153.139:80/ ; done'
Sep  6 11:56:28.972: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n"
Sep  6 11:56:28.972: INFO: stdout: "\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm"
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:28.972: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:58.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-7721 exec execpod-affinitynq8nt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.153.139:80/ ; done'
Sep  6 11:56:59.166: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n"
Sep  6 11:56:59.166: INFO: stdout: "\naffinity-clusterip-transition-2dc4p\naffinity-clusterip-transition-rpx8b\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-2dc4p\naffinity-clusterip-transition-rpx8b\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-2dc4p\naffinity-clusterip-transition-rpx8b\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-2dc4p\naffinity-clusterip-transition-rpx8b\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-2dc4p\naffinity-clusterip-transition-rpx8b\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-2dc4p"
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-2dc4p
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-rpx8b
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-2dc4p
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-rpx8b
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-2dc4p
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-rpx8b
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-2dc4p
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-rpx8b
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-2dc4p
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-rpx8b
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.166: INFO: Received response from host: affinity-clusterip-transition-2dc4p
Sep  6 11:56:59.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-7721 exec execpod-affinitynq8nt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.153.139:80/ ; done'
Sep  6 11:56:59.390: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.153.139:80/\n"
Sep  6 11:56:59.391: INFO: stdout: "\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm\naffinity-clusterip-transition-c6bnm"
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Received response from host: affinity-clusterip-transition-c6bnm
Sep  6 11:56:59.391: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7721, will wait for the garbage collector to delete the pods
Sep  6 11:56:59.467: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.184612ms
Sep  6 11:56:59.568: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.5425ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:57:10.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7721" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:50.673 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":250,"skipped":4305,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:57:10.023: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 11:57:10.074: INFO: Waiting up to 5m0s for pod "downwardapi-volume-922f5b59-bb90-4c30-a52b-3f4c6b13a9d1" in namespace "projected-3638" to be "Succeeded or Failed"
Sep  6 11:57:10.081: INFO: Pod "downwardapi-volume-922f5b59-bb90-4c30-a52b-3f4c6b13a9d1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.760999ms
Sep  6 11:57:12.085: INFO: Pod "downwardapi-volume-922f5b59-bb90-4c30-a52b-3f4c6b13a9d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010800661s
STEP: Saw pod success
Sep  6 11:57:12.085: INFO: Pod "downwardapi-volume-922f5b59-bb90-4c30-a52b-3f4c6b13a9d1" satisfied condition "Succeeded or Failed"
Sep  6 11:57:12.089: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-922f5b59-bb90-4c30-a52b-3f4c6b13a9d1 container client-container: <nil>
STEP: delete the pod
Sep  6 11:57:12.103: INFO: Waiting for pod downwardapi-volume-922f5b59-bb90-4c30-a52b-3f4c6b13a9d1 to disappear
Sep  6 11:57:12.107: INFO: Pod downwardapi-volume-922f5b59-bb90-4c30-a52b-3f4c6b13a9d1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:57:12.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3638" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":251,"skipped":4321,"failed":0}

------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:57:12.115: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Sep  6 11:57:12.143: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Sep  6 11:57:12.151: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  6 11:57:12.151: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Sep  6 11:57:12.158: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  6 11:57:12.158: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Sep  6 11:57:12.171: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep  6 11:57:12.171: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Sep  6 11:57:19.213: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:57:19.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7602" for this suite.

• [SLOW TEST:7.118 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":252,"skipped":4321,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:57:19.233: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 11:57:19.548: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 11:57:22.564: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:57:22.568: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:57:23.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9199" for this suite.
STEP: Destroying namespace "webhook-9199-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":253,"skipped":4336,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:57:23.713: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-1d1b8dfc-7396-4c45-8808-86ccf38943dc
STEP: Creating a pod to test consume secrets
Sep  6 11:57:23.802: INFO: Waiting up to 5m0s for pod "pod-secrets-392962f8-7e2d-4157-aae3-0db51fae7297" in namespace "secrets-5242" to be "Succeeded or Failed"
Sep  6 11:57:23.813: INFO: Pod "pod-secrets-392962f8-7e2d-4157-aae3-0db51fae7297": Phase="Pending", Reason="", readiness=false. Elapsed: 10.704497ms
Sep  6 11:57:25.820: INFO: Pod "pod-secrets-392962f8-7e2d-4157-aae3-0db51fae7297": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017560499s
Sep  6 11:57:27.832: INFO: Pod "pod-secrets-392962f8-7e2d-4157-aae3-0db51fae7297": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029878261s
STEP: Saw pod success
Sep  6 11:57:27.832: INFO: Pod "pod-secrets-392962f8-7e2d-4157-aae3-0db51fae7297" satisfied condition "Succeeded or Failed"
Sep  6 11:57:27.836: INFO: Trying to get logs from node vm114011 pod pod-secrets-392962f8-7e2d-4157-aae3-0db51fae7297 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 11:57:27.850: INFO: Waiting for pod pod-secrets-392962f8-7e2d-4157-aae3-0db51fae7297 to disappear
Sep  6 11:57:27.854: INFO: Pod pod-secrets-392962f8-7e2d-4157-aae3-0db51fae7297 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:57:27.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5242" for this suite.
STEP: Destroying namespace "secret-namespace-2318" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":254,"skipped":4336,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:57:27.868: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Sep  6 11:57:27.911: INFO: Waiting up to 5m0s for pod "client-containers-b1797c4c-7519-4978-bb47-b20fd452547b" in namespace "containers-2917" to be "Succeeded or Failed"
Sep  6 11:57:27.916: INFO: Pod "client-containers-b1797c4c-7519-4978-bb47-b20fd452547b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.365743ms
Sep  6 11:57:29.919: INFO: Pod "client-containers-b1797c4c-7519-4978-bb47-b20fd452547b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008293044s
STEP: Saw pod success
Sep  6 11:57:29.920: INFO: Pod "client-containers-b1797c4c-7519-4978-bb47-b20fd452547b" satisfied condition "Succeeded or Failed"
Sep  6 11:57:29.922: INFO: Trying to get logs from node vm114011 pod client-containers-b1797c4c-7519-4978-bb47-b20fd452547b container test-container: <nil>
STEP: delete the pod
Sep  6 11:57:29.938: INFO: Waiting for pod client-containers-b1797c4c-7519-4978-bb47-b20fd452547b to disappear
Sep  6 11:57:29.941: INFO: Pod client-containers-b1797c4c-7519-4978-bb47-b20fd452547b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:57:29.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2917" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":255,"skipped":4358,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:57:29.948: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 11:57:29.968: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep  6 11:57:31.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-7651 --namespace=crd-publish-openapi-7651 create -f -'
Sep  6 11:57:31.972: INFO: stderr: ""
Sep  6 11:57:31.972: INFO: stdout: "e2e-test-crd-publish-openapi-4928-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  6 11:57:31.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-7651 --namespace=crd-publish-openapi-7651 delete e2e-test-crd-publish-openapi-4928-crds test-cr'
Sep  6 11:57:32.036: INFO: stderr: ""
Sep  6 11:57:32.037: INFO: stdout: "e2e-test-crd-publish-openapi-4928-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep  6 11:57:32.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-7651 --namespace=crd-publish-openapi-7651 apply -f -'
Sep  6 11:57:32.179: INFO: stderr: ""
Sep  6 11:57:32.179: INFO: stdout: "e2e-test-crd-publish-openapi-4928-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  6 11:57:32.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-7651 --namespace=crd-publish-openapi-7651 delete e2e-test-crd-publish-openapi-4928-crds test-cr'
Sep  6 11:57:32.254: INFO: stderr: ""
Sep  6 11:57:32.254: INFO: stdout: "e2e-test-crd-publish-openapi-4928-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep  6 11:57:32.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-7651 explain e2e-test-crd-publish-openapi-4928-crds'
Sep  6 11:57:32.393: INFO: stderr: ""
Sep  6 11:57:32.393: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4928-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:57:35.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7651" for this suite.

• [SLOW TEST:5.198 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":256,"skipped":4359,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:57:35.146: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-5793
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 11:57:35.185: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  6 11:57:35.229: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 11:57:37.233: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:57:39.233: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:57:41.234: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:57:43.233: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:57:45.233: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:57:47.234: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:57:49.233: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:57:51.233: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 11:57:53.233: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep  6 11:57:53.238: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep  6 11:57:53.242: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep  6 11:57:55.275: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.197:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5793 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 11:57:55.275: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 11:57:55.354: INFO: Found all expected endpoints: [netserver-0]
Sep  6 11:57:55.358: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.149:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5793 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 11:57:55.358: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 11:57:55.438: INFO: Found all expected endpoints: [netserver-1]
Sep  6 11:57:55.442: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.63:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5793 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 11:57:55.442: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 11:57:55.527: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:57:55.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5793" for this suite.

• [SLOW TEST:20.395 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":257,"skipped":4368,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:57:55.541: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-d88651db-7781-4ed2-8691-25b0bdd48da2
STEP: Creating a pod to test consume secrets
Sep  6 11:57:55.578: INFO: Waiting up to 5m0s for pod "pod-secrets-029fee64-deec-4869-9cb1-185cde3d380a" in namespace "secrets-5901" to be "Succeeded or Failed"
Sep  6 11:57:55.582: INFO: Pod "pod-secrets-029fee64-deec-4869-9cb1-185cde3d380a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.200361ms
Sep  6 11:57:57.585: INFO: Pod "pod-secrets-029fee64-deec-4869-9cb1-185cde3d380a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006607336s
STEP: Saw pod success
Sep  6 11:57:57.585: INFO: Pod "pod-secrets-029fee64-deec-4869-9cb1-185cde3d380a" satisfied condition "Succeeded or Failed"
Sep  6 11:57:57.588: INFO: Trying to get logs from node vm114012 pod pod-secrets-029fee64-deec-4869-9cb1-185cde3d380a container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 11:57:57.607: INFO: Waiting for pod pod-secrets-029fee64-deec-4869-9cb1-185cde3d380a to disappear
Sep  6 11:57:57.611: INFO: Pod pod-secrets-029fee64-deec-4869-9cb1-185cde3d380a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:57:57.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5901" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":258,"skipped":4371,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:57:57.621: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7042
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7042
I0906 11:57:57.698285      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7042, replica count: 2
Sep  6 11:58:00.748: INFO: Creating new exec pod
I0906 11:58:00.748840      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 11:58:05.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-7042 exec execpod4lgmd -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep  6 11:58:05.963: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  6 11:58:05.963: INFO: stdout: ""
Sep  6 11:58:05.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-7042 exec execpod4lgmd -- /bin/sh -x -c nc -zv -t -w 2 10.111.13.172 80'
Sep  6 11:58:06.095: INFO: stderr: "+ nc -zv -t -w 2 10.111.13.172 80\nConnection to 10.111.13.172 80 port [tcp/http] succeeded!\n"
Sep  6 11:58:06.095: INFO: stdout: ""
Sep  6 11:58:06.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-7042 exec execpod4lgmd -- /bin/sh -x -c nc -zv -t -w 2 172.16.114.13 31938'
Sep  6 11:58:06.229: INFO: stderr: "+ nc -zv -t -w 2 172.16.114.13 31938\nConnection to 172.16.114.13 31938 port [tcp/31938] succeeded!\n"
Sep  6 11:58:06.229: INFO: stdout: ""
Sep  6 11:58:06.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-7042 exec execpod4lgmd -- /bin/sh -x -c nc -zv -t -w 2 172.16.114.12 31938'
Sep  6 11:58:06.375: INFO: stderr: "+ nc -zv -t -w 2 172.16.114.12 31938\nConnection to 172.16.114.12 31938 port [tcp/31938] succeeded!\n"
Sep  6 11:58:06.375: INFO: stdout: ""
Sep  6 11:58:06.375: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 11:58:06.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7042" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:8.791 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":259,"skipped":4397,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 11:58:06.412: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 12:00:06.454: INFO: Deleting pod "var-expansion-6c3c8ede-6113-4906-a1cc-a5646b44a930" in namespace "var-expansion-479"
Sep  6 12:00:06.463: INFO: Wait up to 5m0s for pod "var-expansion-6c3c8ede-6113-4906-a1cc-a5646b44a930" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:00:10.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-479" for this suite.

• [SLOW TEST:124.068 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":260,"skipped":4400,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:00:10.481: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-6jrn
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 12:00:10.563: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6jrn" in namespace "subpath-2624" to be "Succeeded or Failed"
Sep  6 12:00:10.568: INFO: Pod "pod-subpath-test-projected-6jrn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041778ms
Sep  6 12:00:12.571: INFO: Pod "pod-subpath-test-projected-6jrn": Phase="Running", Reason="", readiness=true. Elapsed: 2.007464986s
Sep  6 12:00:14.575: INFO: Pod "pod-subpath-test-projected-6jrn": Phase="Running", Reason="", readiness=true. Elapsed: 4.011269932s
Sep  6 12:00:16.579: INFO: Pod "pod-subpath-test-projected-6jrn": Phase="Running", Reason="", readiness=true. Elapsed: 6.015032553s
Sep  6 12:00:18.582: INFO: Pod "pod-subpath-test-projected-6jrn": Phase="Running", Reason="", readiness=true. Elapsed: 8.018382371s
Sep  6 12:00:20.585: INFO: Pod "pod-subpath-test-projected-6jrn": Phase="Running", Reason="", readiness=true. Elapsed: 10.021734575s
Sep  6 12:00:22.589: INFO: Pod "pod-subpath-test-projected-6jrn": Phase="Running", Reason="", readiness=true. Elapsed: 12.025497404s
Sep  6 12:00:24.594: INFO: Pod "pod-subpath-test-projected-6jrn": Phase="Running", Reason="", readiness=true. Elapsed: 14.030717295s
Sep  6 12:00:26.600: INFO: Pod "pod-subpath-test-projected-6jrn": Phase="Running", Reason="", readiness=true. Elapsed: 16.036537969s
Sep  6 12:00:28.603: INFO: Pod "pod-subpath-test-projected-6jrn": Phase="Running", Reason="", readiness=true. Elapsed: 18.039788303s
Sep  6 12:00:30.607: INFO: Pod "pod-subpath-test-projected-6jrn": Phase="Running", Reason="", readiness=true. Elapsed: 20.043795337s
Sep  6 12:00:32.611: INFO: Pod "pod-subpath-test-projected-6jrn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.047682022s
STEP: Saw pod success
Sep  6 12:00:32.611: INFO: Pod "pod-subpath-test-projected-6jrn" satisfied condition "Succeeded or Failed"
Sep  6 12:00:32.614: INFO: Trying to get logs from node vm114011 pod pod-subpath-test-projected-6jrn container test-container-subpath-projected-6jrn: <nil>
STEP: delete the pod
Sep  6 12:00:32.632: INFO: Waiting for pod pod-subpath-test-projected-6jrn to disappear
Sep  6 12:00:32.637: INFO: Pod pod-subpath-test-projected-6jrn no longer exists
STEP: Deleting pod pod-subpath-test-projected-6jrn
Sep  6 12:00:32.637: INFO: Deleting pod "pod-subpath-test-projected-6jrn" in namespace "subpath-2624"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:00:32.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2624" for this suite.

• [SLOW TEST:22.167 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":261,"skipped":4407,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:00:32.648: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 12:00:32.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-9675 create -f -'
Sep  6 12:00:32.844: INFO: stderr: ""
Sep  6 12:00:32.844: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep  6 12:00:32.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-9675 create -f -'
Sep  6 12:00:33.019: INFO: stderr: ""
Sep  6 12:00:33.019: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep  6 12:00:34.024: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 12:00:34.024: INFO: Found 1 / 1
Sep  6 12:00:34.024: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 12:00:34.028: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 12:00:34.028: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 12:00:34.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-9675 describe pod agnhost-primary-vhnjk'
Sep  6 12:00:34.099: INFO: stderr: ""
Sep  6 12:00:34.099: INFO: stdout: "Name:         agnhost-primary-vhnjk\nNamespace:    kubectl-9675\nPriority:     0\nNode:         vm114011/172.16.114.11\nStart Time:   Tue, 06 Sep 2022 12:00:32 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.244.0.202\nIPs:\n  IP:           10.244.0.202\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://35bccf5e71da7b7fe09846316a71d61371424c770f354166bf4f21ef44c3d459\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 06 Sep 2022 12:00:33 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hkmh5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-hkmh5:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-hkmh5\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-9675/agnhost-primary-vhnjk to vm114011\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Sep  6 12:00:34.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-9675 describe rc agnhost-primary'
Sep  6 12:00:34.186: INFO: stderr: ""
Sep  6 12:00:34.186: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9675\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-vhnjk\n"
Sep  6 12:00:34.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-9675 describe service agnhost-primary'
Sep  6 12:00:34.282: INFO: stderr: ""
Sep  6 12:00:34.282: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9675\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.98.231.106\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.0.202:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  6 12:00:34.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-9675 describe node vm114011'
Sep  6 12:00:34.369: INFO: stderr: ""
Sep  6 12:00:34.369: INFO: stdout: "Name:               vm114011\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=vm114011\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"b2:7e:1e:9f:60:93\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.16.114.11\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 06 Sep 2022 10:09:19 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  vm114011\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 06 Sep 2022 12:00:32 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 06 Sep 2022 11:00:45 +0000   Tue, 06 Sep 2022 11:00:45 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Tue, 06 Sep 2022 11:59:38 +0000   Tue, 06 Sep 2022 10:09:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 06 Sep 2022 11:59:38 +0000   Tue, 06 Sep 2022 10:09:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 06 Sep 2022 11:59:38 +0000   Tue, 06 Sep 2022 10:09:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 06 Sep 2022 11:59:38 +0000   Tue, 06 Sep 2022 10:13:10 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.16.114.11\n  Hostname:    vm114011\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      19688064Ki\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3994664Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nAllocatable:\n  cpu:                    1500m\n  ephemeral-storage:      16546489929\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 2229288Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nSystem Info:\n  Machine ID:                 d48a5767cf734c37b5964a74690c62e7\n  System UUID:                fc624d56-4ef5-1d88-cee2-af8c4faf7f3d\n  Boot ID:                    4a697d20-6251-4f71-807c-da4a411cd2fc\n  Kernel Version:             5.4.0-125-generic\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.9\n  Kubelet Version:            v1.19.16\n  Kube-Proxy Version:         v1.19.16\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (3 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 kube-flannel-ds-7kcnr                                      100m (6%)     100m (6%)   50Mi (2%)        50Mi (2%)      59m\n  kubectl-9675                agnhost-primary-vhnjk                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-9wpsp    0 (0%)        0 (0%)      0 (0%)           0 (0%)         105m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests   Limits\n  --------               --------   ------\n  cpu                    100m (6%)  100m (6%)\n  memory                 50Mi (2%)  50Mi (2%)\n  ephemeral-storage      0 (0%)     0 (0%)\n  hugepages-1Gi          0 (0%)     0 (0%)\n  hugepages-2Mi          0 (0%)     0 (0%)\n  example.com/fakecpu    0          0\n  scheduling.k8s.io/foo  0          0\nEvents:\n  Type    Reason                   Age                  From        Message\n  ----    ------                   ----                 ----        -------\n  Normal  Starting                 111m                 kubelet     Starting kubelet.\n  Normal  NodeHasSufficientMemory  111m (x2 over 111m)  kubelet     Node vm114011 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    111m (x2 over 111m)  kubelet     Node vm114011 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     111m (x2 over 111m)  kubelet     Node vm114011 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  111m                 kubelet     Updated Node Allocatable limit across pods\n  Normal  Starting                 110m                 kube-proxy  Starting kube-proxy.\n  Normal  NodeReady                107m                 kubelet     Node vm114011 status is now: NodeReady\n"
Sep  6 12:00:34.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-9675 describe namespace kubectl-9675'
Sep  6 12:00:34.437: INFO: stderr: ""
Sep  6 12:00:34.437: INFO: stdout: "Name:         kubectl-9675\nLabels:       e2e-framework=kubectl\n              e2e-run=6ff83ee3-076e-4ce8-9fcb-997f420d524a\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:00:34.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9675" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":262,"skipped":4461,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:00:34.445: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-81f61eb6-3859-496c-9240-94f6a75d3d3a in namespace container-probe-411
Sep  6 12:00:36.480: INFO: Started pod busybox-81f61eb6-3859-496c-9240-94f6a75d3d3a in namespace container-probe-411
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 12:00:36.484: INFO: Initial restart count of pod busybox-81f61eb6-3859-496c-9240-94f6a75d3d3a is 0
Sep  6 12:01:28.587: INFO: Restart count of pod container-probe-411/busybox-81f61eb6-3859-496c-9240-94f6a75d3d3a is now 1 (52.103647014s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:01:28.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-411" for this suite.

• [SLOW TEST:54.166 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":263,"skipped":4469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:01:28.617: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-2936107a-ccd8-4988-ac8d-7fde47177e6d
STEP: Creating a pod to test consume secrets
Sep  6 12:01:28.655: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-70d829d8-5089-42e3-85e5-bb57dd1e07de" in namespace "projected-9703" to be "Succeeded or Failed"
Sep  6 12:01:28.659: INFO: Pod "pod-projected-secrets-70d829d8-5089-42e3-85e5-bb57dd1e07de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.899154ms
Sep  6 12:01:30.664: INFO: Pod "pod-projected-secrets-70d829d8-5089-42e3-85e5-bb57dd1e07de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009020283s
STEP: Saw pod success
Sep  6 12:01:30.665: INFO: Pod "pod-projected-secrets-70d829d8-5089-42e3-85e5-bb57dd1e07de" satisfied condition "Succeeded or Failed"
Sep  6 12:01:30.667: INFO: Trying to get logs from node vm114011 pod pod-projected-secrets-70d829d8-5089-42e3-85e5-bb57dd1e07de container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 12:01:30.690: INFO: Waiting for pod pod-projected-secrets-70d829d8-5089-42e3-85e5-bb57dd1e07de to disappear
Sep  6 12:01:30.693: INFO: Pod pod-projected-secrets-70d829d8-5089-42e3-85e5-bb57dd1e07de no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:01:30.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9703" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":264,"skipped":4508,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:01:30.702: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep  6 12:01:30.739: INFO: Waiting up to 5m0s for pod "downward-api-aa4dddd9-c261-453d-9618-a13866b91e02" in namespace "downward-api-1221" to be "Succeeded or Failed"
Sep  6 12:01:30.744: INFO: Pod "downward-api-aa4dddd9-c261-453d-9618-a13866b91e02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.255928ms
Sep  6 12:01:32.748: INFO: Pod "downward-api-aa4dddd9-c261-453d-9618-a13866b91e02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008295392s
STEP: Saw pod success
Sep  6 12:01:32.748: INFO: Pod "downward-api-aa4dddd9-c261-453d-9618-a13866b91e02" satisfied condition "Succeeded or Failed"
Sep  6 12:01:32.751: INFO: Trying to get logs from node vm114011 pod downward-api-aa4dddd9-c261-453d-9618-a13866b91e02 container dapi-container: <nil>
STEP: delete the pod
Sep  6 12:01:32.764: INFO: Waiting for pod downward-api-aa4dddd9-c261-453d-9618-a13866b91e02 to disappear
Sep  6 12:01:32.769: INFO: Pod downward-api-aa4dddd9-c261-453d-9618-a13866b91e02 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:01:32.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1221" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":265,"skipped":4514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:01:32.778: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Sep  6 12:01:32.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-1237 api-versions'
Sep  6 12:01:32.857: INFO: stderr: ""
Sep  6 12:01:32.857: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1alpha1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1alpha1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1alpha1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:01:32.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1237" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":266,"skipped":4551,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:01:32.868: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 12:01:33.300: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 12:01:36.318: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 12:01:36.322: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4261-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:01:37.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7957" for this suite.
STEP: Destroying namespace "webhook-7957-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":267,"skipped":4561,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:01:37.467: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-96f6720e-7ce3-48dc-bdd6-8fa8390738b8
STEP: Creating a pod to test consume configMaps
Sep  6 12:01:37.551: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-76775463-bf85-4587-8084-7177cf7cdccd" in namespace "projected-3756" to be "Succeeded or Failed"
Sep  6 12:01:37.558: INFO: Pod "pod-projected-configmaps-76775463-bf85-4587-8084-7177cf7cdccd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.162193ms
Sep  6 12:01:39.562: INFO: Pod "pod-projected-configmaps-76775463-bf85-4587-8084-7177cf7cdccd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011224817s
STEP: Saw pod success
Sep  6 12:01:39.562: INFO: Pod "pod-projected-configmaps-76775463-bf85-4587-8084-7177cf7cdccd" satisfied condition "Succeeded or Failed"
Sep  6 12:01:39.565: INFO: Trying to get logs from node vm114011 pod pod-projected-configmaps-76775463-bf85-4587-8084-7177cf7cdccd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 12:01:39.583: INFO: Waiting for pod pod-projected-configmaps-76775463-bf85-4587-8084-7177cf7cdccd to disappear
Sep  6 12:01:39.586: INFO: Pod pod-projected-configmaps-76775463-bf85-4587-8084-7177cf7cdccd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:01:39.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3756" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":268,"skipped":4568,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:01:39.595: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-94b89620-9f2b-472d-a374-ffb993084094
STEP: Creating a pod to test consume configMaps
Sep  6 12:01:39.638: INFO: Waiting up to 5m0s for pod "pod-configmaps-f0c64282-d44c-452a-83be-392a80215134" in namespace "configmap-6525" to be "Succeeded or Failed"
Sep  6 12:01:39.650: INFO: Pod "pod-configmaps-f0c64282-d44c-452a-83be-392a80215134": Phase="Pending", Reason="", readiness=false. Elapsed: 12.346991ms
Sep  6 12:01:41.654: INFO: Pod "pod-configmaps-f0c64282-d44c-452a-83be-392a80215134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016058867s
STEP: Saw pod success
Sep  6 12:01:41.654: INFO: Pod "pod-configmaps-f0c64282-d44c-452a-83be-392a80215134" satisfied condition "Succeeded or Failed"
Sep  6 12:01:41.656: INFO: Trying to get logs from node vm114011 pod pod-configmaps-f0c64282-d44c-452a-83be-392a80215134 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 12:01:41.674: INFO: Waiting for pod pod-configmaps-f0c64282-d44c-452a-83be-392a80215134 to disappear
Sep  6 12:01:41.676: INFO: Pod pod-configmaps-f0c64282-d44c-452a-83be-392a80215134 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:01:41.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6525" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":269,"skipped":4580,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:01:41.685: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-0b914c3d-efbc-4e37-8d01-f9d3b5a334c8
Sep  6 12:01:41.712: INFO: Pod name my-hostname-basic-0b914c3d-efbc-4e37-8d01-f9d3b5a334c8: Found 0 pods out of 1
Sep  6 12:01:46.715: INFO: Pod name my-hostname-basic-0b914c3d-efbc-4e37-8d01-f9d3b5a334c8: Found 1 pods out of 1
Sep  6 12:01:46.715: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0b914c3d-efbc-4e37-8d01-f9d3b5a334c8" are running
Sep  6 12:01:46.719: INFO: Pod "my-hostname-basic-0b914c3d-efbc-4e37-8d01-f9d3b5a334c8-96895" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-06 12:01:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-06 12:01:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-06 12:01:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-09-06 12:01:41 +0000 UTC Reason: Message:}])
Sep  6 12:01:46.720: INFO: Trying to dial the pod
Sep  6 12:01:51.733: INFO: Controller my-hostname-basic-0b914c3d-efbc-4e37-8d01-f9d3b5a334c8: Got expected result from replica 1 [my-hostname-basic-0b914c3d-efbc-4e37-8d01-f9d3b5a334c8-96895]: "my-hostname-basic-0b914c3d-efbc-4e37-8d01-f9d3b5a334c8-96895", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:01:51.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3644" for this suite.

• [SLOW TEST:10.058 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":270,"skipped":4582,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:01:51.743: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Sep  6 12:01:51.773: INFO: Waiting up to 5m0s for pod "client-containers-7edccc49-970d-4465-b9fe-ed7f779b7385" in namespace "containers-1875" to be "Succeeded or Failed"
Sep  6 12:01:51.775: INFO: Pod "client-containers-7edccc49-970d-4465-b9fe-ed7f779b7385": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047124ms
Sep  6 12:01:53.778: INFO: Pod "client-containers-7edccc49-970d-4465-b9fe-ed7f779b7385": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005578518s
STEP: Saw pod success
Sep  6 12:01:53.778: INFO: Pod "client-containers-7edccc49-970d-4465-b9fe-ed7f779b7385" satisfied condition "Succeeded or Failed"
Sep  6 12:01:53.783: INFO: Trying to get logs from node vm114012 pod client-containers-7edccc49-970d-4465-b9fe-ed7f779b7385 container test-container: <nil>
STEP: delete the pod
Sep  6 12:01:53.805: INFO: Waiting for pod client-containers-7edccc49-970d-4465-b9fe-ed7f779b7385 to disappear
Sep  6 12:01:53.807: INFO: Pod client-containers-7edccc49-970d-4465-b9fe-ed7f779b7385 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:01:53.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1875" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":271,"skipped":4607,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:01:53.818: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-174beada-7101-44c1-abc8-a616398381f4
STEP: Creating a pod to test consume configMaps
Sep  6 12:01:53.852: INFO: Waiting up to 5m0s for pod "pod-configmaps-5fe4ce4e-bbdc-4a8d-9948-eb375cbd312e" in namespace "configmap-8082" to be "Succeeded or Failed"
Sep  6 12:01:53.858: INFO: Pod "pod-configmaps-5fe4ce4e-bbdc-4a8d-9948-eb375cbd312e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.68456ms
Sep  6 12:01:55.862: INFO: Pod "pod-configmaps-5fe4ce4e-bbdc-4a8d-9948-eb375cbd312e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01000975s
Sep  6 12:01:57.867: INFO: Pod "pod-configmaps-5fe4ce4e-bbdc-4a8d-9948-eb375cbd312e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014298957s
STEP: Saw pod success
Sep  6 12:01:57.867: INFO: Pod "pod-configmaps-5fe4ce4e-bbdc-4a8d-9948-eb375cbd312e" satisfied condition "Succeeded or Failed"
Sep  6 12:01:57.871: INFO: Trying to get logs from node vm114012 pod pod-configmaps-5fe4ce4e-bbdc-4a8d-9948-eb375cbd312e container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 12:01:57.888: INFO: Waiting for pod pod-configmaps-5fe4ce4e-bbdc-4a8d-9948-eb375cbd312e to disappear
Sep  6 12:01:57.898: INFO: Pod pod-configmaps-5fe4ce4e-bbdc-4a8d-9948-eb375cbd312e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:01:57.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8082" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":272,"skipped":4619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:01:57.909: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep  6 12:01:57.938: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 12:01:57.945: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 12:01:57.947: INFO: 
Logging pods the apiserver thinks is on node vm114011 before test
Sep  6 12:01:57.950: INFO: kube-flannel-ds-7kcnr from kube-system started at 2022-09-06 11:00:41 +0000 UTC (1 container statuses recorded)
Sep  6 12:01:57.950: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 12:01:57.950: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-9wpsp from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 12:01:57.950: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 12:01:57.950: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 12:01:57.950: INFO: 
Logging pods the apiserver thinks is on node vm114012 before test
Sep  6 12:01:57.973: INFO: kube-flannel-ds-8kz6j from kube-system started at 2022-09-06 10:12:05 +0000 UTC (1 container statuses recorded)
Sep  6 12:01:57.973: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 12:01:57.973: INFO: metrics-server-6f58bc76cc-dv8ws from kube-system started at 2022-09-06 10:13:09 +0000 UTC (1 container statuses recorded)
Sep  6 12:01:57.973: INFO: 	Container metrics-server ready: true, restart count 0
Sep  6 12:01:57.973: INFO: sonobuoy from sonobuoy started at 2022-09-06 10:14:15 +0000 UTC (1 container statuses recorded)
Sep  6 12:01:57.973: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 12:01:57.973: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-tdhsh from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 12:01:57.973: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 12:01:57.973: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 12:01:57.973: INFO: 
Logging pods the apiserver thinks is on node vm114013 before test
Sep  6 12:01:57.983: INFO: coredns-588b5cd46d-jcnzp from kube-system started at 2022-09-06 10:13:07 +0000 UTC (1 container statuses recorded)
Sep  6 12:01:57.984: INFO: 	Container coredns ready: true, restart count 0
Sep  6 12:01:57.984: INFO: kube-flannel-ds-jxdkc from kube-system started at 2022-09-06 10:12:05 +0000 UTC (1 container statuses recorded)
Sep  6 12:01:57.984: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 12:01:57.984: INFO: metrics-server-6f58bc76cc-hppkr from kube-system started at 2022-09-06 10:13:09 +0000 UTC (1 container statuses recorded)
Sep  6 12:01:57.984: INFO: 	Container metrics-server ready: true, restart count 0
Sep  6 12:01:57.984: INFO: sonobuoy-e2e-job-8d290fb976034269 from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 12:01:57.984: INFO: 	Container e2e ready: true, restart count 0
Sep  6 12:01:57.984: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 12:01:57.984: INFO: sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-lb9kk from sonobuoy started at 2022-09-06 10:14:41 +0000 UTC (2 container statuses recorded)
Sep  6 12:01:57.984: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 12:01:57.984: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node vm114011
STEP: verifying the node has the label node vm114012
STEP: verifying the node has the label node vm114013
Sep  6 12:01:58.045: INFO: Pod coredns-588b5cd46d-jcnzp requesting resource cpu=100m on Node vm114013
Sep  6 12:01:58.045: INFO: Pod kube-flannel-ds-7kcnr requesting resource cpu=100m on Node vm114011
Sep  6 12:01:58.045: INFO: Pod kube-flannel-ds-8kz6j requesting resource cpu=100m on Node vm114012
Sep  6 12:01:58.045: INFO: Pod kube-flannel-ds-jxdkc requesting resource cpu=100m on Node vm114013
Sep  6 12:01:58.045: INFO: Pod metrics-server-6f58bc76cc-dv8ws requesting resource cpu=100m on Node vm114012
Sep  6 12:01:58.045: INFO: Pod metrics-server-6f58bc76cc-hppkr requesting resource cpu=100m on Node vm114013
Sep  6 12:01:58.045: INFO: Pod sonobuoy requesting resource cpu=0m on Node vm114012
Sep  6 12:01:58.045: INFO: Pod sonobuoy-e2e-job-8d290fb976034269 requesting resource cpu=0m on Node vm114013
Sep  6 12:01:58.045: INFO: Pod sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-9wpsp requesting resource cpu=0m on Node vm114011
Sep  6 12:01:58.045: INFO: Pod sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-lb9kk requesting resource cpu=0m on Node vm114013
Sep  6 12:01:58.045: INFO: Pod sonobuoy-systemd-logs-daemon-set-8211d454e8a84da5-tdhsh requesting resource cpu=0m on Node vm114012
STEP: Starting Pods to consume most of the cluster CPU.
Sep  6 12:01:58.045: INFO: Creating a pod which consumes cpu=1050m on Node vm114013
Sep  6 12:01:58.053: INFO: Creating a pod which consumes cpu=980m on Node vm114011
Sep  6 12:01:58.063: INFO: Creating a pod which consumes cpu=1120m on Node vm114012
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0b72bc15-8495-4d16-acaa-f1adc73171d6.171243c3311524c5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8683/filler-pod-0b72bc15-8495-4d16-acaa-f1adc73171d6 to vm114011]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0b72bc15-8495-4d16-acaa-f1adc73171d6.171243c353e4dcd6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0b72bc15-8495-4d16-acaa-f1adc73171d6.171243c35664c0f0], Reason = [Created], Message = [Created container filler-pod-0b72bc15-8495-4d16-acaa-f1adc73171d6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0b72bc15-8495-4d16-acaa-f1adc73171d6.171243c35c726793], Reason = [Started], Message = [Started container filler-pod-0b72bc15-8495-4d16-acaa-f1adc73171d6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-395da675-b788-44ec-b497-b6cf8b7e0c95.171243c330ade115], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8683/filler-pod-395da675-b788-44ec-b497-b6cf8b7e0c95 to vm114013]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-395da675-b788-44ec-b497-b6cf8b7e0c95.171243c37421979c], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "default-token-9wgth" : failed to sync secret cache: timed out waiting for the condition]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-395da675-b788-44ec-b497-b6cf8b7e0c95.171243c3aca5b228], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-395da675-b788-44ec-b497-b6cf8b7e0c95.171243c3aee6eb9a], Reason = [Created], Message = [Created container filler-pod-395da675-b788-44ec-b497-b6cf8b7e0c95]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-395da675-b788-44ec-b497-b6cf8b7e0c95.171243c3b483cd03], Reason = [Started], Message = [Started container filler-pod-395da675-b788-44ec-b497-b6cf8b7e0c95]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90d52a96-b50e-4a3e-8c3d-10ada5000794.171243c332ad4afc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8683/filler-pod-90d52a96-b50e-4a3e-8c3d-10ada5000794 to vm114012]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-90d52a96-b50e-4a3e-8c3d-10ada5000794.171243c376eee3ea], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "default-token-9wgth" : failed to sync secret cache: timed out waiting for the condition]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90d52a96-b50e-4a3e-8c3d-10ada5000794.171243c3ad7331ad], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90d52a96-b50e-4a3e-8c3d-10ada5000794.171243c3af65648a], Reason = [Created], Message = [Created container filler-pod-90d52a96-b50e-4a3e-8c3d-10ada5000794]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-90d52a96-b50e-4a3e-8c3d-10ada5000794.171243c3b4f079c2], Reason = [Started], Message = [Started container filler-pod-90d52a96-b50e-4a3e-8c3d-10ada5000794]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.171243c4212320cd], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.171243c421915062], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node vm114011
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node vm114012
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node vm114013
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:02:03.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8683" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.251 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":273,"skipped":4651,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:02:03.160: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep  6 12:02:03.188: INFO: PodSpec: initContainers in spec.initContainers
Sep  6 12:02:42.067: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2bd64990-d5a8-4376-acd0-8afbad253a57", GenerateName:"", Namespace:"init-container-1340", SelfLink:"/api/v1/namespaces/init-container-1340/pods/pod-init-2bd64990-d5a8-4376-acd0-8afbad253a57", UID:"59708651-8a32-4eb3-b32d-552d04516e17", ResourceVersion:"33649", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63798062523, loc:(*time.Location)(0x771eac0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"188586536"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0040b5020), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0040b5040)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0040b5060), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0040b5080)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-m7vs9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00725d640), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-m7vs9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-m7vs9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-m7vs9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0052533c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"vm114011", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0032d82a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005253450)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005253470)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005253478), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00525347c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002772a40), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798062523, loc:(*time.Location)(0x771eac0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798062523, loc:(*time.Location)(0x771eac0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798062523, loc:(*time.Location)(0x771eac0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798062523, loc:(*time.Location)(0x771eac0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.114.11", PodIP:"10.244.0.210", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.0.210"}}, StartTime:(*v1.Time)(0xc0040b50a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0032d83f0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0032d8460)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://758dda3f30582e6db53507e5fed77f4d33bbbca69cf722c921198d7519fd5265", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0040b50e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0040b50c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0052534ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:02:42.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1340" for this suite.

• [SLOW TEST:38.920 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":274,"skipped":4655,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:02:42.081: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-8844f9eb-4f9f-4645-b105-4b02f1a32486
STEP: Creating a pod to test consume secrets
Sep  6 12:02:42.113: INFO: Waiting up to 5m0s for pod "pod-secrets-59644e11-15aa-489a-835c-0a31290413ea" in namespace "secrets-7251" to be "Succeeded or Failed"
Sep  6 12:02:42.118: INFO: Pod "pod-secrets-59644e11-15aa-489a-835c-0a31290413ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.512715ms
Sep  6 12:02:44.121: INFO: Pod "pod-secrets-59644e11-15aa-489a-835c-0a31290413ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00822455s
STEP: Saw pod success
Sep  6 12:02:44.121: INFO: Pod "pod-secrets-59644e11-15aa-489a-835c-0a31290413ea" satisfied condition "Succeeded or Failed"
Sep  6 12:02:44.130: INFO: Trying to get logs from node vm114012 pod pod-secrets-59644e11-15aa-489a-835c-0a31290413ea container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 12:02:44.143: INFO: Waiting for pod pod-secrets-59644e11-15aa-489a-835c-0a31290413ea to disappear
Sep  6 12:02:44.146: INFO: Pod pod-secrets-59644e11-15aa-489a-835c-0a31290413ea no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:02:44.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7251" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":275,"skipped":4660,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:02:44.154: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0906 12:02:44.750899      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0906 12:02:44.751026      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0906 12:02:44.751040      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep  6 12:02:44.751: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:02:44.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3649" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":276,"skipped":4670,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:02:44.770: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep  6 12:02:44.794: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:02:48.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6189" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":277,"skipped":4687,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:02:48.143: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 12:02:48.176: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-81b7561f-d2bd-4768-adfb-f3e04eae76a9" in namespace "security-context-test-7382" to be "Succeeded or Failed"
Sep  6 12:02:48.180: INFO: Pod "busybox-privileged-false-81b7561f-d2bd-4768-adfb-f3e04eae76a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.850941ms
Sep  6 12:02:50.184: INFO: Pod "busybox-privileged-false-81b7561f-d2bd-4768-adfb-f3e04eae76a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006723107s
Sep  6 12:02:50.184: INFO: Pod "busybox-privileged-false-81b7561f-d2bd-4768-adfb-f3e04eae76a9" satisfied condition "Succeeded or Failed"
Sep  6 12:02:50.191: INFO: Got logs for pod "busybox-privileged-false-81b7561f-d2bd-4768-adfb-f3e04eae76a9": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:02:50.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7382" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":278,"skipped":4723,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:02:50.201: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Sep  6 12:02:50.237: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  6 12:03:50.255: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Sep  6 12:03:50.279: INFO: Created pod: pod0-sched-preemption-low-priority
Sep  6 12:03:50.298: INFO: Created pod: pod1-sched-preemption-medium-priority
Sep  6 12:03:50.320: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:04:02.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9416" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:72.208 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":279,"skipped":4744,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:04:02.415: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:04:02.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3792" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":280,"skipped":4748,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:04:02.500: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 12:04:02.784: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  6 12:04:04.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798062642, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798062642, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63798062642, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63798062642, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 12:04:07.806: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 12:04:07.817: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8836-crds.webhook.example.com via the AdmissionRegistration API
Sep  6 12:04:08.424: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:04:09.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2856" for this suite.
STEP: Destroying namespace "webhook-2856-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.715 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":281,"skipped":4773,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:04:09.216: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 12:04:09.253: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:04:09.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5760" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":282,"skipped":4786,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:04:09.825: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 12:04:09.866: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3af521a-8499-43d4-9281-a8e3cd875428" in namespace "downward-api-3294" to be "Succeeded or Failed"
Sep  6 12:04:09.869: INFO: Pod "downwardapi-volume-a3af521a-8499-43d4-9281-a8e3cd875428": Phase="Pending", Reason="", readiness=false. Elapsed: 3.063212ms
Sep  6 12:04:11.873: INFO: Pod "downwardapi-volume-a3af521a-8499-43d4-9281-a8e3cd875428": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006533099s
STEP: Saw pod success
Sep  6 12:04:11.873: INFO: Pod "downwardapi-volume-a3af521a-8499-43d4-9281-a8e3cd875428" satisfied condition "Succeeded or Failed"
Sep  6 12:04:11.878: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-a3af521a-8499-43d4-9281-a8e3cd875428 container client-container: <nil>
STEP: delete the pod
Sep  6 12:04:11.902: INFO: Waiting for pod downwardapi-volume-a3af521a-8499-43d4-9281-a8e3cd875428 to disappear
Sep  6 12:04:11.906: INFO: Pod downwardapi-volume-a3af521a-8499-43d4-9281-a8e3cd875428 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:04:11.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3294" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":283,"skipped":4796,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:04:11.914: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 12:04:12.051: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3627c623-997d-4860-a6d2-86ce076156ef", Controller:(*bool)(0xc0043892da), BlockOwnerDeletion:(*bool)(0xc0043892db)}}
Sep  6 12:04:12.073: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"43b02a30-aef4-469d-9e54-db240ea52c0a", Controller:(*bool)(0xc00298c8ce), BlockOwnerDeletion:(*bool)(0xc00298c8cf)}}
Sep  6 12:04:12.089: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a4b58cd9-1fce-46e1-8963-b6e71b8d5e99", Controller:(*bool)(0xc00298cb06), BlockOwnerDeletion:(*bool)(0xc00298cb07)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:04:17.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8607" for this suite.

• [SLOW TEST:5.195 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":284,"skipped":4807,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:04:17.111: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:04:32.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-432" for this suite.
STEP: Destroying namespace "nsdeletetest-9883" for this suite.
Sep  6 12:04:32.222: INFO: Namespace nsdeletetest-9883 was already deleted
STEP: Destroying namespace "nsdeletetest-4638" for this suite.

• [SLOW TEST:15.114 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":285,"skipped":4808,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:04:32.226: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 12:04:32.276: INFO: Number of nodes with available pods: 0
Sep  6 12:04:32.276: INFO: Node vm114011 is running more than one daemon pod
Sep  6 12:04:33.283: INFO: Number of nodes with available pods: 0
Sep  6 12:04:33.283: INFO: Node vm114011 is running more than one daemon pod
Sep  6 12:04:34.285: INFO: Number of nodes with available pods: 2
Sep  6 12:04:34.285: INFO: Node vm114011 is running more than one daemon pod
Sep  6 12:04:35.283: INFO: Number of nodes with available pods: 3
Sep  6 12:04:35.283: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep  6 12:04:35.301: INFO: Number of nodes with available pods: 3
Sep  6 12:04:35.301: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-813, will wait for the garbage collector to delete the pods
Sep  6 12:04:36.375: INFO: Deleting DaemonSet.extensions daemon-set took: 7.606751ms
Sep  6 12:04:36.475: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.402787ms
Sep  6 12:05:57.880: INFO: Number of nodes with available pods: 0
Sep  6 12:05:57.880: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 12:05:57.891: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-813/daemonsets","resourceVersion":"34665"},"items":null}

Sep  6 12:05:57.895: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-813/pods","resourceVersion":"34665"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:05:57.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-813" for this suite.

• [SLOW TEST:85.686 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":286,"skipped":4826,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:05:57.912: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4736.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4736.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 12:05:59.967: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:05:59.970: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:05:59.978: INFO: Unable to read jessie_udp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:05:59.981: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:05:59.981: INFO: Lookups using dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 12:06:04.991: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:04.995: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:05.006: INFO: Unable to read jessie_udp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:05.009: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:05.009: INFO: Lookups using dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 12:06:09.991: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:09.997: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:10.006: INFO: Unable to read jessie_udp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:10.009: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:10.009: INFO: Lookups using dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 12:06:14.991: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:14.995: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:15.004: INFO: Unable to read jessie_udp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:15.008: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:15.008: INFO: Lookups using dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 12:06:19.992: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:19.996: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:20.005: INFO: Unable to read jessie_udp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:20.008: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:20.008: INFO: Lookups using dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 12:06:24.995: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:24.998: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:25.007: INFO: Unable to read jessie_udp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:25.010: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de: the server could not find the requested resource (get pods dns-test-7be29bde-6849-4477-b231-9cf67540d1de)
Sep  6 12:06:25.011: INFO: Lookups using dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 12:06:30.010: INFO: DNS probes using dns-4736/dns-test-7be29bde-6849-4477-b231-9cf67540d1de succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:06:30.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4736" for this suite.

• [SLOW TEST:32.127 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":287,"skipped":4830,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:06:30.041: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-w6p8g in namespace proxy-9221
I0906 12:06:30.102162      20 runners.go:190] Created replication controller with name: proxy-service-w6p8g, namespace: proxy-9221, replica count: 1
I0906 12:06:31.152959      20 runners.go:190] proxy-service-w6p8g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 12:06:32.153645      20 runners.go:190] proxy-service-w6p8g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 12:06:33.154151      20 runners.go:190] proxy-service-w6p8g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 12:06:34.154847      20 runners.go:190] proxy-service-w6p8g Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 12:06:34.158: INFO: setup took 4.078305368s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep  6 12:06:34.172: INFO: (0) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 13.135325ms)
Sep  6 12:06:34.172: INFO: (0) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 13.559064ms)
Sep  6 12:06:34.172: INFO: (0) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 13.636127ms)
Sep  6 12:06:34.172: INFO: (0) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 13.409656ms)
Sep  6 12:06:34.172: INFO: (0) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 13.41369ms)
Sep  6 12:06:34.175: INFO: (0) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 16.198403ms)
Sep  6 12:06:34.176: INFO: (0) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 17.305616ms)
Sep  6 12:06:34.176: INFO: (0) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 17.518412ms)
Sep  6 12:06:34.177: INFO: (0) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 18.718658ms)
Sep  6 12:06:34.177: INFO: (0) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 18.620085ms)
Sep  6 12:06:34.177: INFO: (0) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 18.790713ms)
Sep  6 12:06:34.177: INFO: (0) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 18.348097ms)
Sep  6 12:06:34.177: INFO: (0) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 18.380468ms)
Sep  6 12:06:34.178: INFO: (0) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 19.57294ms)
Sep  6 12:06:34.180: INFO: (0) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 20.595975ms)
Sep  6 12:06:34.181: INFO: (0) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 21.75822ms)
Sep  6 12:06:34.193: INFO: (1) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 11.255599ms)
Sep  6 12:06:34.193: INFO: (1) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 12.306597ms)
Sep  6 12:06:34.197: INFO: (1) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 15.573782ms)
Sep  6 12:06:34.197: INFO: (1) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 15.838973ms)
Sep  6 12:06:34.198: INFO: (1) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 16.188017ms)
Sep  6 12:06:34.198: INFO: (1) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 16.056419ms)
Sep  6 12:06:34.198: INFO: (1) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 16.2597ms)
Sep  6 12:06:34.198: INFO: (1) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 16.374586ms)
Sep  6 12:06:34.198: INFO: (1) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 16.587202ms)
Sep  6 12:06:34.198: INFO: (1) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 17.762883ms)
Sep  6 12:06:34.199: INFO: (1) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 16.821527ms)
Sep  6 12:06:34.201: INFO: (1) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 18.934002ms)
Sep  6 12:06:34.201: INFO: (1) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 19.166115ms)
Sep  6 12:06:34.201: INFO: (1) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 19.146949ms)
Sep  6 12:06:34.201: INFO: (1) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 19.545281ms)
Sep  6 12:06:34.201: INFO: (1) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 19.670795ms)
Sep  6 12:06:34.213: INFO: (2) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 10.981641ms)
Sep  6 12:06:34.213: INFO: (2) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 11.048816ms)
Sep  6 12:06:34.214: INFO: (2) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 12.281592ms)
Sep  6 12:06:34.214: INFO: (2) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 11.530729ms)
Sep  6 12:06:34.214: INFO: (2) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 12.485167ms)
Sep  6 12:06:34.215: INFO: (2) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 13.343195ms)
Sep  6 12:06:34.215: INFO: (2) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 13.432556ms)
Sep  6 12:06:34.215: INFO: (2) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 13.297635ms)
Sep  6 12:06:34.215: INFO: (2) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 12.965706ms)
Sep  6 12:06:34.215: INFO: (2) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 13.389806ms)
Sep  6 12:06:34.215: INFO: (2) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 13.393734ms)
Sep  6 12:06:34.215: INFO: (2) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 12.599113ms)
Sep  6 12:06:34.215: INFO: (2) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 13.143263ms)
Sep  6 12:06:34.216: INFO: (2) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 14.302291ms)
Sep  6 12:06:34.216: INFO: (2) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 14.360651ms)
Sep  6 12:06:34.216: INFO: (2) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 14.43586ms)
Sep  6 12:06:34.227: INFO: (3) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 10.210492ms)
Sep  6 12:06:34.228: INFO: (3) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 11.570238ms)
Sep  6 12:06:34.228: INFO: (3) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 11.670031ms)
Sep  6 12:06:34.228: INFO: (3) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 11.933282ms)
Sep  6 12:06:34.230: INFO: (3) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 13.098453ms)
Sep  6 12:06:34.230: INFO: (3) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 13.698074ms)
Sep  6 12:06:34.231: INFO: (3) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 14.13615ms)
Sep  6 12:06:34.231: INFO: (3) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 14.227871ms)
Sep  6 12:06:34.231: INFO: (3) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 14.622296ms)
Sep  6 12:06:34.231: INFO: (3) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 14.088342ms)
Sep  6 12:06:34.231: INFO: (3) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 13.944654ms)
Sep  6 12:06:34.231: INFO: (3) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 14.307279ms)
Sep  6 12:06:34.231: INFO: (3) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 14.411914ms)
Sep  6 12:06:34.232: INFO: (3) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 15.251787ms)
Sep  6 12:06:34.232: INFO: (3) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 15.329382ms)
Sep  6 12:06:34.232: INFO: (3) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 15.308794ms)
Sep  6 12:06:34.240: INFO: (4) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 7.239857ms)
Sep  6 12:06:34.241: INFO: (4) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 8.394571ms)
Sep  6 12:06:34.242: INFO: (4) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 9.289163ms)
Sep  6 12:06:34.248: INFO: (4) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 15.541561ms)
Sep  6 12:06:34.248: INFO: (4) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 15.881437ms)
Sep  6 12:06:34.251: INFO: (4) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 18.123835ms)
Sep  6 12:06:34.251: INFO: (4) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 18.499244ms)
Sep  6 12:06:34.251: INFO: (4) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 18.292058ms)
Sep  6 12:06:34.251: INFO: (4) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 18.273884ms)
Sep  6 12:06:34.252: INFO: (4) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 18.86352ms)
Sep  6 12:06:34.252: INFO: (4) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 18.926129ms)
Sep  6 12:06:34.252: INFO: (4) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 18.82464ms)
Sep  6 12:06:34.252: INFO: (4) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 18.492671ms)
Sep  6 12:06:34.252: INFO: (4) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 18.977014ms)
Sep  6 12:06:34.253: INFO: (4) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 19.535351ms)
Sep  6 12:06:34.253: INFO: (4) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 19.528431ms)
Sep  6 12:06:34.258: INFO: (5) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 5.63188ms)
Sep  6 12:06:34.266: INFO: (5) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 12.980721ms)
Sep  6 12:06:34.266: INFO: (5) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 13.436683ms)
Sep  6 12:06:34.266: INFO: (5) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 13.36676ms)
Sep  6 12:06:34.266: INFO: (5) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 13.273677ms)
Sep  6 12:06:34.267: INFO: (5) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 14.337007ms)
Sep  6 12:06:34.267: INFO: (5) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 14.351514ms)
Sep  6 12:06:34.268: INFO: (5) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 15.536832ms)
Sep  6 12:06:34.268: INFO: (5) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 15.516855ms)
Sep  6 12:06:34.269: INFO: (5) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 15.614066ms)
Sep  6 12:06:34.270: INFO: (5) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 16.902166ms)
Sep  6 12:06:34.270: INFO: (5) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 17.194771ms)
Sep  6 12:06:34.271: INFO: (5) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 17.680376ms)
Sep  6 12:06:34.275: INFO: (5) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 21.915267ms)
Sep  6 12:06:34.275: INFO: (5) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 22.222771ms)
Sep  6 12:06:34.275: INFO: (5) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 22.24926ms)
Sep  6 12:06:34.284: INFO: (6) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 8.346757ms)
Sep  6 12:06:34.284: INFO: (6) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 8.292972ms)
Sep  6 12:06:34.284: INFO: (6) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 8.082409ms)
Sep  6 12:06:34.286: INFO: (6) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 9.635197ms)
Sep  6 12:06:34.286: INFO: (6) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 10.42575ms)
Sep  6 12:06:34.286: INFO: (6) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 10.61348ms)
Sep  6 12:06:34.287: INFO: (6) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 11.47746ms)
Sep  6 12:06:34.288: INFO: (6) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 12.188875ms)
Sep  6 12:06:34.288: INFO: (6) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 11.950564ms)
Sep  6 12:06:34.288: INFO: (6) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 12.012088ms)
Sep  6 12:06:34.288: INFO: (6) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 12.233881ms)
Sep  6 12:06:34.288: INFO: (6) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 12.273175ms)
Sep  6 12:06:34.288: INFO: (6) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 12.221309ms)
Sep  6 12:06:34.289: INFO: (6) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 12.863174ms)
Sep  6 12:06:34.289: INFO: (6) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 13.364695ms)
Sep  6 12:06:34.289: INFO: (6) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 13.315865ms)
Sep  6 12:06:34.300: INFO: (7) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 9.803554ms)
Sep  6 12:06:34.300: INFO: (7) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 10.147785ms)
Sep  6 12:06:34.300: INFO: (7) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 10.131735ms)
Sep  6 12:06:34.302: INFO: (7) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 12.731261ms)
Sep  6 12:06:34.303: INFO: (7) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 13.303429ms)
Sep  6 12:06:34.304: INFO: (7) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 13.656497ms)
Sep  6 12:06:34.304: INFO: (7) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 14.054108ms)
Sep  6 12:06:34.305: INFO: (7) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 14.54737ms)
Sep  6 12:06:34.305: INFO: (7) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 15.19168ms)
Sep  6 12:06:34.305: INFO: (7) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 15.039116ms)
Sep  6 12:06:34.305: INFO: (7) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 14.721745ms)
Sep  6 12:06:34.308: INFO: (7) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 17.597692ms)
Sep  6 12:06:34.309: INFO: (7) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 18.767351ms)
Sep  6 12:06:34.309: INFO: (7) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 18.311413ms)
Sep  6 12:06:34.310: INFO: (7) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 19.304132ms)
Sep  6 12:06:34.310: INFO: (7) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 19.539079ms)
Sep  6 12:06:34.318: INFO: (8) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 7.923813ms)
Sep  6 12:06:34.318: INFO: (8) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 7.669438ms)
Sep  6 12:06:34.318: INFO: (8) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 7.777019ms)
Sep  6 12:06:34.318: INFO: (8) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 7.935684ms)
Sep  6 12:06:34.318: INFO: (8) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 8.111191ms)
Sep  6 12:06:34.320: INFO: (8) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 9.651313ms)
Sep  6 12:06:34.320: INFO: (8) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 9.420702ms)
Sep  6 12:06:34.323: INFO: (8) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 11.885113ms)
Sep  6 12:06:34.323: INFO: (8) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 11.834029ms)
Sep  6 12:06:34.323: INFO: (8) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 11.897147ms)
Sep  6 12:06:34.324: INFO: (8) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 13.587383ms)
Sep  6 12:06:34.324: INFO: (8) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 14.000089ms)
Sep  6 12:06:34.324: INFO: (8) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 13.235786ms)
Sep  6 12:06:34.325: INFO: (8) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 14.381323ms)
Sep  6 12:06:34.326: INFO: (8) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 15.510633ms)
Sep  6 12:06:34.326: INFO: (8) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 15.402903ms)
Sep  6 12:06:34.334: INFO: (9) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 7.537833ms)
Sep  6 12:06:34.337: INFO: (9) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 9.352764ms)
Sep  6 12:06:34.337: INFO: (9) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 9.645289ms)
Sep  6 12:06:34.337: INFO: (9) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 9.964604ms)
Sep  6 12:06:34.337: INFO: (9) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 10.256948ms)
Sep  6 12:06:34.337: INFO: (9) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 9.847696ms)
Sep  6 12:06:34.337: INFO: (9) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 10.784721ms)
Sep  6 12:06:34.337: INFO: (9) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 10.559362ms)
Sep  6 12:06:34.337: INFO: (9) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 9.976296ms)
Sep  6 12:06:34.337: INFO: (9) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 10.127858ms)
Sep  6 12:06:34.337: INFO: (9) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 10.035796ms)
Sep  6 12:06:34.337: INFO: (9) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 10.535257ms)
Sep  6 12:06:34.337: INFO: (9) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 9.512011ms)
Sep  6 12:06:34.338: INFO: (9) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 11.315254ms)
Sep  6 12:06:34.338: INFO: (9) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 11.342554ms)
Sep  6 12:06:34.338: INFO: (9) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 11.745596ms)
Sep  6 12:06:34.350: INFO: (10) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 11.696252ms)
Sep  6 12:06:34.351: INFO: (10) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 12.272826ms)
Sep  6 12:06:34.351: INFO: (10) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 11.521286ms)
Sep  6 12:06:34.351: INFO: (10) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 12.423366ms)
Sep  6 12:06:34.351: INFO: (10) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 11.764058ms)
Sep  6 12:06:34.351: INFO: (10) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 11.948521ms)
Sep  6 12:06:34.352: INFO: (10) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 12.206943ms)
Sep  6 12:06:34.352: INFO: (10) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 12.179923ms)
Sep  6 12:06:34.353: INFO: (10) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 13.508753ms)
Sep  6 12:06:34.354: INFO: (10) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 15.368313ms)
Sep  6 12:06:34.354: INFO: (10) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 16.139419ms)
Sep  6 12:06:34.355: INFO: (10) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 16.482383ms)
Sep  6 12:06:34.356: INFO: (10) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 17.620909ms)
Sep  6 12:06:34.356: INFO: (10) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 17.243532ms)
Sep  6 12:06:34.356: INFO: (10) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 17.134393ms)
Sep  6 12:06:34.356: INFO: (10) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 17.079476ms)
Sep  6 12:06:34.361: INFO: (11) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 4.81742ms)
Sep  6 12:06:34.366: INFO: (11) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 9.77869ms)
Sep  6 12:06:34.369: INFO: (11) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 12.097985ms)
Sep  6 12:06:34.369: INFO: (11) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 12.319834ms)
Sep  6 12:06:34.370: INFO: (11) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 13.354897ms)
Sep  6 12:06:34.370: INFO: (11) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 13.312859ms)
Sep  6 12:06:34.370: INFO: (11) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 13.725903ms)
Sep  6 12:06:34.371: INFO: (11) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 14.815643ms)
Sep  6 12:06:34.371: INFO: (11) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 14.726937ms)
Sep  6 12:06:34.373: INFO: (11) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 15.890041ms)
Sep  6 12:06:34.373: INFO: (11) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 15.956545ms)
Sep  6 12:06:34.373: INFO: (11) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 15.986039ms)
Sep  6 12:06:34.373: INFO: (11) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 16.039151ms)
Sep  6 12:06:34.373: INFO: (11) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 16.074443ms)
Sep  6 12:06:34.374: INFO: (11) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 17.085898ms)
Sep  6 12:06:34.374: INFO: (11) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 17.17007ms)
Sep  6 12:06:34.387: INFO: (12) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 13.636274ms)
Sep  6 12:06:34.388: INFO: (12) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 13.938695ms)
Sep  6 12:06:34.388: INFO: (12) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 14.081071ms)
Sep  6 12:06:34.388: INFO: (12) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 14.245186ms)
Sep  6 12:06:34.390: INFO: (12) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 15.980942ms)
Sep  6 12:06:34.391: INFO: (12) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 17.420373ms)
Sep  6 12:06:34.391: INFO: (12) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 17.496394ms)
Sep  6 12:06:34.391: INFO: (12) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 17.452963ms)
Sep  6 12:06:34.391: INFO: (12) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 17.58ms)
Sep  6 12:06:34.392: INFO: (12) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 17.787486ms)
Sep  6 12:06:34.394: INFO: (12) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 19.687246ms)
Sep  6 12:06:34.394: INFO: (12) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 20.199437ms)
Sep  6 12:06:34.394: INFO: (12) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 20.253106ms)
Sep  6 12:06:34.394: INFO: (12) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 20.288783ms)
Sep  6 12:06:34.394: INFO: (12) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 20.319904ms)
Sep  6 12:06:34.394: INFO: (12) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 20.301687ms)
Sep  6 12:06:34.406: INFO: (13) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 11.859594ms)
Sep  6 12:06:34.406: INFO: (13) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 11.330932ms)
Sep  6 12:06:34.407: INFO: (13) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 11.992036ms)
Sep  6 12:06:34.407: INFO: (13) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 11.910179ms)
Sep  6 12:06:34.407: INFO: (13) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 12.417183ms)
Sep  6 12:06:34.407: INFO: (13) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 11.54901ms)
Sep  6 12:06:34.407: INFO: (13) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 11.850507ms)
Sep  6 12:06:34.407: INFO: (13) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 12.077781ms)
Sep  6 12:06:34.407: INFO: (13) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 12.507732ms)
Sep  6 12:06:34.407: INFO: (13) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 12.705626ms)
Sep  6 12:06:34.407: INFO: (13) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 12.306542ms)
Sep  6 12:06:34.409: INFO: (13) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 13.859455ms)
Sep  6 12:06:34.411: INFO: (13) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 15.439296ms)
Sep  6 12:06:34.411: INFO: (13) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 16.002545ms)
Sep  6 12:06:34.411: INFO: (13) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 15.952252ms)
Sep  6 12:06:34.412: INFO: (13) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 16.362867ms)
Sep  6 12:06:34.417: INFO: (14) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 4.588585ms)
Sep  6 12:06:34.420: INFO: (14) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 7.88479ms)
Sep  6 12:06:34.420: INFO: (14) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 8.018429ms)
Sep  6 12:06:34.425: INFO: (14) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 12.83917ms)
Sep  6 12:06:34.425: INFO: (14) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 12.992532ms)
Sep  6 12:06:34.425: INFO: (14) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 12.878139ms)
Sep  6 12:06:34.425: INFO: (14) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 13.039008ms)
Sep  6 12:06:34.425: INFO: (14) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 13.248222ms)
Sep  6 12:06:34.425: INFO: (14) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 13.348646ms)
Sep  6 12:06:34.425: INFO: (14) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 13.272704ms)
Sep  6 12:06:34.426: INFO: (14) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 13.680497ms)
Sep  6 12:06:34.426: INFO: (14) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 14.195583ms)
Sep  6 12:06:34.426: INFO: (14) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 14.088938ms)
Sep  6 12:06:34.426: INFO: (14) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 14.201839ms)
Sep  6 12:06:34.427: INFO: (14) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 14.923573ms)
Sep  6 12:06:34.430: INFO: (14) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 18.315468ms)
Sep  6 12:06:34.443: INFO: (15) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 12.130476ms)
Sep  6 12:06:34.443: INFO: (15) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 12.078504ms)
Sep  6 12:06:34.443: INFO: (15) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 12.223758ms)
Sep  6 12:06:34.443: INFO: (15) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 12.368895ms)
Sep  6 12:06:34.443: INFO: (15) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 12.37173ms)
Sep  6 12:06:34.443: INFO: (15) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 12.273757ms)
Sep  6 12:06:34.443: INFO: (15) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 12.461017ms)
Sep  6 12:06:34.443: INFO: (15) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 12.35565ms)
Sep  6 12:06:34.443: INFO: (15) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 12.390225ms)
Sep  6 12:06:34.443: INFO: (15) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 12.480803ms)
Sep  6 12:06:34.443: INFO: (15) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 12.411844ms)
Sep  6 12:06:34.443: INFO: (15) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 12.808674ms)
Sep  6 12:06:34.444: INFO: (15) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 12.902965ms)
Sep  6 12:06:34.444: INFO: (15) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 13.731311ms)
Sep  6 12:06:34.444: INFO: (15) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 13.737222ms)
Sep  6 12:06:34.451: INFO: (15) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 20.056932ms)
Sep  6 12:06:34.468: INFO: (16) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 16.227282ms)
Sep  6 12:06:34.468: INFO: (16) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 16.784532ms)
Sep  6 12:06:34.468: INFO: (16) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 16.598534ms)
Sep  6 12:06:34.469: INFO: (16) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 17.476331ms)
Sep  6 12:06:34.469: INFO: (16) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 16.841229ms)
Sep  6 12:06:34.469: INFO: (16) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 17.065636ms)
Sep  6 12:06:34.469: INFO: (16) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 17.511948ms)
Sep  6 12:06:34.469: INFO: (16) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 17.698643ms)
Sep  6 12:06:34.469: INFO: (16) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 18.026787ms)
Sep  6 12:06:34.469: INFO: (16) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 16.876891ms)
Sep  6 12:06:34.469: INFO: (16) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 17.093044ms)
Sep  6 12:06:34.469: INFO: (16) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 17.497418ms)
Sep  6 12:06:34.469: INFO: (16) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 17.228246ms)
Sep  6 12:06:34.469: INFO: (16) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 17.656925ms)
Sep  6 12:06:34.469: INFO: (16) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 17.080294ms)
Sep  6 12:06:34.469: INFO: (16) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 17.933343ms)
Sep  6 12:06:34.477: INFO: (17) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 8.260544ms)
Sep  6 12:06:34.477: INFO: (17) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 8.373431ms)
Sep  6 12:06:34.483: INFO: (17) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 13.870082ms)
Sep  6 12:06:34.483: INFO: (17) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 13.920894ms)
Sep  6 12:06:34.483: INFO: (17) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 14.236383ms)
Sep  6 12:06:34.483: INFO: (17) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 14.24555ms)
Sep  6 12:06:34.484: INFO: (17) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 14.30489ms)
Sep  6 12:06:34.484: INFO: (17) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 14.416823ms)
Sep  6 12:06:34.484: INFO: (17) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 14.434947ms)
Sep  6 12:06:34.484: INFO: (17) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 14.5945ms)
Sep  6 12:06:34.485: INFO: (17) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 15.954388ms)
Sep  6 12:06:34.485: INFO: (17) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 16.083298ms)
Sep  6 12:06:34.485: INFO: (17) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 16.122582ms)
Sep  6 12:06:34.486: INFO: (17) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 16.418382ms)
Sep  6 12:06:34.486: INFO: (17) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 16.380602ms)
Sep  6 12:06:34.486: INFO: (17) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 16.443789ms)
Sep  6 12:06:34.495: INFO: (18) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 9.480237ms)
Sep  6 12:06:34.498: INFO: (18) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 11.970399ms)
Sep  6 12:06:34.498: INFO: (18) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 12.430797ms)
Sep  6 12:06:34.498: INFO: (18) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 12.268845ms)
Sep  6 12:06:34.498: INFO: (18) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 12.27606ms)
Sep  6 12:06:34.499: INFO: (18) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 13.000368ms)
Sep  6 12:06:34.499: INFO: (18) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 13.344739ms)
Sep  6 12:06:34.499: INFO: (18) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 13.466166ms)
Sep  6 12:06:34.499: INFO: (18) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 13.197327ms)
Sep  6 12:06:34.500: INFO: (18) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 13.962552ms)
Sep  6 12:06:34.500: INFO: (18) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 13.644199ms)
Sep  6 12:06:34.500: INFO: (18) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 13.954537ms)
Sep  6 12:06:34.500: INFO: (18) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 13.937636ms)
Sep  6 12:06:34.501: INFO: (18) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 14.822282ms)
Sep  6 12:06:34.502: INFO: (18) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 15.889206ms)
Sep  6 12:06:34.502: INFO: (18) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 15.65521ms)
Sep  6 12:06:34.508: INFO: (19) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname1/proxy/: foo (200; 6.243462ms)
Sep  6 12:06:34.508: INFO: (19) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname1/proxy/: foo (200; 6.487787ms)
Sep  6 12:06:34.509: INFO: (19) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 7.378767ms)
Sep  6 12:06:34.509: INFO: (19) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm/proxy/rewriteme">test</a> (200; 7.371678ms)
Sep  6 12:06:34.515: INFO: (19) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:443/proxy/tlsrewritem... (200; 13.447914ms)
Sep  6 12:06:34.516: INFO: (19) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">test<... (200; 13.681709ms)
Sep  6 12:06:34.516: INFO: (19) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/: <a href="/api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:1080/proxy/rewriteme">... (200; 13.881221ms)
Sep  6 12:06:34.516: INFO: (19) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:462/proxy/: tls qux (200; 13.666751ms)
Sep  6 12:06:34.516: INFO: (19) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:162/proxy/: bar (200; 13.767369ms)
Sep  6 12:06:34.516: INFO: (19) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname1/proxy/: tls baz (200; 13.908541ms)
Sep  6 12:06:34.516: INFO: (19) /api/v1/namespaces/proxy-9221/pods/http:proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 13.724709ms)
Sep  6 12:06:34.516: INFO: (19) /api/v1/namespaces/proxy-9221/services/proxy-service-w6p8g:portname2/proxy/: bar (200; 13.895718ms)
Sep  6 12:06:34.516: INFO: (19) /api/v1/namespaces/proxy-9221/pods/proxy-service-w6p8g-mxtkm:160/proxy/: foo (200; 13.81685ms)
Sep  6 12:06:34.516: INFO: (19) /api/v1/namespaces/proxy-9221/pods/https:proxy-service-w6p8g-mxtkm:460/proxy/: tls baz (200; 13.801492ms)
Sep  6 12:06:34.517: INFO: (19) /api/v1/namespaces/proxy-9221/services/http:proxy-service-w6p8g:portname2/proxy/: bar (200; 14.823476ms)
Sep  6 12:06:34.517: INFO: (19) /api/v1/namespaces/proxy-9221/services/https:proxy-service-w6p8g:tlsportname2/proxy/: tls qux (200; 14.926531ms)
STEP: deleting ReplicationController proxy-service-w6p8g in namespace proxy-9221, will wait for the garbage collector to delete the pods
Sep  6 12:06:34.578: INFO: Deleting ReplicationController proxy-service-w6p8g took: 5.407024ms
Sep  6 12:06:34.679: INFO: Terminating ReplicationController proxy-service-w6p8g pods took: 100.768451ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:06:36.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9221" for this suite.

• [SLOW TEST:6.149 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":288,"skipped":4864,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:06:36.191: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 12:06:36.226: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6853c230-c458-420d-a51f-bec0796bb022" in namespace "downward-api-7452" to be "Succeeded or Failed"
Sep  6 12:06:36.231: INFO: Pod "downwardapi-volume-6853c230-c458-420d-a51f-bec0796bb022": Phase="Pending", Reason="", readiness=false. Elapsed: 4.504512ms
Sep  6 12:06:38.234: INFO: Pod "downwardapi-volume-6853c230-c458-420d-a51f-bec0796bb022": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007795772s
STEP: Saw pod success
Sep  6 12:06:38.234: INFO: Pod "downwardapi-volume-6853c230-c458-420d-a51f-bec0796bb022" satisfied condition "Succeeded or Failed"
Sep  6 12:06:38.238: INFO: Trying to get logs from node vm114011 pod downwardapi-volume-6853c230-c458-420d-a51f-bec0796bb022 container client-container: <nil>
STEP: delete the pod
Sep  6 12:06:38.264: INFO: Waiting for pod downwardapi-volume-6853c230-c458-420d-a51f-bec0796bb022 to disappear
Sep  6 12:06:38.268: INFO: Pod downwardapi-volume-6853c230-c458-420d-a51f-bec0796bb022 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:06:38.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7452" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":289,"skipped":4868,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:06:38.282: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2829.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2829.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2829.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2829.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2829.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2829.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 12:06:40.373: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:40.379: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:40.393: INFO: Unable to read jessie_udp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:40.397: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:40.397: INFO: Lookups using dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 12:06:45.409: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:45.412: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:45.429: INFO: Unable to read jessie_udp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:45.437: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:45.437: INFO: Lookups using dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 12:06:50.408: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:50.412: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:50.421: INFO: Unable to read jessie_udp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:50.423: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:50.423: INFO: Lookups using dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 12:06:55.408: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:55.412: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:55.422: INFO: Unable to read jessie_udp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:55.426: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:06:55.426: INFO: Lookups using dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 12:07:00.407: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:07:00.409: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:07:00.419: INFO: Unable to read jessie_udp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:07:00.422: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:07:00.422: INFO: Lookups using dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 12:07:05.407: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:07:05.410: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:07:05.422: INFO: Unable to read jessie_udp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:07:05.428: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3: the server could not find the requested resource (get pods dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3)
Sep  6 12:07:05.428: INFO: Lookups using dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep  6 12:07:10.420: INFO: DNS probes using dns-2829/dns-test-b58768e1-bbce-4028-9e90-0ff4289225d3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:07:10.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2829" for this suite.

• [SLOW TEST:32.235 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":290,"skipped":4868,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:07:10.517: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 12:07:10.560: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep  6 12:07:13.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-3147 --namespace=crd-publish-openapi-3147 create -f -'
Sep  6 12:07:13.577: INFO: stderr: ""
Sep  6 12:07:13.577: INFO: stdout: "e2e-test-crd-publish-openapi-8381-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  6 12:07:13.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-3147 --namespace=crd-publish-openapi-3147 delete e2e-test-crd-publish-openapi-8381-crds test-cr'
Sep  6 12:07:13.645: INFO: stderr: ""
Sep  6 12:07:13.645: INFO: stdout: "e2e-test-crd-publish-openapi-8381-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep  6 12:07:13.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-3147 --namespace=crd-publish-openapi-3147 apply -f -'
Sep  6 12:07:13.790: INFO: stderr: ""
Sep  6 12:07:13.790: INFO: stdout: "e2e-test-crd-publish-openapi-8381-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  6 12:07:13.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-3147 --namespace=crd-publish-openapi-3147 delete e2e-test-crd-publish-openapi-8381-crds test-cr'
Sep  6 12:07:13.864: INFO: stderr: ""
Sep  6 12:07:13.864: INFO: stdout: "e2e-test-crd-publish-openapi-8381-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep  6 12:07:13.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=crd-publish-openapi-3147 explain e2e-test-crd-publish-openapi-8381-crds'
Sep  6 12:07:14.011: INFO: stderr: ""
Sep  6 12:07:14.011: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8381-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:07:15.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3147" for this suite.

• [SLOW TEST:5.229 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":291,"skipped":4871,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:07:15.747: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  6 12:07:15.776: INFO: Waiting up to 5m0s for pod "pod-35255bfd-6c0b-4edd-bc2a-394be74b1733" in namespace "emptydir-7939" to be "Succeeded or Failed"
Sep  6 12:07:15.782: INFO: Pod "pod-35255bfd-6c0b-4edd-bc2a-394be74b1733": Phase="Pending", Reason="", readiness=false. Elapsed: 6.656141ms
Sep  6 12:07:17.786: INFO: Pod "pod-35255bfd-6c0b-4edd-bc2a-394be74b1733": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010130836s
STEP: Saw pod success
Sep  6 12:07:17.786: INFO: Pod "pod-35255bfd-6c0b-4edd-bc2a-394be74b1733" satisfied condition "Succeeded or Failed"
Sep  6 12:07:17.790: INFO: Trying to get logs from node vm114011 pod pod-35255bfd-6c0b-4edd-bc2a-394be74b1733 container test-container: <nil>
STEP: delete the pod
Sep  6 12:07:17.805: INFO: Waiting for pod pod-35255bfd-6c0b-4edd-bc2a-394be74b1733 to disappear
Sep  6 12:07:17.810: INFO: Pod pod-35255bfd-6c0b-4edd-bc2a-394be74b1733 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:07:17.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7939" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":292,"skipped":4874,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:07:17.820: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep  6 12:07:20.389: INFO: Successfully updated pod "annotationupdate7ff2d36a-0b35-4cb6-87f1-66c2104b2f80"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:07:24.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8018" for this suite.

• [SLOW TEST:6.597 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":293,"skipped":4893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:07:24.419: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-dde91e54-7498-4a5a-bcfa-d85c58b187e8
STEP: Creating secret with name s-test-opt-upd-0492af78-80a6-4a63-adbe-b0591f223413
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-dde91e54-7498-4a5a-bcfa-d85c58b187e8
STEP: Updating secret s-test-opt-upd-0492af78-80a6-4a63-adbe-b0591f223413
STEP: Creating secret with name s-test-opt-create-c07ab3ed-e370-4281-a991-0e7ef5b59929
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:07:28.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6020" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":294,"skipped":4945,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:07:28.540: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 12:07:28.563: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:07:29.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3295" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":295,"skipped":4989,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:07:29.677: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 12:07:29.727: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 12:07:29.758: INFO: Number of nodes with available pods: 0
Sep  6 12:07:29.758: INFO: Node vm114011 is running more than one daemon pod
Sep  6 12:07:30.765: INFO: Number of nodes with available pods: 1
Sep  6 12:07:30.765: INFO: Node vm114011 is running more than one daemon pod
Sep  6 12:07:31.764: INFO: Number of nodes with available pods: 2
Sep  6 12:07:31.764: INFO: Node vm114013 is running more than one daemon pod
Sep  6 12:07:32.765: INFO: Number of nodes with available pods: 3
Sep  6 12:07:32.765: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep  6 12:07:32.793: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:32.793: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:32.793: INFO: Wrong image for pod: daemon-set-wsf74. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:33.802: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:33.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:33.802: INFO: Wrong image for pod: daemon-set-wsf74. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:34.810: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:34.810: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:34.810: INFO: Wrong image for pod: daemon-set-wsf74. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:34.810: INFO: Pod daemon-set-wsf74 is not available
Sep  6 12:07:35.803: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:35.803: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:35.803: INFO: Wrong image for pod: daemon-set-wsf74. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:35.803: INFO: Pod daemon-set-wsf74 is not available
Sep  6 12:07:36.802: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:36.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:36.802: INFO: Wrong image for pod: daemon-set-wsf74. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:36.802: INFO: Pod daemon-set-wsf74 is not available
Sep  6 12:07:37.801: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:37.801: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:37.801: INFO: Wrong image for pod: daemon-set-wsf74. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:37.801: INFO: Pod daemon-set-wsf74 is not available
Sep  6 12:07:38.803: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:38.803: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:38.803: INFO: Wrong image for pod: daemon-set-wsf74. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:38.803: INFO: Pod daemon-set-wsf74 is not available
Sep  6 12:07:39.802: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:39.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:39.802: INFO: Wrong image for pod: daemon-set-wsf74. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:39.802: INFO: Pod daemon-set-wsf74 is not available
Sep  6 12:07:40.801: INFO: Pod daemon-set-459lx is not available
Sep  6 12:07:40.801: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:40.801: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:41.802: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:41.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:42.802: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:42.802: INFO: Pod daemon-set-jmqrk is not available
Sep  6 12:07:42.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:43.801: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:43.801: INFO: Pod daemon-set-jmqrk is not available
Sep  6 12:07:43.801: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:44.802: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:44.802: INFO: Pod daemon-set-jmqrk is not available
Sep  6 12:07:44.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:45.802: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:45.802: INFO: Pod daemon-set-jmqrk is not available
Sep  6 12:07:45.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:46.802: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:46.802: INFO: Pod daemon-set-jmqrk is not available
Sep  6 12:07:46.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:47.802: INFO: Wrong image for pod: daemon-set-jmqrk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:47.802: INFO: Pod daemon-set-jmqrk is not available
Sep  6 12:07:47.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:48.802: INFO: Pod daemon-set-9fzrk is not available
Sep  6 12:07:48.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:49.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:50.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:50.802: INFO: Pod daemon-set-r6vth is not available
Sep  6 12:07:51.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:51.802: INFO: Pod daemon-set-r6vth is not available
Sep  6 12:07:52.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:52.802: INFO: Pod daemon-set-r6vth is not available
Sep  6 12:07:53.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:53.802: INFO: Pod daemon-set-r6vth is not available
Sep  6 12:07:54.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:54.802: INFO: Pod daemon-set-r6vth is not available
Sep  6 12:07:55.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:55.803: INFO: Pod daemon-set-r6vth is not available
Sep  6 12:07:56.802: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:56.802: INFO: Pod daemon-set-r6vth is not available
Sep  6 12:07:57.801: INFO: Wrong image for pod: daemon-set-r6vth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Sep  6 12:07:57.801: INFO: Pod daemon-set-r6vth is not available
Sep  6 12:07:58.802: INFO: Pod daemon-set-gl2wj is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep  6 12:07:58.812: INFO: Number of nodes with available pods: 2
Sep  6 12:07:58.812: INFO: Node vm114012 is running more than one daemon pod
Sep  6 12:07:59.817: INFO: Number of nodes with available pods: 3
Sep  6 12:07:59.818: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7649, will wait for the garbage collector to delete the pods
Sep  6 12:07:59.887: INFO: Deleting DaemonSet.extensions daemon-set took: 5.317419ms
Sep  6 12:07:59.987: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.43659ms
Sep  6 12:08:09.992: INFO: Number of nodes with available pods: 0
Sep  6 12:08:09.992: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 12:08:09.996: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7649/daemonsets","resourceVersion":"35458"},"items":null}

Sep  6 12:08:09.999: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7649/pods","resourceVersion":"35458"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:08:10.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7649" for this suite.

• [SLOW TEST:40.338 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":296,"skipped":5002,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:08:10.016: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-561674cb-07ea-47fe-9986-6eb13564b9fa
STEP: Creating a pod to test consume configMaps
Sep  6 12:08:10.049: INFO: Waiting up to 5m0s for pod "pod-configmaps-14d5f194-abc0-4229-ae77-0d9f8f4cf904" in namespace "configmap-422" to be "Succeeded or Failed"
Sep  6 12:08:10.051: INFO: Pod "pod-configmaps-14d5f194-abc0-4229-ae77-0d9f8f4cf904": Phase="Pending", Reason="", readiness=false. Elapsed: 2.704549ms
Sep  6 12:08:12.055: INFO: Pod "pod-configmaps-14d5f194-abc0-4229-ae77-0d9f8f4cf904": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006502213s
STEP: Saw pod success
Sep  6 12:08:12.055: INFO: Pod "pod-configmaps-14d5f194-abc0-4229-ae77-0d9f8f4cf904" satisfied condition "Succeeded or Failed"
Sep  6 12:08:12.060: INFO: Trying to get logs from node vm114011 pod pod-configmaps-14d5f194-abc0-4229-ae77-0d9f8f4cf904 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 12:08:12.076: INFO: Waiting for pod pod-configmaps-14d5f194-abc0-4229-ae77-0d9f8f4cf904 to disappear
Sep  6 12:08:12.079: INFO: Pod pod-configmaps-14d5f194-abc0-4229-ae77-0d9f8f4cf904 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:08:12.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-422" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":297,"skipped":5010,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:08:12.087: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-f94e9db8-85b9-475e-98fc-0b93c7fba08f
STEP: Creating a pod to test consume secrets
Sep  6 12:08:12.117: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cb4df3f6-ca29-415a-ad50-52f12ee3417f" in namespace "projected-9967" to be "Succeeded or Failed"
Sep  6 12:08:12.121: INFO: Pod "pod-projected-secrets-cb4df3f6-ca29-415a-ad50-52f12ee3417f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.410955ms
Sep  6 12:08:14.126: INFO: Pod "pod-projected-secrets-cb4df3f6-ca29-415a-ad50-52f12ee3417f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00847454s
STEP: Saw pod success
Sep  6 12:08:14.126: INFO: Pod "pod-projected-secrets-cb4df3f6-ca29-415a-ad50-52f12ee3417f" satisfied condition "Succeeded or Failed"
Sep  6 12:08:14.130: INFO: Trying to get logs from node vm114011 pod pod-projected-secrets-cb4df3f6-ca29-415a-ad50-52f12ee3417f container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 12:08:14.148: INFO: Waiting for pod pod-projected-secrets-cb4df3f6-ca29-415a-ad50-52f12ee3417f to disappear
Sep  6 12:08:14.151: INFO: Pod pod-projected-secrets-cb4df3f6-ca29-415a-ad50-52f12ee3417f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:08:14.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9967" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":298,"skipped":5044,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:08:14.163: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1546
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep  6 12:08:14.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-5396 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Sep  6 12:08:14.259: INFO: stderr: ""
Sep  6 12:08:14.259: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Sep  6 12:08:19.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-5396 get pod e2e-test-httpd-pod -o json'
Sep  6 12:08:19.370: INFO: stderr: ""
Sep  6 12:08:19.370: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-09-06T12:08:14Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2022-09-06T12:08:14Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.244.0.225\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2022-09-06T12:08:15Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5396\",\n        \"resourceVersion\": \"35563\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5396/pods/e2e-test-httpd-pod\",\n        \"uid\": \"3de8534f-8f8c-42ec-8247-744dcafc5aed\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-p5bmw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"vm114011\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-p5bmw\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-p5bmw\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-06T12:08:14Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-06T12:08:15Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-06T12:08:15Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-09-06T12:08:14Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://2531c746b0eebea8fa78f9f5f4977f51c1c3b9dcb61333ef276fe57d06e25c13\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-09-06T12:08:14Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.114.11\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.0.225\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.0.225\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-09-06T12:08:14Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep  6 12:08:19.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-5396 replace -f -'
Sep  6 12:08:19.556: INFO: stderr: ""
Sep  6 12:08:19.556: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Sep  6 12:08:19.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=kubectl-5396 delete pods e2e-test-httpd-pod'
Sep  6 12:08:29.908: INFO: stderr: ""
Sep  6 12:08:29.908: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:08:29.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5396" for this suite.

• [SLOW TEST:15.754 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":299,"skipped":5068,"failed":0}
S
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:08:29.917: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Sep  6 12:08:29.944: INFO: created test-event-1
Sep  6 12:08:29.948: INFO: created test-event-2
Sep  6 12:08:29.951: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Sep  6 12:08:29.954: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Sep  6 12:08:29.962: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:08:29.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-665" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":300,"skipped":5069,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:08:29.975: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8152
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8152
STEP: creating replication controller externalsvc in namespace services-8152
I0906 12:08:30.042356      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8152, replica count: 2
I0906 12:08:33.093619      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Sep  6 12:08:33.109: INFO: Creating new exec pod
Sep  6 12:08:35.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-098945227 --namespace=services-8152 exec execpod7mm7s -- /bin/sh -x -c nslookup nodeport-service.services-8152.svc.cluster.local'
Sep  6 12:08:35.292: INFO: stderr: "+ nslookup nodeport-service.services-8152.svc.cluster.local\n"
Sep  6 12:08:35.292: INFO: stdout: "Server:\t\t10.96.0.2\nAddress:\t10.96.0.2#53\n\nnodeport-service.services-8152.svc.cluster.local\tcanonical name = externalsvc.services-8152.svc.cluster.local.\nName:\texternalsvc.services-8152.svc.cluster.local\nAddress: 10.105.147.63\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8152, will wait for the garbage collector to delete the pods
Sep  6 12:08:35.350: INFO: Deleting ReplicationController externalsvc took: 5.016275ms
Sep  6 12:08:35.451: INFO: Terminating ReplicationController externalsvc pods took: 100.498143ms
Sep  6 12:08:49.988: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:08:50.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8152" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:20.052 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":301,"skipped":5074,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:08:50.027: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep  6 12:08:50.080: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:08:59.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1976" for this suite.

• [SLOW TEST:9.884 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":302,"skipped":5076,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:08:59.910: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-9174/configmap-test-da11039e-aa46-4b37-990d-bf8ed72985e5
STEP: Creating a pod to test consume configMaps
Sep  6 12:08:59.945: INFO: Waiting up to 5m0s for pod "pod-configmaps-5ec57400-519f-4533-b94d-97c7165c68c9" in namespace "configmap-9174" to be "Succeeded or Failed"
Sep  6 12:08:59.948: INFO: Pod "pod-configmaps-5ec57400-519f-4533-b94d-97c7165c68c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.602163ms
Sep  6 12:09:01.951: INFO: Pod "pod-configmaps-5ec57400-519f-4533-b94d-97c7165c68c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006361826s
STEP: Saw pod success
Sep  6 12:09:01.951: INFO: Pod "pod-configmaps-5ec57400-519f-4533-b94d-97c7165c68c9" satisfied condition "Succeeded or Failed"
Sep  6 12:09:01.954: INFO: Trying to get logs from node vm114011 pod pod-configmaps-5ec57400-519f-4533-b94d-97c7165c68c9 container env-test: <nil>
STEP: delete the pod
Sep  6 12:09:01.967: INFO: Waiting for pod pod-configmaps-5ec57400-519f-4533-b94d-97c7165c68c9 to disappear
Sep  6 12:09:01.969: INFO: Pod pod-configmaps-5ec57400-519f-4533-b94d-97c7165c68c9 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:09:01.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9174" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":303,"skipped":5078,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:09:01.978: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-7880
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 12:09:02.000: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  6 12:09:02.052: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 12:09:04.057: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 12:09:06.058: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 12:09:08.058: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 12:09:10.057: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 12:09:12.058: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 12:09:14.057: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 12:09:16.059: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 12:09:18.058: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 12:09:20.056: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  6 12:09:22.058: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep  6 12:09:22.072: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep  6 12:09:22.077: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep  6 12:09:24.096: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.170:8080/dial?request=hostname&protocol=http&host=10.244.0.229&port=8080&tries=1'] Namespace:pod-network-test-7880 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 12:09:24.096: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 12:09:24.193: INFO: Waiting for responses: map[]
Sep  6 12:09:24.197: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.170:8080/dial?request=hostname&protocol=http&host=10.244.1.169&port=8080&tries=1'] Namespace:pod-network-test-7880 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 12:09:24.197: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 12:09:24.270: INFO: Waiting for responses: map[]
Sep  6 12:09:24.274: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.170:8080/dial?request=hostname&protocol=http&host=10.244.2.70&port=8080&tries=1'] Namespace:pod-network-test-7880 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 12:09:24.274: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
Sep  6 12:09:24.351: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:09:24.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7880" for this suite.

• [SLOW TEST:22.382 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":304,"skipped":5096,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 12:09:24.362: INFO: >>> kubeConfig: /tmp/kubeconfig-098945227
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-456c8d7c-faa4-4d2d-a169-57fa89f78813
STEP: Creating a pod to test consume configMaps
Sep  6 12:09:24.400: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c115996b-8fda-43cc-8540-0cc07aa77698" in namespace "projected-5694" to be "Succeeded or Failed"
Sep  6 12:09:24.412: INFO: Pod "pod-projected-configmaps-c115996b-8fda-43cc-8540-0cc07aa77698": Phase="Pending", Reason="", readiness=false. Elapsed: 11.606305ms
Sep  6 12:09:26.416: INFO: Pod "pod-projected-configmaps-c115996b-8fda-43cc-8540-0cc07aa77698": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015480018s
STEP: Saw pod success
Sep  6 12:09:26.416: INFO: Pod "pod-projected-configmaps-c115996b-8fda-43cc-8540-0cc07aa77698" satisfied condition "Succeeded or Failed"
Sep  6 12:09:26.419: INFO: Trying to get logs from node vm114011 pod pod-projected-configmaps-c115996b-8fda-43cc-8540-0cc07aa77698 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 12:09:26.436: INFO: Waiting for pod pod-projected-configmaps-c115996b-8fda-43cc-8540-0cc07aa77698 to disappear
Sep  6 12:09:26.439: INFO: Pod pod-projected-configmaps-c115996b-8fda-43cc-8540-0cc07aa77698 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 12:09:26.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5694" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":305,"skipped":5137,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSep  6 12:09:26.446: INFO: Running AfterSuite actions on all nodes
Sep  6 12:09:26.446: INFO: Running AfterSuite actions on node 1
Sep  6 12:09:26.446: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":5179,"failed":0}

Ran 305 of 5484 Specs in 6830.367 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 5179 Skipped
PASS

Ginkgo ran 1 suite in 1h53m51.354059435s
Test Suite Passed
